### 6. 衍生问题：Decoder 在训练的过程中使用 Teacher Forcing 如何进行并行化训练呢？

### 6.1 并行化训练的实现方式
- 在序列到序列（Seq2Seq）模型的训练中，解码器（Decoder）需要逐步生成输出序列。在不使用**教师强制（Teacher Forcing）**的情况下，解码器在每个时间步的输入都是前一个时间步的输出。这意味着每个时间步的计算都依赖于前一个时间步，无法并行化计算。
- 使用**教师强制**后，我们在训练过程中将解码器每个时间步的输入替换为目标序列中对应的真实词语。这样，所有时间步的输入都是已知的，彼此之间没有依赖关系，因此可以同时计算解码器在所有时间步的输出，实现并行化训练。

### 6.2 举例说明
假设我们有一个句子：
- 输入序列（Encoder 输入）：`<Begin> I have a cat`
- 目标输出序列（解码器的目标输出）：`I have a cat <end>`

#### a. 不使用教师强制（无法并行化）
解码器的输入和输出过程：
1. 时间步 \( t=1 \)：
   - 输入：`<Begin>`
   - 输出预测：\( y1 \)（模型预测的第一个词）
2. 时间步 \( t=2 \)：
   - 输入：\( y1 \)（模型在 \( t=1 \) 时预测的输出）
   - 输出预测：\( y2 \)
3. 时间步 \( t=3 \)：
   - 输入：\( y2 \)
   - 输出预测：\( y3 \)
4. 时间步 \( t=4 \)：
   - 输入：\( y3 \)
   - 输出预测：\( y4 \)
5. 时间步 \( t=5 \)：
   - 输入：\( y4 \)
   - 输出预测：\( y5 \)

在这种情况下，每个时间步的输入依赖于前一个时间步的输出。因此，必须按顺序逐步计算，无法并行。

#### b. 使用教师强制（实现并行化）
解码器的输入和目标输出：
- 解码器输入序列：`<Begin> I have a cat`
- 解码器目标输出序列：`I have a cat <end>`

**并行化训练过程：**
1. 准备解码器的输入和目标输出：

| 时间步 | 解码器输入              | 目标输出 |
| ------ | ----------------------- | -------- |
| \( t=1 \) | `<Begin>`               | `I`      |
| \( t=2 \) | `<Begin> I`             | `have`   |
| \( t=3 \) | `<Begin> I have`        | `a`      |
| \( t=4 \) | `<Begin> I have a`      | `cat`    |
| \( t=5 \) | `<Begin> I have a cat`  | `<end>`  |

2. 并行计算解码器的输出：
   - 输入处理：将所有输入词转换为词向量或嵌入表示，形成一个矩阵。
   - 模型计算：将整个输入序列一次性传递给解码器，并行计算所有时间步的输出。
   - 输出预测：解码器在每个时间步输出对应的预测（\( y1 \) 到 \( y5 \)），这可以在一次前向传播中完成。
3. 损失计算和反向传播：
   - 损失函数：计算每个时间步的输出预测与目标输出之间的损失。
   - 并行计算损失：由于所有时间步的预测已经得到，可以并行计算每个时间步的损失。
   - 梯度计算和更新：并行地对模型参数计算梯度并进行更新。

**关键点：**
- **时间步之间没有依赖性：**
  - 使用**教师强制**后，解码器的输入序列完全由真实的目标序列决定，不再依赖于模型在前一时间步的预测输出。
- **利用矩阵运算并行化：**
  - 通过将整个序列的数据组织成矩阵形式，利用现代深度学习框架的向量化和并行计算能力（如 GPU 加速），一次性计算整个序列的输出。

### 6.3 小节：为什么使用教师强制可以并行化？
- **已知的输入序列：**
  - 在训练中，我们使用目标序列中正确的词作为解码器的输入，而不是模型自己预测的词。
- **去除时间步依赖：**
  - 这种做法消除了时间步之间的依赖关系，使得每个时间步的计算仅依赖于已知的数据。
- **并行计算所有时间步：**
  - 因为所有的输入都是已知的，我们可以将整个序列的输入同时提供给模型，模型可以同时计算所有时间步的输出。
 
# 7.Transfoer总结

- 和RNN不同，可以比较好的并行化训练的

-  Transformer本身不能利用单词的顺序的信息的，因此需要再输入当中添加位置Embedding,,否则Transformer就是一个词袋模型，
-  重点就是Self-Attention结构，其中用到了G,K,V矩阵通过输出进行线性变换的到的
-  Transfoerm当中的Multi-Head Attention当中有多个Self-Attention可以捕获单词之间多种维度上的相关系统attention score;
