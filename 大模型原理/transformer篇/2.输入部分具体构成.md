### 2. Transformer 的输入部分具体是怎么构成的

**Transformer 中单词的输入表示 \( x \) 由单词 Embedding 和位置 Embedding 相加得到。**

<img width="430" height="94" alt="image" src="https://github.com/user-attachments/assets/a7a866e5-7662-4caa-83e5-eebb81db1280" />


### Transformer 的输入表示
### 2.1 单词 Embedding
- 单词的 Embedding 有很多种方式可以获取，
- 例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。

### 2.2 位置 Embedding
- **Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。**
- **因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。**
- **所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。**
- 位置 Embedding 用 \( \text{PE} \) 表示，\( \text{PE} \) 的维度与单词 Embedding 是一样的。
- \( \text{PE} \) 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：
  \[
  PE_{(pos,2i)} = \sin \left(pos / 10000^{2i/d}\right)
  \]
  \[
  PE_{(pos,2i + 1)} = \cos \left(pos / 10000^{2i/d}\right)
  \]
pos 表示单词在句子中的位置，d 表示 \( \text{PE} \) 的维度 (与词 Embedding 一样)，
\( 2i \) 表示偶数的维度，\( 2i + 1 \) 表示奇数维度 (即 \( 2i \leq d, 2i + 1 \leq d \))。


使用这种公式计算 **PE** 有以下的好处：
- 使 **PE** 能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以快速计算出第 21 位的 Embedding。
- 可以让模型容易地计算出相对位置，对于固定长度的间距 \( k \)，**PE**(\( \text{pos}+k \)) 可以用 **PE**(\( \text{pos} \)) 计算得到。因为：
\[
\sin(A + B) = \sin(A)\cos(B) + \cos(A)\sin(B),
\]
\[
\cos(A + B) = \cos(A)\cos(B) - \sin(A)\sin(B)
\]
- 将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示**向量 \( \text{x} \)**，**\( \text{x} \) 就是 Transformer 的输入**。

