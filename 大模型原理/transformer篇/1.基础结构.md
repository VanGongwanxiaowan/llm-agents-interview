### Transformer系列
参考资料:
- 李宏毅老师介绍:
  - https://www.youtube.com/watch?v=ugWDlIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_FQOl1JefRL4&index=61
- 可视化说明:
  - https://bbycroft.net/llm
- 其他大佬文章:
  - https://baijiahao.baidu.com/s?id=1651219987457222196&wfr=spider&for=pc
  - https://jalammar.github.io/illustrated-transformer/

- **2017年由Google团队提出: 《Attention is All You Need》**
- **纵使现在提出了很多新的框架，比如：Mamba，TTT**

### Mamba
- https://arxiv.org/pdf/2312.00752
**由卡内基梅隆大学和普林斯顿大学的研究人员2023年开发，Mamba通过引入状态空间模型（SSM），实现更高的训练速度和更强的表示能力。**

### TTT
- https://arxiv.org/pdf/2407.04620
**TTT是斯坦福大学、UCSD、UC伯克利和Meta的研究团队2024.8联合推出的一种全新架构，旨在通过机器学习模型取代RNN的隐藏状态，从而优化语言模型方法。**

# 1.介绍一下Transformer的基础结构

## 整体结构

<img width="252" height="307" alt="image" src="https://github.com/user-attachments/assets/0e5dd9b1-2682-4b6e-937f-5bba5cc5b2b1" />


<img width="360" height="209" alt="image" src="https://github.com/user-attachments/assets/cae78b06-43ad-4c2a-9dd2-c66113856618" />


## 结构举例

transformer是由necoder和decoder两个部分组成的，encoder和decoder都包含6个block

可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。

Transformer 的工作流程大体如下：

第一步：获取输入句子的每一个单词的表示向量 \( X \)，\( X \) 由单词的 Embedding 和单词位置的 Embedding 相加得到。

<img width="352" height="127" alt="image" src="https://github.com/user-attachments/assets/83904427-77b0-43b6-a323-1d9fde286585" />


第二步：将得到的单词表示向量矩阵 (每一行是一个单词的表示 \( x \)) 传入 Encoder 中。

经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 \( C \)。

单词向量矩阵用 \( X(n×d) \) 表示，\( n \) 是句子中单词个数，\( d \) 是表示向量的维度 (论文中 \( d = 512 \))。

每一个 Encoder block 输出的矩阵维度与输入完全一致。


<img width="261" height="294" alt="image" src="https://github.com/user-attachments/assets/77704ac4-5f4b-482e-a5f8-d2bf46c2d116" />

### Transformer Encoder 编码句子信息
**第三步：将 Encoder 输出的编码信息矩阵 \( C \) 传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 \( 1 \sim i \) 翻译下一个单词 \( i + 1 \)。**

在使用的过程中，翻译到单词 \( i + 1 \) 的时候需要通过 **Mask (掩盖)** 操作遮盖住 \( i + 1 \) 之后的单词。

<img width="453" height="249" alt="image" src="https://github.com/user-attachments/assets/f45344b3-8207-43fc-bbc0-18281eddca09" />


### Transformer Decoder 预测
- **上图 Decoder 接收了 Encoder 的编码矩阵 \( C \)，然后首先输入一个翻译开始符 "\<Begin\>"，预测第一个单词 "I"**；
- **然后输入翻译开始符 "\<Begin\>" 和单词 "I"，预测单词 "have"，以此类推**。









