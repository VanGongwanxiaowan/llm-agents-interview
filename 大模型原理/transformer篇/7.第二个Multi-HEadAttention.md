### 5.2 第二个 Multi-Head Attention
**Decoder block** 第二个 **Multi-Head Attention** 变化不大，主要的区别在于其中 **Self-Attention** 的 **K**, **V** 矩阵不是使用上一个 **Decoder block** 的输出计算的，而是使用 **Encoder** 的编码信息矩阵 **C** 计算的。
根据 **Encoder** 的输出 **C** 计算得到 **K**, **V**，根据上一个 **Decoder block** 的输出 **Z** 计算 **Q**(如果是第一个 **Decoder block** 则使用输入矩阵 **X** 进行计算)，后续的计算方法与之前描述的一致。
这样做的好处是在 **Decoder** 的时候，每一位单词都可以利用到 **Encoder** 所有单词的信息 (这些信息无需 **Mask**)。

transformer中，连接 **encoder** 和 **decoder** 的中间矩阵 **C**，会被使用几次？
- 中间矩阵 **C** 会被使用 **N** 次，其中 **N** 是解码器的层数。例如，若解码器有 6 层，则 **C** 会被使用 6 次。

<img width="329" height="185" alt="image" src="https://github.com/user-attachments/assets/ae3619bb-c6dd-4ed5-8903-fbd80fa05620" />


### 5.3 Softmax 预测输出单词
**Decoder block** 最后的部分是利用 **Softmax** 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 **Z**，因为 **Mask** 的存在，使得单词 0 的输出 \( Z_0 \) 只包含单词 0 的信息，如下：


<img width="286" height="104" alt="image" src="https://github.com/user-attachments/assets/2320b99f-340b-42d5-948a-7e22fc2f4827" />

<img width="639" height="443" alt="image" src="https://github.com/user-attachments/assets/86e26fc6-0311-4654-9ade-ab6fc7ce8699" />
