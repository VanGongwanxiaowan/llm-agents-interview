# Decoder结构

<img width="251" height="288" alt="image" src="https://github.com/user-attachments/assets/bcbf8673-ad78-4b65-b201-c66c475fe7fe" />


### Transformer Decoder block
上图红色部分为 Transformer 的 **Decoder block** 结构，与 **Encoder block** 相似，但是存在一些区别：
- 包含两个 **Multi-Head Attention** 层。
  - 第一个 **Multi-Head Attention** 层采用了 **Masked** 操作。
  - 第二个 **Multi-Head Attention** 层的**K**, **V**矩阵使用 **Encoder** 的编码信息矩阵**C**进行计算，而**Q**使用上一个 **Decoder block** 的输出计算。
- 最后有一个 **Softmax** 层计算下一个翻译单词的概率。

### 5.1 第一个 Multi-Head Attention
**Decoder block** 的第一个 **Multi-Head Attention** 采用了 **Masked** 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 \( i \) 个单词，才可以翻译第 \( i + 1 \) 个单词。
通过 **Masked** 操作可以防止第 \( i \) 个单词知道 \( i + 1 \) 个单词之后的信息。
下面以 "我有一只猫" 翻译成 "I have a cat" 为例，了解一下 **Masked** 操作。
下面的描述中使用了类似 **Teacher Forcing** 的概念。

### Teacher Forcing 的概念
在处理像语言翻译这样的任务时，我们希望模型能够从一句话中逐词生成相应的翻译。为了训练模型能有效地做到这一点，我们经常使用一种叫做"**Teacher Forcing**"的方法。简单来说，这方法是一种告诉模型正确答案，以便它能更好学习的技巧。
想象一下，你在教小朋友用英语造句，在训练的时候，你不希望小朋友总是自由发挥，而是希望他们按照正确的例子来练习，这样才能尽快学会如何正确造句。对于模型来说也是类似的：
1. **训练阶段**：
   - 假设模型正在学习从英语到法语翻译句子。对于每个单词，模型都需要生成下一个单词。
   - 在"**Teacher Forcing**"中，每一步模型不是依据自己前一步猜测的单词，而是直接使用正确的下一个单词来帮它做出判断。这就像告诉小朋友"接下来应该说这个单词哦！"
2. **好处**：
   - 这样做加快了模型的学习速度，因为它总是"走在正确的路上"，不用一直被自己错误的猜测拖累。
   - 帮助模型更快理解语言的结构和不同单词之间的关系。
3. **使用时机**：
   - 在模型训练时，我们使用"**Teacher Forcing**"来快速让模型学习。
   - 当模型真正独立工作时，比如实时翻译时，它就不再有正确答案可以参考，只能根据自己上一步的输出继续推断，这时候模型就要展示自己的真实水平了。

<img width="643" height="470" alt="image" src="https://github.com/user-attachments/assets/e54eddec-8f71-4483-98ca-0d05426ebf24" />

<img width="622" height="484" alt="image" src="https://github.com/user-attachments/assets/29522e34-9ccb-49fc-91f8-1b48b9388d75" />

<img width="625" height="435" alt="image" src="https://github.com/user-attachments/assets/8bfa3bd6-d130-4774-9623-2a514429e10c" />








