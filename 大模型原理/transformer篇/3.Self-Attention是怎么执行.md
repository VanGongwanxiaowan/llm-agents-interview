# 3.Self-Attention是怎么执行的
<img width="260" height="386" alt="image" src="https://github.com/user-attachments/assets/307c6c15-8475-480c-a083-56a99b826dc9" />


### Transformer 整体结构
- 上图是论文中 Transformer 的内部结构图，左侧为 **Encoder block**，右侧为 **Decoder block**。
- 红色圈中的部分为 **Multi-Head Attention**，是由多个 **Self-Attention** 组成的，可以看到 **Encoder block** 包含一个 **Multi-Head Attention**，而 **Decoder block** 包含两个 **Multi-Head Attention** (其中有一个用到 **Masked**)。
- **Multi-Head Attention** 上方还包括一个 **Add & Norm** 层，**Add** 表示残差连接 (Residual Connection) 用于防止网络退化，**Norm** 表示 **Layer Normalization**，用于对每一层的激活值进行归一化。

因为 **Self-Attention** 是 Transformer 的重点，所以我们重点关注 **Multi-Head Attention** 以及 **Self-Attention**，首先详细了解一下 **Self-Attention** 的内部逻辑。

### 3.1 Self-Attention 结构

<img width="200" height="216" alt="image" src="https://github.com/user-attachments/assets/2442b476-ecb4-4363-b534-3ef94ba44d24" />


### Self-Attention 结构
- 上图是 **Self-Attention** 的结构，在计算的时候需要用到矩阵 **Q**(查询)，**K**(键值)，**V**(值)。
- 在实际中，**Self-Attention** 接收的是输入(单词的表示向量组成的矩阵 **X**) 或者上一个 **Encoder block** 的输出。
- 而 **Q**, **K**, **V** 正是通过 **Self-Attention** 的输入进行线性变换得到的。

### 3.2 Q, K, V 的计算
**Self-Attention** 的输入用矩阵 **X** 进行表示，则可以使用线性变阵矩阵 **WQ**, **WK**, **WV** 计算得到 **Q**, **K**, **V**。
计算如下图所示，注意 **X**, **Q**, **K**, **V** 的每一行都表示一个单词。

<img width="262" height="301" alt="image" src="https://github.com/user-attachments/assets/db2b53c9-d6fb-41bc-b9de-bf32f43c260d" />


### 3.3.Self-Attention的输出

<img width="527" height="257" alt="image" src="https://github.com/user-attachments/assets/751be517-c467-4785-8568-e4e53b76209f" />

<img width="431" height="153" alt="image" src="https://github.com/user-attachments/assets/fa93f924-1993-4dcf-9058-00008b8b3b87" />

<img width="454" height="128" alt="image" src="https://github.com/user-attachments/assets/1836af11-5b31-4edb-8e2b-268e85552e59" />


<img width="465" height="178" alt="image" src="https://github.com/user-attachments/assets/3a74b6cf-5dea-47c9-b504-3138f9a29270" />

Z1的计算方法


