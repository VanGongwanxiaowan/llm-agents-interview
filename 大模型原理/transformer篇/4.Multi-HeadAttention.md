### 3.4 Multi-Head Attention
在上一步，我们已经知道怎么通过 **Self-Attention** 计算得到输出矩阵 \( \text{Z} \)，而 **Multi-Head Attention** 是由多个 **Self-Attention** 组合形成的，下图是论文中 **Multi-Head Attention** 的结构图。

<img width="262" height="292" alt="image" src="https://github.com/user-attachments/assets/e593f350-edef-4a1e-98e1-a63428421f4f" />


### Multi-Head Attention
从上图可以看到 **Multi-Head Attention** 包含多个 **Self-Attention** 层，首先将输入 \( \text{X} \) 分别传递到 \( \text{h} \) 个不同的 **Self-Attention** 中，计算得到 \( \text{h} \) 个输出矩阵 \( \text{Z} \)。下图是 \( \text{h}=8 \) 时候的情况，此时会得到 8 个输出矩阵 \( \text{Z} \)。

<img width="262" height="292" alt="image" src="https://github.com/user-attachments/assets/025b74d0-c05a-43ce-aafa-1b9232893f6a" />

### 多个Self-Attention
得到 8 个输出矩阵 **Z1** 到 **Z8** 之后，**Multi-Head Attention** 将它们拼接在一起 (**Concat**)，然后传入一个 **Linear** 层，得到 **Multi-Head Attention** 最终的输出 **Z**。

<img width="316" height="195" alt="image" src="https://github.com/user-attachments/assets/8d93076e-b50f-4d70-afbd-495134a38ab9" />


### Multi-Head Attention 的输出
可以看到 **Multi-Head Attention** 输出的矩阵 **Z** 与其输入的矩阵 **X** 的维度是一样的。

