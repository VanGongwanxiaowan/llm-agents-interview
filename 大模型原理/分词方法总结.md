# LLM中的Tokenizers
## 一、概述
- **主题**：LLM（大语言模型）中的Tokenizers（分词器）

- **核心内容框架**：Transformer的输入是什么、Tokenizer的作用、三种不同分词粒度的Tokenizers、四种常用的Subword Tokenizers

## 二、Transformer的输入
### （一）宏观与微观层面
1. **宏观**：文本、图像、音频、视频等多种类型数据
2. **微观**：最终需转化为Token（数字），作为Transformer的输入

### （二）文本输入处理示例
以文本“This is a input text.”为例：
1. **Tokenization（分词）**：得到["[CLS]", "This", "is", "a", "input", ".", "[SEP]"]，每个token对应一个id，如[101, 2023, 2003, 1037, 7953, 1012, 102]
2. **Embeddings（嵌入）**：每个Token对应的嵌入向量是学习得到的，例如[-0.0208, 0.0031, -0.0123, 0.0151, 0.0390, -0.0558, -0.0440, -0.0236, -0.0283, -0.0037, -0.0402, 0.0119, -0.0016, 0.0057, 0.0069, -0.0099, -0.0095, 0.0199, -0.0788, -0.0352, 0.0202,...]

### （三）Embedding层代码实现
Embedding layer是Transformer模型的一部分，代码如下：
```python
class InputEmbedding(nn.Module):
    """Implementation of the input embeddings"""
    def __init__(self, d_model: int, vocab_size: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, d_model)
    
    def forward(self, x):
        return self.embedding(x) * math.sqrt(self.d_model)
```

## 三、Tokenizer的作用
1. **核心功能**：将文本序列转化为数字序列（token编号），作为Transformer的输入
2. **重要性**：是训练和微调LLM必不可少的一部分
3. **不同模型分词示例**
    - **gpt-4/ gpt-3.5-turbo /text-embedding-ada-002**：对“Hello,this is tianxin.”分词，得到10个TOKENS，对应23个CHARACTERS，token序列为[9906, 11, 420, 374, 259, 1122, 59866, 13]
    - **LLaMA/Llama 2**：对“Hello,this is tianxin.”分词，得到8个TOKENS，对应23个CHARACTERS，token序列为[1, 15043, 29892, 445, 338, 260, 713, 29916, 262, 29889]（序列含起始标识）

4. **相关参考工具**
    - HuggingFace：https://huggingface.co/spaces/Xenova/the-tokenizer-playground
    - OpenAI：https://platform.openai.com/tokenizer

## 四、三种不同分词粒度的Tokenizers
### （一）Word-based Tokenizers（基于词的分词器）
1. **分词方式**：将文本划分为一个个词（包括标点），不同工具处理细节有差异
    - 未精细处理标点：如将“Don't you love Transformers? We sure do.”划分为["Don't", "you", "love", "Transformers?", "We", "sure", "do."]
    - 标点单独拆分：划分为["Don", "'", "t", "you", "love", "Transformers", "?", "We", "sure", "do", "."]
    - 按语义拆分（如spaCy和Moses）：划分为["Do", "n't", "you", "love", "Transformers", "?", "We", "sure", "do", "."]
2. **优缺点**
    - **优点**：符合人的自然语言和直觉
    - **缺点**：词表非常大；相同意思的词可能被划分为不同的token；限制词表大小时，未知词用特殊token表示，会导致信息丢失，模型性能大打折扣。例如“He's gonna do cool malapromisms”分词可能得到[250, UNKNOWN, 861, 10000, UNKNOWN]
3. **相关参考**：https://huggingface.co/docs/transformers/tokenizer_summary

### （二）Character-based Tokenizers（基于字符的分词器）
1. **分词方式**：将文本划分为一个个字符（包括标点），如对“What is your name?"划分为["W", "h", "a", "t", "i", "s", "y", "o", "u", "r", "n", "a", "m", "e", "?"]
2. **优缺点**
    - **优点**：可以表示任意（英文）文本，不会出现word-based中的unknown情况；对西文来说，不需要很大的词表，例如英文只需不到256个字符
    - **缺点**：相对单词来说信息量非常低，模型性能一般很差；相对于word-based来说，会产生很长的Token序列；中文也需要很大一个词表
3. **分词序列对比**
    - Word-based tokenization：序列较短，如[250, 861, 10000, UNKNOWN]
    - Character-based tokenization：序列较长，如[12, 5, 20, 40, 19, 4, 15, 3, 15, 15, 12, 13, 1, 12, 15, 16, 18, 15, 16, 9, 19, 13, 19, 13]
4. **相关参考**：https://huggingface.co/docs/transformers/tokenizer_summary

### （三）Subword-based Tokenizers（基于子词的分词器）
1. **设计理念**：结合Word-based和Character-based的优点，常用词不应该再被切分成更小的token或子词（subword），不常用的词或词群应该用子词来表示
2. **分词示例**
    - “tokenization”可根据不同模型划分为不同子词组合，如BERT中可能划分为["token", "##ization"]
    - “dog”和“dogs”，“dog”作为常用词不拆分，“dogs”可能拆分为["dog", "##s"]
3. **优缺点**
    - **优点**：Vocabulary的大小固定，能保持语义，适合多语言
    - **避免的问题**：避免了Word-based词表大、未知词多、相同语义不同形式词拆分不同的问题；避免了Character-based单个token信息量少、序列长的问题
4. **常见方法与对应模型**

|分词方法|典型模型|
|----|----|
|BPE/BBPE|GPT, GPT-2, GPT-J, GPT-Neo, RoBERTa, BART, LLaMA, ChatGLM-6B, Baichuan|
|WordPiece|BERT, DistilBERT, MobileBERT|
|Unigram|AlBERT, T5, mBART, XLNet|
|SentencePiece（基于BPE或Unigram）|适用于多种模型，尤其多语言场景|

5. **相关参考**：https://huggingface.co/docs/transformers/tokenizer_summary

## 五、四种常用的Subword Tokenizers
### （一）Byte-Pair Encoding (BPE) Tokenization（字节对编码分词器）
1. **核心算法**：包含“词频统计”与“词表合并”两部分
2. **具体步骤（示例）**
    - **词频统计（pre-tokenization）**：一般采用word-based tokenization，如语料为("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
    - **按初始词表切分**：将每个词拆分为单个字符，得到("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)，初始基本词表为["b", "g", "h", "n", "p", "s", "u"]
    - **统计相邻token频率并合并**
        - 第1次统计相邻token频率：hu:15, ug:20, pu:17, un:16, bu:4, gs:5，将最高频pair“ug”添加到词表，并用“ug”替换原词中的“u”“g”，得到("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
        - 第2次统计：hug:15, pug:5, pu:12, un:16, bu:4, ugs:5，将最高频pair“un”添加到词表，替换后得到("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
        - 第3次统计：hug:15, pug:5, pun:12, bun:4, ugs:5，将最高频pair“hug”添加到词表，替换后得到("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
    - **停止合并**：BPE的合并次数是超参数，达到预设次数停止，得到最终词表
3. **分词应用**：用最终词表进行tokenization，如“hug”→["hug"]，“bug”→["b", "ug"]，“mug”→["<unk>", "ug"]（“mug”不在词表中）
4. **模型词表示例**：GPT的词汇表大小为40478，因为它有478个基本字符，并且在40000次合并后停止
5. **相关参考**
    - 论文：Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural machine translation of rare words with subword units." arXiv preprint arXiv:1508.07909 (2015)
    - 文档：https://huggingface.co/docs/transformers/tokenizer_summary、https://arxiv.org/abs/1508.07909

### （二）Byte-level BPE (BBPE)（字节级BPE）
1. **BPE的缺点**：包含所有可能的基本字符（token）的基本词汇表可能相当大，例如将所有Unicode字符都视为基本字符（如中文）
2. **改进方式**：将字节（byte）视为基本token，两个字节合并即可以表示Unicode，如中文、日文、阿拉伯文、表情符号等等
3. **模型词表示例**：GPT-2的词汇表大小为50257，对应于256字节的基本token、一个特殊的文本结束token和通过50000个合并得到的token

### （三）WordPiece Tokenization（词片分词器）
1. **与BPE的异同**
    - **相似点**：大体和BPE类似，都通过逐步合并构建词表
    - **不同点**
        - 构建基本词表时，除第一个字母，会添加##作为前缀（BERT），如“word”→["w", "##o", "##r", "##d"]
        - 合并依据不同：使用类似联合概率的大小而不是次数对token进行合并，公式为：$pair得分 =\frac{ pair 出现的次数 }{ token 1 出现的次数 \times token2出现的次数}$，该算法优先考虑单个token在词表中不太频繁的pair进行合并
2. **合并示例**
    - 假设词表为["b", "h", "p", "##g", "##n", "##s", "##u"]，语料为("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
    - 按词表切分：("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
    - 计算pair出现次数：hu:15, ug:20, pu:17, un:16, bu:4, gs:5
    - 计算单个token出现次数：h:15, u:36, g:20, p:17, n:16, b:4, s:5
    - 计算每个pair的概率：hu: 15/(15×36)，ug: 20/(36×20)，pu: 17/(17×36)，un: 16/(36×16)，bu: 4/(4×36)，gs: 5/(20×5)
    - 合并概率最大的pair：("##g", "##s")→("##gs")
3. **合并逻辑示例**：不一定会合并（“un”、“##able”），即使这对词在词汇表中出现得很频繁，因为“un”和“##able”可能都会出现在很多其他单词中且频率很高；相比之下，像（“hu”、“##gging”）这样的一对可能会更快地融合（假设“hugging”一词经常出现在词汇表中），因为“hu”和“##gging”单独出现的频率可能较低
4. **相关参考**
    - 论文：https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf
    - 文档：https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt

### （四）Unigram Tokenization（单字分词器）
1. **应用场景**：经常在SentencePiece中使用，是AlBERT、T5、mBART、Big Bird和XLNet等模型使用的tokenization算法
2. **核心流程**：从初始一个很大的词表，通过多次token删减，最终形成一个较小的词表
3. **关键概念与步骤**
    - **Unigram model（单字模型）**：假设每个词的出现都是独立的，对于文本序列“t1, t2, t3, ..., tN”，其概率计算为$P(t 1,t2,t3,... ,tN)=P(t1) × P(t2)× P(t3) × ... × P(tN)$
    - **文本切分**：根据词表计算每种切分的概率，任选概率最大的切分方式。例如对“hug”，词表中h:10/180、u:36/180、g:36/180、hu:10/180、ug:36/180，不同切分概率为：
        - h u g：(10/180)×(36/180)×(36/180)=2.22e-03
        - hu g：(10/180)×(36/180)=1.11e-02
        - h ug：(10/180)×(36/180)=1.11e-02
        - 选择概率最大的“hu g”或“h ug”切分方式
    - **Unigram loss（单字损失）**：计算语料的Unigram loss（负对数似然），公式为$\sum freq×(-log(P(word)))$。例如某语料各部分频率及对应切分概率得分，计算loss为10×(-log(1.11e-02)) + 12×(-log(1.33e-02)) + 5×(-log(5.56e-03)) + 4×(-log(4.44e-03)) + 5×(-log(5.56e-03))=170.4
    - **Token删减**：尝试删去一个token，计算对应的unigram loss，删除p%使得loss增加最少的token，且不删除基本token（如26个字母和标点），迭代直到词表的大小缩减到预设值
    - **优化方式**：为单词的每种划分计算概率非常耗时，采用更高效的维特比（Viterbi）算法
4. **注意事项**：Unigram会遇到unknown的情况
5. **相关参考**
    - 论文：https://arxiv.org/pdf/1804.10959.pdf
    - 文档：https://huggingface.co/learn/nlp-course/chapter6/7、https://www.youtube.com/watch?v=TGZfZVuF9Yc

### （五）SentencePiece Tokenization（句子片段分词器）
1. **解决的问题**：BPE、WordPiece、Unigram的缺点是假设输入文本使用空格来分隔单词，但并非所有语言都使用空格来分隔单词（如中文、韩文、日文、阿拉伯语）；虽可使用特定语言的pre-tokenizer分词，但不太通用
2. **改进方式**：将输入视为输入字节流，包括空格，然后使用Byte-level BPE或unigram算法来构建适当的词汇表
3. **分词示例**
    - **SentencePiece（以XLNetTokenizer为例）**：
    ```python
    from transformers import XLNetTokenizer
    tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
    tokenizer.tokenize("Don't you love 🤗 Transformers? We sure do.")
    # 输出：["▁Don", "'", "t", "▁you", "▁love", "▁", "🤗", "▁", "Transform", "ers", "?", "▁We", "▁sure", "▁do", "."]
    ```
    - **WordPiece（以BertTokenizer为例）**：
    ```python
    from transformers import BertTokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokenizer.tokenize("I have a new GPU!")
    # 输出：["i", "have", "a", "new", "gp", "##u", "!"]
    ```
4. **相关参考**：https://arxiv.org/pdf/1808.06226.pdf、https://huggingface.co/docs/transformers/tokenizer_summary

## 六、各分词方法优缺点对比
|分词方法|优点|缺点|
|----|----|----|
|BPE|允许较大的词汇表大小；能很好地处理稀有词和未登录词（out-of-vocabulary words）|单个词可能会被拆分为很多子词；不适用于没有明确词边界的语言|
|WordPiece|能很好地处理稀有词和未登录词；对子词的分词效率高|单个词可能会被拆分为很多子词|
|Unigram|在大词汇表上扩展性好；对子词的分词效率高|单个词可能会被拆分为很多子词|
|SentencePiece with BPE|对子词的分词效率高；能处理任何字符，无未登录词；对子词的分词效率高|单个词可能会被拆分为很多子词|
|Word-level Tokenization|实现简单；能处理任何词或字符|不能很好地处理稀有词和未登录词；单个词可能会被拆分为很多子词|

## 七、总结
1. **Transformer的输入**：宏观为多类型数据，微观需转化为Token（数字），经Embedding层处理为嵌入向量
2. **Tokenizer的作用**：将文本序列转化为数字序列，是LLM训练和微调的关键环节
3. **三种分词粒度**
    - Word-based：符合直觉，但词表大、易有未知词
    - Character-based：无未知词，但信息密度低、序列长
    - Subword-based：平衡词表大小与语义保留，适合多语言
4. **四种Subword分词器**
    - BPE/BBPE：通过词频统计与合并构建词表，BBPE以字节为基本单位
    - WordPiece：基于概率合并，添加##前缀标识非词首子词
    - Unigram：从大词表逐步删减，基于Unigram loss选择删减token
    - SentencePiece：处理无空格分隔语言，基于BPE或Unigram

