### 5. 在使用RAG时候，有哪些优化策略（上）
- **RAG**各个环节均有着极大的优化空间。下面我们将从之前讲的5个环节中穿插12个具体优化策略来依次讲解。
- 下面介绍的方法均在AI开发框架**langchain**和**LLamaIndex**中有具体实现，具体操作方法可参考官方文档。

### 5.1 知识文档准备阶段
#### a. 数据清洗
（此处无具体内容，推测为后续讲解或需结合其他资料）

#### b. 分块处理
在**RAG**系统中，文档需要分割成多个文本块再进行向量嵌入。
在不考虑大模型输入长度限制和成本问题情况下，其目的是在保持语义上的连贯性的同时，尽可能减少嵌入内容中的噪声，从而更有效地找到与用户查询最相关的文档部分。
如果分块太大，可能包含太多不相关的信息，从而降低了检索的准确性。相反，分块太小可能会丢失必要的上下文信息，导致生成的回应缺乏连贯性或深度。
在**RAG**系统中实施合适的文档分块策略，旨在找到这种平衡，确保信息的完整性和相关性。一般来说，理想的文本块应当在没有周围上下文的情况下对人类来说仍然有意义，这样对语言模型来说也是有意义的。

**分块方法的选择**
- 固定大小的分块：这是最简单和直接的方法，我们直接设定块中的字数，并选择块之间是否重复内容。
- 通常，我们会保持块之间的一些重叠，以确保语义上下文不会在块之间丢失。与其他形式的分块相比，固定大小分块简单易用且不需要很多计算资源。

**内容分块**
- 顾名思义，根据文档的具体内容进行分块，例如根据标点符号（如句号）分割。或者直接使用更高级的**NLTK**或者**spaCy**库提供的句子分割功能。

**递归分块**
- 在大多数情况下推荐的方法。
- 其通过重复地应用分块规则来递归地分解文本。
- 例如，在**langchain**中会先通过段落换行符（\n\n）进行分割。然后，检查这些块的大小。如果大小不超过一定阈值，则该块被保留。对于大小超过标准的块，使用单换行符（\n）再次分割。以此类推，不断根据块大小更新更小的分块规则（如空格，句号）。这种方法可以灵活地调整块的大小。例如，对于文本中的密集信息部分，可能需要更细的分割来捕捉细节；而对于信息较少的部分，则可以使用更大的块。而它的挑战在于，需要制定精细的规则来决定何时和如何分割文本。

**从小到大分块**
- 既然小的分块和大的分块各有各的优势，一种更为直接的解决方案是把同一文档进行从大到小所有尺寸的分割，然后把不同大小的分块全部存进向量数据库，并保存每个分块的上下级关系，进行递归搜索。但可想而知，因为我们要存储大量重复的内容，这种方案的缺点就是需要更大的储存空间。

**特殊结构分块**
- 针对特定结构化内容的专门分割器。这些分割器特别设计来处理这些类型的文档，以确保正确地保留和理解其结构。**langchain**提供的特殊分割器包括：**Markdown**文件，**Latex**文件，以及各种主流代码语言分割器。

**分块大小的选择**
- 上述方法中无一例外最终都需要设定一个参数——块的大小，那么我们如何选择呢？
- 首先不同的嵌入模型有其最佳输入大小。比如**Openai**的**text-embedding-ada-002**的模型在256 或 512大小的块上效果更好。
- 其次，文档的类型和用户查询的长度及复杂性也是决定分块大小的重要因素。处理长篇文章或书籍时，较大的分块有助于保留更多的上下文和主题连贯性；而对于社交媒体帖子，较小的分块可能更适合捕捉每个帖子的精确语义。如果用户的查询通常是简短和具体的，较小的分块可能更为合适；相反，如果查询较为复杂，可能需要更大的分块。
- 实际场景中，我们可能还是需要不断实验调整，在一些测试中，128大小的分块往往是最佳选择，在无从下手时，可以从这个大小作为起点进行测试。

### 5.2 嵌入模型阶段
#### a. 嵌入模型
- 我们提到过嵌入模型能帮助我们把文本转换成向量，显然不同的嵌入模型带来的效果也不尽相同，例如，**Word2Vec**模型，尽管功能强大，但存在一个重要的局限性：其生成的词向量是静态的。一旦模型训练完成，每个词的向量表示就固定不变，这在处理一词多义的情况时可能导致问题。
- 比如，“我买了一张光盘”，这里“光盘”指的是具体的圆形盘片，而在“光盘行动”中，“光盘”则指的是把餐盘里的食物吃光，是一种倡导节约的行为。
- 语义完全不一样的词向量却是固定的。相比之下，引入自注意力机制的模型，如**BERT**，能够提供动态的词义理解。这意味着它可以根据上下文动态地调整词义，使得同一个词在不同语境下有不同的向量表示。在之前的例子中，“光盘”这个词在两个句子中会有不同的向量，从而更准确地捕捉其语义。
- 有些项目为了让模型对特定垂直领域的词汇有更好的理解，会嵌入模型进行微调。但在这里我们并不推荐这种方法，一方面其对训练数据的质量有较高要求，另一方面也需要较多的人力物力投入，且效果未必理想，最终得不偿失。
- 在这种情况下，对于具体应该如何选择嵌入模型，推荐参考**Hugging Face**推出的嵌入模型排行榜**MTEB**。这个排行榜提供了多种模型的性能比较，能帮助我们做出更明智的选择。同时，要注意并非所有嵌入模型都支持中文，因此在选择时应查阅模型说明。
  - 目前**sota**表现是北京大学和腾讯团队一起开源的**Conan embedding**；
  - **C-MTEB (Chinese Massive Text Embedding Benchmark)**：中文海量文本嵌入测试基准。

### 5.3 向量数据库阶段
#### a. 元数据
- 当在向量数据库中存储向量数据时，某些数据库支持将向量与元数据（即非向量化的数据）一同存储。为向量添加元数据标注是一种提高检索效率的有效策略，它在处理搜索结果时发挥着重要作用。
- 例如，日期就是一种常见的元数据标签。它能够帮助我们根据时间顺序进行筛选。设想一下，如果我们正在开发一款允许用户查询他们电子邮件历史记录的应用程序。在这种情况下，日期最近的电子邮件可能与用户的查询更相关。然而，从嵌入的角度来看，我们无法直接判断这些邮件与用户查询的相似度。通过将每封电子邮件的日期作为元数据附加到其嵌入中，我们可以在检索过程中优先考虑最近日期的邮件，从而提高搜索结果的相关性。
- 此外，我们还可以添加诸如章节或小节的引用，文本的关键信息、小节标题或关键词等作为元数据。这些元数据不仅有助于改进知识检索的准确性，还能为最终用户提供更加丰富和精确的搜索体验。
