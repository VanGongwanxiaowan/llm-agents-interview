### 5. 目前主流的LLMs开源模型体系有哪些？（Prefix Decoder, Causal Decoder和Encoder-Decoder区别是什么？）
- 在预训练语言模型时代，自然语言处理领域广泛采用了**预训练 + 微调**的范式，并诞生了以 **BERT** 为代表的**编码器（Encoder-only）** 架构、以 **GPT** 为代表的**解码器（Decoder-only）** 架构和以 **T5** 为代表的**编码器-解码器（Encoder-decoder）** 架构的大规模预训练语言模型。
- 随着 **GPT** 系列模型的成功发展，当前自然语言处理领域走向了生成式大语言模型的道路，**解码器架构** 已经成为了目前大语言模型的主流架构。进一步，解码器架构还可以细分为两个变种架构，包括**因果解码器（Causal Decoder）** 架构和**前缀解码器（Prefix Decoder）** 架构。值得注意的是，学术界所提到解码器架构时，通常指的都是**因果解码器** 架构。

下图针对三种架构（**Causal Decoder**, **Prefix Decoder** 和 **Encoder-Decoder**）进行了对比：



<img width="625" height="231" alt="image" src="https://github.com/user-attachments/assets/38ee7903-926c-4c53-8c02-c59ec426be34" />


### Encoder-Decoder
- **Encoder-Decoder** 架构是自然语言处理领域里一种经典的模型结构，广泛应用于如机器翻译等多项任务。原始的 **Transformer** 模型也使用了这一架构，组合了两个分别担任编码器和解码器的 **Transformer** 模块。
- 如下图所示，此架构在编码器端采用了**双向自注意力**机制对输入信息进行编码处理，而在解码器端则使用了**交叉注意力**与**掩码自注意力**机制，进而通过自回归的方式对输出进行生成。
- 基于**编码器-解码器**设计的预训练语言模型在众多自然语言理解与生成任务中展现出了优异的性能，但是目前只有如 **FLAN-T5** 等少数大语言模型是基于**编码器-解码器**架构构建而成的。

- 

<img width="335" height="288" alt="image" src="https://github.com/user-attachments/assets/b7b41771-9c3e-4380-8182-ef0110d5e25e" />




### Causal Decoder
- Causal LM是因果语言模型，目前流行的大多数模型都是这种结构，别无他因，因为GPT系列模型内部结构就是它，还有开源界的LLaMa也是。**Causal Decoder架构的典型代表就是GPT系列模型，使用的是单向注意力掩码，以确保每个输入token只能注意到过去的token和它本身，输入和输出的token通过Decoder以相同的方式进行处理。**
- 在下图中，灰色代表对应的两个token互相之间看不到，否则就代表可以看到。例如，“Survery”可以看到前面的“A”，但是看不到后面的“of”。**Causal Decoder**的sequence mask矩阵是一种典型的下三角矩阵。
- 在因果解码器架构中，最具有代表性的模型就是 OpenAI 推出的 GPT系列。伴随着 GPT-3 的成功，因果解码器被广泛采用于各种大语言模型中，包括 BLOOM、LLaMA（Meta）等。

<img width="269" height="257" alt="image" src="https://github.com/user-attachments/assets/a0a4bb87-cef7-46b2-b1a1-21f02b0a577c" />




### Prefix Decoder
- **Prefix Decoder** 架构也被称为非因果解码器架构，对于因果解码器的掩码机制进行了修改。该架构和因果解码器一样，仅仅使用了解码器组件。
- 与之不同的是，该架构参考了**编码器-解码器**的设计，对于输入和输出部分进行了特定处理。如下图所示，前缀解码器对于输入（前缀）部分使用**双向注意力**进行编码，而对于输出部分利用**单向的掩码注意力**利用该词元本身和前面的词元进行自回归地预测。
- 与**编码器-解码器**不同的是，前缀解码器在编码和解码过程中是共享参数的，并没有划分为独立的解码器和编码器。
- 当前，基于前缀解码器架构的代表性大语言模型包括 **GLM-130B** 和 **U-PaLM（Google）**。



<img width="259" height="271" alt="image" src="https://github.com/user-attachments/assets/bc238a02-fabf-40ee-bb73-6d46b907544d" />



### 总结
Prefix Decoder, Causal Decoder 和 Encoder-Decoder 区别在于 **attention mask** 不同
- **Encoder-Decoder（代表：T5）**：
  - 在输入上采用**双向注意力**，对问题的编码理解更充分；
  - 适用任务：在偏理解的 NLP 任务上效果好；
  - 在长文本生成任务上效果差，训练效率低；
- **Causal Decoder（代表：GPT系列）**：
  - 自回归语言模型，预训练和下游应用是完全一致的，严格遵守只有后面的token才能看到前面的token的规则；
  - 适用任务：文本生成任务效果好；
  - 训练效率高，**zero-shot** 能力更强，具有涌现能力；
- **Prefix Decoder（代表：GLM）**：
  - 特点：**prefix**部分的token互相能看到；
  - 适用任务：文本生成任务效果好；





