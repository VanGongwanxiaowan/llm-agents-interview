### 4.如何评价RAG项目效果的好坏
#### 4.1 针对检索环节的评估
- **MRR 平均倒数排名**：查询（或推荐请求）的排名倒数
  - 平均倒数排名（Mean Reciprocal Rank, MRR）是一种常用的评估信息检索（Information Retrieval, IR）系统表现的指标，尤其用于衡量搜索引擎、推荐系统等根据查询返回的多个结果中的相关性。
  - 结果列表中，第一个结果匹配，分数为1，第二个匹配分数为0.5，第n个匹配分数为1/n，如果没有匹配的句子分数为0。最终的分数为所有得分之和，再求平均。

**MRR的意义**：
- MRR 值越高，表示系统对用户查询的响应越好，因为第一个相关结果更可能出现在较高的排名位置。
- 如果第一个相关结果排名在前几个位置，倒数排名接近1，会提高MRR值。
- 如果第一个相关结果排得很靠后，倒数排名就会较小，MRR值较低。

**举个例子**：
假设我们有3个查询：
1. 第一个查询的第一个相关结果排在第2位（倒数排名 = 1/2）
2. 第二个查询的第一个相关结果排在第5位（倒数排名 = 1/5）
3. 第三个查询的第一个相关结果排在第1位（倒数排名 = 1/1）
那么，MRR就会是：
\[
MRR = \frac{1}{3}\left(\frac{1}{2} + \frac{1}{5} + \frac{1}{1}\right) = \frac{1}{3}(0.5 + 0.2 + 1) = \frac{1}{3} \times 1.7 = 0.567
\]

**总结**：
- MRR衡量的是相关结果首次出现的位置（越靠前越好）。
- 适用于多结果排序任务，如搜索引擎查询、推荐系统等。
- **Hits Rate 命中率**：前k项中，包含正确信息的项的数目占比。
- **NDCG**
  - DCG的两个思想：
    1. 高关联度的结果比一般关联度的结果更影响最终的指标得分；
    2. 有高关联度的结果出现在更靠前的位置的时候，指标会越高；
  参考：搜索评价指标——NDCG

#### 4.2 针对生成环节的评估
- **非量化**：完整性、正确性、相关性
- **量化**：Rouge-L
  - Rouge-L是一种用于评价文本生成质量的指标，通常在自动摘要、机器翻译和文本生成任务中使用。它是 Rouge（Recall-Oriented Understudy for Gisting Evaluation）评估指标系列中的一种，专门通过最长公共子序列（Longest Common Subsequence, LCS）来测量生成文本和参考文本之间的相似性。
  - 基本思想为由多个专家分别生成人工摘要，构成标准摘要集，将系统生成的自动摘要与人工生成的标准摘要相对比，通过统计二者之间重叠的基本单元(n元语法、词序列和词对)的数目，来评价摘要的质量。
  - Rouge-L的计算主要包括两个方面：
    1. 召回率(Recall)：参考文本中与生成文本匹配的最长公共子序列的长度，与参考文本的总长度之比。
    2. 精确率(Precision)：生成文本中与参考文本匹配的最长公共子序列的长度，与生成文本的总长度之比。
  - 然后计算 F1 分数，即在召回率和精确率之间的调和平均，来作为 Rouge-L 的最终分数：
\[
\text{Rouge-L} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]
  - 由于 Rouge-L 注重最长公共子序列，这意味着它比 Rouge-1 或 Rouge-2 更能衡量文本生成的结构和顺序是否与参考文本接近。因此，它在衡量文段的连贯性和句子顺序上具有优势。
