https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485219&idx=1&sn=6cb0d3e9e3faa0b29bed3b62670df149&chksm=e8af26d9dfd8afcf1d3ed132bd30f73ca5cbf057e7882ff59bb57e57a6edc573e10126306200&scene=21#wechat_redirect


# 图解强化学习——基础概念1 网页总结
## 一、强化学习（RL）概览
### 1. RL在机器学习领域的定位
机器学习主要分为三类，三者核心差异如下表所示：
|学习类型|核心特点|数据/反馈依赖|
|----|----|----|
|监督学习（Supervised Learning）|用带标记数据输入，预测结果|依赖大规模带标记数据集，通过“损失函数”接收反馈|
|无监督学习（Unsupervised Learning）|用未标记数据输入，挖掘数据隐藏模式（如聚类、异常检测）|依赖大规模未标记数据集，无“监督者”反馈|
|强化学习（Reinforcement Learning）|通过与外部世界互动获取输入和反馈，确定最佳行动方案|不依赖预先收集的数据集，依赖与环境互动的动态反馈|

### 2. RL与其他学习类型的关键区别
- 无明确“监督者”：没有类似“损失函数”的直接指导信号。
- 数据动态获取：无需预先准备数据集，通过与真实世界互动实时获取数据。
- 时序决策属性：需在时间序列上迭代决策，而非单次推理（如分类问题），适用于“未知正确答案”的场景。

### 3. RL的应用场景
主要解决控制任务与决策任务，覆盖多领域：
- 智能控制：无人机操纵、自动驾驶汽车、机器人导航与任务执行（如物品搬运）。
- 金融领域：投资组合管理、交易决策。
- 娱乐领域：围棋（AlphaGo）、国际象棋、视频游戏（Atari游戏、星际争霸II/AlphaStar）。
- 大语言模型：强化学习从人类反馈中学习（RLHF）优化模型性能，自动化任务策略学习（自动摘要、对话生成）。

### 4. RL的学习机制：试错法
模拟人类学习过程，核心流程为：
1. Agent（智能体）执行某个行动；
2. 从环境接收该行动结果的反馈（正面/负面）；
3. 重复“行动-反馈”循环，学习“优质行动”与“劣质行动”，优化决策。


## 二、RL问题的核心框架：马尔可夫决策过程（MDP）
### 1. MDP的五大组成部分
|组成部分|定义|示例（机器人训练场景）|
|----|----|----|
|代理（Agent）|需通过RL训练和建模的系统|待训练的机器人|
|环境（Environment）|Agent所处并互动的场景，含各类影响因素|机器人导航的地形、风、摩擦、光照、温度等|
|状态（State）|某一时刻Agent与Environment的整体状况|机器人位置、周围物体位置、风向风速|
|行动（Action）|Agent为互动采取的具体动作|机器人右转、左转、前进、后退、弯腰、举手|
|奖励（Reward）|Environment对Agent行动的反馈，评估行动优劣|机器人撞墙（负面奖励）、找到目标物体（正面奖励）|

### 2. MDP的构建步骤
1. 确定Agent与Environment：明确代理角色、问题范围及所处环境。
2. 定义State：需包含决策所需全部信息（满足“自包含”，即仅依赖当前状态即可决策，无需历史信息；若需历史信息，需将其封装进状态变量）。
3. 梳理Action：列出Agent可执行的所有可能动作。
4. 设计Reward：需贴合目标行为，确保反馈能引导Agent学习期望动作。


## 三、MDP的运行机制
### 1. 以井字游戏为例的MDP定义
- Agent：一方玩家；Environment：对手玩家。
- State：棋盘上双方标记的当前位置。
- Action：9种（选择3x3格子中任一位置放置标记）。
- Reward：赢（+10分）、输（-10分）、合理移动未获胜（0分）。

### 2. 单时间步的运行流程
1. Environment将当前状态（current state）输入Agent；
2. Agent根据当前状态选择行动（无需依赖历史状态/行动）；
3. Agent的行动输入Environment；
4. Environment根据当前状态与行动，转移至下一个状态，并给出Reward（Environment的内在规则对Agent透明，视为“黑箱”）；
5. Reward作为Agent上一步行动的反馈，完成一个时间步，进入下一循环。

### 3. Agent的核心目标
最大化“奖励总额”，既包括即时奖励（immediate reward），也包括随时间累积的奖励总和（cumulative rewards）。


## 四、MDP的任务类型（按时间步迭代特性）
### 1. 回合制任务（Episodic Tasks）
- 有明确终点（终端状态，Terminal State），从起始状态到终端状态的完整序列为一个“回合”。
- 回合结束后可重置至起始状态（或随机选择起始状态），重复新回合，各回合相互独立（如游戏每一轮）。

### 2. 连续性任务（Continuing Tasks）
- 无终点，可永久持续（或至系统停止），如生产管理、仓库自动化机器人的持续工作。


## 五、状态转换与Agent行动决策的关键说明
### 1. 环境的状态转换逻辑
- Agent选择行动后，Environment根据内在规则（如物理法则、市场规律等，视为“黑箱”，可能是真实环境或模拟器）决定下一个状态（含概率性，如相同行动可能进入不同状态）及对应Reward。
- 若构建环境模型，可通过“状态转移矩阵”描述：映射“特定状态+行动”至“下一个状态（带概率）+奖励”。

### 2. Agent的行动决策依据
依赖三个核心概念（下一篇文章详细阐述）：
- 回报（Return）
- 策略（Policy）
- 价值（Value）


## 六、文章背景与参考资料
### 1. 文章定位
- 系列第一篇，聚焦RL基础概念与术语，后续将加入RL在大语言模型中的应用。
- 参考来源：Ketan Doshi博客专栏、李宏毅机器学习讲稿、《动手学强化学习》。

### 2. RL的发展简史
- 起源：20世纪初行为心理学（巴甫洛夫条件反射、斯金纳操作条件作用）。
- 关键突破：2013年DeepMind提出深度Q网络（DQN）解决Atari游戏；后续AlphaGo（围棋）、AlphaStar（星际争霸II）等推动深度强化学习（DRL）发展。

### 3. 参考资料
1. https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060
2. https://www.inping.com/j/1eb82528/
3. https://easyai.tech/ai-definition
4. 李宏毅机器学习课程：https://www.bilibili.com/video/BV1JE411g7XF?p=108
