https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485509&idx=1&sn=88569921ec264a763de849916a7d4d63&chksm=e8af29bfdfd8a0a9cd13b56f5a9d6cfcb2c24d73e7fbc608803000822e5508b65178121844ff&cur_album_id=3386763082605215744&scene=189#wechat_redirect

# 图解强化学习——无模型算法1 网页总结
## 一、文章基础信息
1. **原创与来源**：由Afunby创作，发布于“Afunby的 AI Lab”，发布时间为2024年05月09日18:45，发布地点标注为广东。
2. **内容参考**：基于Ketan Doshi博客专栏，同时参考李宏毅老师讲稿及《动手学强化学习》相关内容，后续章节拟加入强化学习在大语言模型中的应用，旨在以通俗语言助力读者深入理解RL原理与应用。
3. **系列定位**：为强化学习系列第4篇文章，前三篇分别介绍强化学习基础概念（2篇）、算法分类及贝尔曼方程，文中提供了前三篇文章的链接。


## 二、核心内容：无模型算法相关阐述
### （一）无模型算法的两种核心方向
1. **策略与价值函数的关联**
    - 每项策略对应状态价值（V值）和状态-行动价值（Q值）两个价值函数，可通过价值函数比较策略优劣，若Y策略价值函数高于X策略，则Y策略更优。
    - 持续寻找更优策略，最终能得到优于所有策略的“最优策略”，其对应的最优状态值和最优状态-行动值也优于其他价值函数，找到最优策略与找到最优“状态-行动值”等价。
2. **基于策略与基于价值的算法差异**
    - **基于策略（Policy-based）算法**：直接锁定最优策略，无需间接计算。
    - **基于价值（Value-based）算法**：先确定最优状态-动作价值，再由此派生出最优策略，常简称“基于价值的算法”。

### （二）基于价值的算法细节
1. **最优策略的确定方式**
    - 学习过程中，通过与环境交互（如借助Q学习或Sarsa算法）更新Q值，依据环境反馈（如奖励）调整Q值以反映状态-动作对真实价值。
    - 当Q值足够准确时，最优策略显现：在每个状态下选择Q值最大的动作，公式可表示为对应最优策略π*(s)；通常最优策略是确定性的，若两Q值平局，可能变为随机性策略（如对战游戏中，避免动作可预测被对手击败）。
2. **算法的问题解决范畴**：除解决控制问题外，也存在用于解决预测问题的基于价值的算法，该类算法以状态值为基础，或基于状态-动作值、策略。

### （三）无模型算法分类与求解逻辑
1. **分类依据：实现方式**
    - 以策略或值的实现方式为划分标准，较简单算法用查找表实现，较先进算法用神经网络等函数近似器实现，据此可对无模型算法进行分类。
2. **求解特点：迭代解决方案**
    - RL问题无法用代数方法直接求解，需采用迭代算法。
    - 各类基于价值和基于策略的算法虽细节易让人困惑，但均基于共通原则；所有算法均从对目标量的任意估计开始，通过从环境获取数据，逐步优化估计值，且都包含四个基本步骤（下一篇文章将详细阐释步骤并对比两种方法差异）。

## 三、文章总结要点
1. 无模型算法可分为基于价值和基于策略两类。
2. 基于价值的RL算法主要通过更新Q值逐步逼近最优策略。
3. 基于策略的RL算法直接调整每个状态下采取各可能动作的概率，通常用策略梯度技术寻找最优策略。

## 四、其他补充信息
1. **互动相关**：文中包含微信赞赏二维码及“喜欢作者”“写留言”等互动功能，当前显示0人付费，暂无留言（或有1条留言，表述存在小冲突）。
2. **系列导航**：标注了上一篇（图解强化学习-算法分类及贝尔曼方程）和下一篇（图解强化学习——无模型算法2）文章的链接，方便读者按系列阅读。
3. **标签与目录**：带有“强化学习”“图解强化学习”“#技术图解”“图解 Reinforcement Learning”等标签，还提及“强化学习 · 目录”，便于内容检索。
