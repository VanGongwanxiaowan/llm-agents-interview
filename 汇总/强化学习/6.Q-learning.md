https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485747&idx=1&sn=fd9eb47e517103d8c7a83033e865bd5d&chksm=e8af28c9dfd8a1df6d57e20ac81f7c7d2bf2b638bd5b70b9946ee2d1104f0cc09bd228906d0d&cur_album_id=3386763082605215744&scene=189#wechat_redirect

# 图解强化学习——Q-Learning 网页总结
## 一、文章基础信息
1. **原创与来源**：由 Afunby 创作，发布于“Afunby的 AI Lab”，发布时间为2024年05月16日，地点标注为陕西。
2. **内容参考**：基于 Ketan Doshi 博客专栏，结合李宏毅老师讲稿及《动手学强化学习》相关内容，后续拟加入强化学习在大语言模型中的应用。
3. **系列定位**：是强化学习系列第6篇文章，为后续讲解 DQN 算法奠定基础，前5篇文章分别介绍强化学习基础概念（2篇）、算法分类及贝尔曼方程、无模型算法（2篇），并提供前5篇文章链接。


## 二、Q-Learning 算法核心内容
### （一）算法概述
1. **核心工具**：依赖状态-动作值 Q 表（Q 值表），表中每行对应一个状态，每列对应一个动作，单元格为相应状态-动作对的估计 Q 值。
2. **Q 值初始与更新**：初始时所有 Q 值设为0，Agent 与环境互动获取反馈后，利用贝尔曼方程反复改进 Q 值，直至收敛到最优 Q 值。

### （二）Q 表构建
以3x3网格游戏为例：
- **游戏规则**：玩家从“Start”方格出发，目标是“Goal”方格（获5分 Reward），部分方格安全（0分）、部分危险（-10分），玩家可“上下左右”移动。
- **Q 表维度**：共9个位置状态、4种移动方向，构建9行4列的 Q 表，初始值均为0，可查询任意位置-动作组合的 Q 值（如((2,2), up)对应位置(2,2)与动作“up”的 Q 值）。

### （三）算法整体流程
1. **核心目标**：Agent 学习每个状态-动作对的最优 Q 值，进而确定最优策略。
2. **最优 Q 值关键信息**
    - 定义：给定状态𝑠和动作𝑎，能获得的最大预期总回报估计，基于“Agent 后续状态均选最优动作”的理想假设。
    - 作用：是最优策略的基础，给定状态𝑆时，选择 Q 值最高的动作𝑎，所有状态的该动作集合即构成最优策略。
3. **学习过程**：Agent 初始随机选动作，通过与环境交互，依据获得的 Reward 判断动作优劣，逐步更新 Q 值，流程与前序无模型算法文章类似，核心特点在于基于贝尔曼方程变体的 Q 值更新方式。

### （四）时间步中的两种动作
1. **动作选择与反馈**：算法第2步，Agent 用ε-贪婪策略从当前状态（如 S1）选动作（如 a1）执行，获 Reward（如 R1）和下一个状态（如 S2）。
2. **动作在 Q 值更新中的作用**
    - **当前动作**：在环境中实际执行，其对应的 Q 值（如 Q1）会被更新。
    - **目标动作**：从下一个状态的可选动作中选 Q 值最高的动作（如 a4），仅用于计算更新当前动作的 Q 值，不一定在后续时间步执行。
3. **策略类型**：属于“离线策略（off-policy）”学习，因实际执行的“行为策略”（含探索的ε-贪婪策略）与用于学习的“目标策略”（选最优动作的策略）不同。

### （五）Q 表填充与 Q 值准确性提升
1. **Q 表填充起点**：初始 Q 表全为0，Agent 用ε-贪婪策略选动作、获环境反馈后，通过更新公式（结合 Reward）填充 Q 表，使 Q 值逐步接近理论最优值。
2. **Q 值准确性提升原因（第一部分）**
    - 核心逻辑：每经过一个时间步，Q 值估算会结合实际观测的 Reward 更新，变得更精确。
    - 更新公式因素：公式含三个项，其中“获得的 Reward”是具体数据，为 Agent 提供环境实际经验；其余两项为估计值，初始不准确但后续会优化。
    - 实例表现：多次访问同一状态-动作对（如 S3与a1），虽单次 Reward 可能因环境动态性波动，但随回合增加，Reward 逐渐稳定于期望值，Q 值也随之稳定（Q 值即该状态-动作对多次重复的平均总回报）。
3. **Q 值准确性提升原因（第二部分）**
    - 终止状态的带动作用：回合最后时间步（如 T 步），Agent 到终止状态，无目标动作，“max”项为0，终止 Q 值更新完全依赖实际 Reward，准确性提升；后续回合访问终止前状态（如 T-1 步）时，终止 Q 值会“回溯”，带动终止前 Q 值准确性提升。
    - 连锁效应：随回合增加，准确的 Q 值会逐步向更早的时间步（如 T-2 步）传递，路径上的 Q 值均以实际观测为基础优化，整体准确性提高。

### （六）Q 值收敛到最优值
1. **收敛逻辑**：ε-贪婪策略鼓励 Agent 探索更多状态和动作，迭代次数越多，探索路径越广，Agent 越可能尝试所有选项以寻找更优 Q 值。
2. **收敛过程**：每次迭代 Q 值都会改善，迭代足够多次后，Agent 评估所有可能选项，不再能找到更优 Q 值，Q 值逐渐接近最优值（可参考数学证明文章《Convergence of Q-learning: a simple proof》）。


## 三、总结与补充信息
1. **Q-Learning 核心概括（作者版）**
    - 采用离线策略，平衡探索需求与最优 Q 值收敛目标。
    - 收敛性直观理解：回合最后时间步基于实际 Reward 更新 Q 值，其前时间步的 Q 值迭代中，实际 Reward 占比逐渐增大，随时间“回流”至整个路径的 Q 值。
2. **额外资源**
    - 提供 Q-Learning 收敛性证明文章链接：http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf
    - 提供 Ketan Doshi 博客链接：https://towardsdatascience.com/reinforcement-learning-explained-visually-part-4-q-learning-step-by-step-b65efb731d3e
    - 作者邀请读者添加个人微信，共同探讨数据科学、深度学习、大模型相关技术。
3. **系列文章衔接**：本文上一篇为《图解强化学习——无模型算法 2》，下一篇将讲解《图解强化学习 — DQN》。
