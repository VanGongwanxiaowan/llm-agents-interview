https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485551&idx=1&sn=9df50d7ccbd8f08b817349254b0c7054&chksm=e8af2995dfd8a0831f002197d0f3131c955c6fb9f591df7a75588cd48e865d365c185eadecdd&cur_album_id=3386763082605215744&scene=189#wechat_redirect                


# 图解强化学习——无模型算法2 网页总结



## 一、文章基础信息
1. **原创与来源**：由Afunby创作，发布于“Afunby的 AI Lab”，发布时间为2024年05月13日22:11，发布地点标注为广东。
2. **内容参考**：基于Ketan Doshi博客专栏，参考李宏毅老师讲稿及《动手学强化学习》相关内容，后续拟加入强化学习在大语言模型中的应用，旨在以简单语言助力读者深入理解RL原理与应用。
3. **系列定位**：为强化学习系列第5篇文章，前4篇分别介绍强化学习基础概念、算法分类、贝尔曼方程运作原理及无模型算法概述，文中提供了前3篇文章的链接。


## 二、无模型算法的迭代解决方案（核心步骤）
无模型算法中，Value-based（基于价值）和Policy-based（基于策略）算法均有4个基本步骤，具体如下：

### （一）初始化估计
- **基于价值的算法**：初始化“最佳状态-行动价值表”，初始时所有值设为零，因算法对正确值一无所知。
- **基于策略的算法**：初始化“最佳策略表”，表中包含每个状态下每个行动的概率，初始值同样为零。

### （二）执行动作
1. **核心原则**：Agent需在探索（Exploration）与利用（Exploitation）间找到平衡，以尝试所有可能路径并发现最佳选项。
    - **探索**：通过随机选择动作探索新路径，学习初期因对动作好坏无认知，需借此观察奖励以发现优质动作。
    - **利用**：选择当前最优动作以最大化回报，适用于Agent已探索所有可能动作的情况。
2. **不同算法的动作选择方式**
    - **基于策略的算法**：依据持续更新的最优策略表选择动作，表中概率越高的动作被选中的可能性越大。
    - **基于价值的算法**：采用ε-贪婪策略（ε-greedy strategy），通过探索率ε平衡探索与利用。初始ε设为1，之后每个训练周期按一定比率递减；选择动作时，以ε概率随机选择（探索），以“1-ε”概率选当前最优动作（利用），训练初期更倾向探索，后期更多转向利用。

### （三）从环境中获得反馈
Agent执行所选动作后，会以奖励的形式从环境中获取反馈。

### （四）改进估计
1. **基于策略的Agent**：以最大化累积奖励为目标，利用环境反馈改进对目标量及策略函数的估计，若获得正面奖励，会提高刚采取动作的概率，以便下次面临相同情形时更倾向选择该动作。
2. **基于价值的Agent**：结合环境反馈，依据贝尔曼方程完善对最优价值的估计，缩小之前估计值与基于新奖励得出的实际价值间的差距，为后续在相同状态下选择动作提供更准确依据。
    - **贝尔曼方程关键要点**：通过递归关系计算Q值，已知下一步状态-动作对的Q值即可计算当前Q值，无需遍历整个回合；计算状态-动作价值有两种方式（基于当前状态的状态-动作价值、结合即时奖励与下一个状态的状态-动作价值），理论上准确时两种方式结果一致，不一致的“差异”即估计“误差”，算法通过减少误差完善Q值估计。


## 三、算法流程整合与改进估算方法
### （一）流程整合
Agent完成上述4个步骤后，获得更准确的估计值，标志一轮流程结束。算法会持续执行该流程至回合结束，随后开启新回合并重复过程。

### （二）改进估算的关键因素（不同算法差异所在）
1. **频率**：指更新估算前Agent执行的前向步数，有三种选择
    - **回合**：Agent执行动作、记录奖励直至回合结束，利用累积奖励更新预测值。
    - **一步**：每执行一步，基于观察到的奖励立即更新。
    - **N步**：介于前两者之间，执行N步后更新。
2. **深度**：指更新时回溯的步数，有三种策略
    - **回合**：若执行至回合结束，更新路径上所有状态-动作对的估计值。
    - **一步**：仅更新当前状态-动作对的估计值。
    - **N步**：执行N步后，更新对应范围内的估计值。
3. **公式**：更新估计值的数学公式，主要分两类
    - **基于价值的更新**：采用贝尔曼方程的某种形式，引入TD误差等“误差”值更新Q值。
    - **基于策略的更新**：根据奖励质量调整动作概率，奖励好则提高对应动作选择概率，反之降低。


## 四、无模型算法间的关系与文章总结
1. **算法关系**：不同无模型算法虽各有差异，但遵循共通设计模式；文章不覆盖所有算法，重点介绍深度强化学习中常用算法，并提及编制表格概述算法间联系（文中含表格图片）。
2. **文章总结**：本文及上一篇无模型算法文章概述了RL问题的解决方案，探讨了常用算法的共同主题与技术，下篇文章将深入讲解基于深度学习技术的RL算法。
