https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485784&idx=1&sn=983cb1c3628a9198646fa3999993082f&chksm=e8af28a2dfd8a1b4589638ed7376958d640a17e805a4e71222349f5c2495e2161804eedf7402&cur_album_id=3386763082605215744&scene=189#wechat_redirect


# 图解强化学习——DQN网页总结
## 一、文章基础信息
1. **创作背景**：本文是强化学习系列第7篇，基于Ketan Doshi博客专栏、李宏毅老师讲稿及《动手学强化学习》内容创作，后续拟加入强化学习在大语言模型中的应用，旨在以通俗语言助力读者深入理解RL原理与应用。
2. **关联文章**：前文围绕强化学习基础概念、算法分类、贝尔曼方程、无模型算法及Q-Learning展开，可通过文中链接查看前6篇内容（[图解强化学习——基础概念 1]()、[图解强化学习——基础概念 2]()等）。


## 二、DQN核心概述
1. **Q表的局限性**：Q-Learning通过（状态数s，动作数a）维度的Q表映射“状态-动作”对与Q值，但现实问题中状态数量庞大，构建完整Q表在计算上不切实际。
2. **Q函数的解决方案**：用Q函数替代Q表，实现“状态-动作”对到Q值的映射，以应对复杂现实问题。
3. **DQN的本质**：采用深度Q网络（DQN）这一神经网络拟合Q函数，将状态映射到所有可选动作的Q值，通过学习网络权重预测最优Q值，核心逻辑与Q-Learning相似（从随机Q值估计开始，用ε-贪婪策略探索环境），但依赖对当前动作和目标动作Q值的双重评估优化Q值估计。


## 三、DQN架构组成
| 组成部分 | 核心功能 |
|----------|----------|
| Q网络 | 训练目标，学习生成最优状态-动作值；若状态为数值集合，可用带隐藏层的线性网络，若为图像/文本，可采用CNN/RNN架构 |
| 目标网络 | 不参与训练，用于预测目标Q值，保持目标Q值一段时间内稳定，预设时间步后复制Q网络权重更新自身 |
| Experience Replay（经验回放） | 与环境互动生成训练数据（样本格式：当前状态St、动作at、奖励Rt、下一个状态St+1），随机抽取批次样本供网络训练 |


## 四、DQN工作流程
### （一）整体流程（单时间步）
1. **收集训练数据**：Experience Replay从当前状态St以ε-贪婪策略选动作at，执行后获奖励Rt和下一个状态St+1，保存（St，at，Rt，St+1）为样本。
2. **双网络预测Q值**：从Experience Replay随机抽批次样本，Q网络预测“预测Q值（Predicted Q Value）”（基于样本当前状态和动作），目标网络预测“目标Q值（Target Q Value）”（基于样本下一个状态的最优动作Q值）。
3. **训练Q网络**：结合样本的预测Q值、目标Q值和奖励计算损失，反向传播更新Q网络权重，目标网络不参与训练。

### （二）详细流程（多时间步+多回合）
1. **初始化阶段**：与环境互动若干次以启动Experience Replay数据收集；Q网络用随机权重初始化，并将权重复制到目标网络。
2. **数据生成阶段**：从首个时间步起，Experience Replay利用Q网络（扮演Agent）选ε-贪婪动作，与环境互动生成（St，at，Rt，St+1）样本并保存，此阶段不训练DQN。
3. **训练阶段**：
    - 从Experience Replay随机选批次样本输入双网络；
    - Q网络预测样本当前状态下所有动作Q值，选取样本动作对应的Q值作为预测Q值；
    - 目标网络预测样本下一个状态下所有动作Q值，取最大值后加样本奖励，得到目标Q值；
    - 计算预测Q值与目标Q值的损失，通过梯度下降反向传播更新Q网络权重，目标网络保持不变。
4. **权重更新**：重复上述流程至预设时间步T，将Q网络权重复制到目标网络，之后继续循环。


## 五、关键设计的必要性
1. **为何需要Experience Replay？**
    - 解决样本相关性问题：连续动作数据高度相关，非独立同分布数据会导致网络拟合近期数据，经验回放随机抽取样本打破相关性，满足独立假设，助力网络学习通用权重。
    - 提高样本效率：存储的样本可多次使用，适配深度神经网络梯度学习需求。
    - 稳定训练：批量样本训练避免单样本梯度波动过大，使网络权重稳定收敛。
2. **为何需要目标网络？**
    - 若仅用Q网络，其权重每时间步更新会导致目标Q值预测方向变化，使预测Q值不稳定（类似“追逐移动目标”）。
    - 目标网络不训练，确保目标Q值一段时间内稳定，预设时间步后更新权重，平衡稳定性与Q值预测准确性，研究表明可提升训练稳定性。


## 六、DQN核心思想与补充
1. **核心思想总结**
    - 个人总结：用神经网络拟合动作价值函数Q（s,a），以时序差分误差为损失函数训练网络；用经验回放实现样本独立假设、提高样本效率，用目标网络减少目标Q值波动。
    - GPT4.0总结：用深度神经网络近似“状态-动作”对Q值，处理高维状态空间问题；通过时序差分学习最小化预测Q值与目标Q值误差优化参数；经验回放打破样本相关性、提升稳定性与数据利用率；目标网络稳定学习目标，减少训练振荡，助力收敛。
2. **时序差分误差与损失函数**：时序差分误差是“当前预测Q值”与“贝尔曼方程得到的目标Q值”的差异，在DQN中用于计算损失函数，进而通过反向传播更新网络权重，使预测Q值接近目标Q值。


## 七、结论与后续方向
1. **DQN与Q-Learning的关联**：二者均基于预测Q值、目标Q值和奖励优化，Q-Learning用公式调整Q值，DQN（神经网络）用损失函数计算并训练提升预测能力。
2. **后续内容**：将深入探讨深度强化学习，研究另一种热门算法——策略梯度。
