https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485471&idx=1&sn=f65a0a9994cfd3b1cfb8194a59fa0fc5&chksm=e8af29e5dfd8a0f3f730600e61c1ff94508d076bedb20669c983ed97aeb4971bf92980c01019&cur_album_id=3386763082605215744&scene=190#rd

# 微信公众平台文章《图解强化学习-算法分类及贝尔曼方程》总结
## 一、文章基础信息
- **标题**：图解强化学习-算法分类及贝尔曼方程
- **作者**：Afunby（来自“Afunby的 AI Lab”）
- **发布时间**：2024年05月06日 07:58
- **发布地点**：广东
- **内容基础**：基于Ketan Doshi博客专栏，参考李宏毅老师讲稿及《动手学强化学习》，后续拟加入强化学习在大语言模型中的应用，旨在用简单语言让读者深入理解RL原理与应用
- **系列定位**：《图解强化学习》系列第三篇，前两篇为《图解强化学习——基础概念 1》《图解强化学习——基础概念 2》


## 二、强化学习（RL）问题解决方案分类
### 1. 按“基于模型 VS 无模型”分类
|分类|适用场景|核心特点|
|----|----|----|
|基于模型（Model-based）|已知环境内部运行机制，能准确判断当前状态执行动作后的下一个状态和奖励|无需与环境实际交互，通过分析即可找到解决方案，如将棋局规则和策略编码进程序下棋|
|无模型（Model-free）|环境复杂、内部动态未知|将环境视为“黑盒”，依赖与环境直接互动试错，观察动作后的状态变化和奖励来学习，如下棋时对规则一无所知，靠观察动作和奖励抽象学习|

### 2. 按“预测 VS 控制”分类
- **预测问题（Prediction problem）**：给定某一策略（不限最优策略），目标是预测使用该策略能获得的累积回报（即价值函数）
- **控制问题（Control problem）**：不提前给定策略，通过探索尝试，在所有可能策略中找到能获取最大回报的最优策略，多数实际RL问题属于此类

### 3. RL算法分类与聚焦方向
- 结合上述两种分类可对常见RL算法进行划分
- 现实中大部分RL问题属于“无模型的控制问题”，因此文章系列后续将聚焦此类问题，对基于模型的解决方案仅做简略讨论


## 三、无模型方法（Model-free Approaches）核心内容
### 1. 与环境互动机制
- 算法扮演Agent角色，因环境内部机制不可见，需通过直接互动“观察”环境行为
- Agent每执行一个动作，记录环境反馈（下一个状态和对应奖励），并依据反馈不断调整策略，是持续学习适应的过程，类似人类通过试错获取经验

### 2. 训练数据来源
- Agent与环境互动时，每个时间步的动作会形成特定路径（即“轨迹”）
- 这些轨迹是算法学习和训练的基础数据


## 四、贝尔曼方程（强化学习数学基础）
### 1. 核心原理：从终止状态回溯
- 若Agent从某状态出发，仅执行一个动作就到达终止状态，该状态的回报等于执行此动作获得的奖励（如G₇=R₇）
- 对于终止状态前的状态（如S₆），其回报=到达下一个状态的动作奖励+下一个状态回报的折扣值（如G₆=R₆+γR₇=R₆+γG₇，γ为折扣因子）

### 2. 三种关键方程形式
|方程类型|核心拆解逻辑|关注重点|
|----|----|----|
|回报的Bellman收益方程|任何状态的回报=当前状态到下一个状态的即时奖励+从下一个状态按特定策略行动的未来折扣回报|单一路径上的累积奖励|
|状态价值函数的Bellman方程|状态价值（平均预期回报）=当前状态执行下一个动作的即时奖励+时间折扣后、从下一状态遵循策略的价值|多条路径上的平均预期回报，综合各种可能动作的回报水平|
|状态-动作对价值函数的Bellman期望方程|状态-动作价值=达到下一状态的动作即时奖励+后续步骤遵循策略获得的下一状态折扣价值|状态与动作结合后的价值|

### 3. 贝尔曼方程的作用
- **递归计算回报**：无需走完可能漫长、高成本或无止境的回合，若知道下一步回报，可利用递归关系计算当前状态回报，仅需迈出第一步并观察回报，再结合后续时间步回报即可
- **基于估算优化结果**：计算回报有两种方式——直接计算当前状态至回合结束的实际回报（计算代价高，常用估计值）、将单步奖励与下一状态回报相加；通过对比两种估算结果的误差，可修正估算，改进结果（所有RL算法均会用到此逻辑）


## 五、后续内容预告
文章已让读者初步了解RL问题及解决方案，后续将深入探讨解决RL问题的技术，且因实际中无模型方法应用广泛，会聚焦无模型算法展开讨论
