https://mp.weixin.qq.com/s?__biz=MzIzMDc2Njc0MQ==&mid=2247485852&idx=1&sn=42df79c7def4017e11fa504ed941492d&chksm=e8af2866dfd8a170c2d7a831210702627772fcfa5b0d85f372b5b29672618946c9b347017e56&cur_album_id=3386763082605215744&scene=189#wechat_redirect

# 图解强化学习——策略梯度 网页总结
## 一、文章基础信息
1. **创作背景**：基于Ketan Doshi博客专栏，参考李宏毅老师讲稿及《动手学强化学习》，后续拟加入强化学习在大语言模型中的应用，以通俗语言助读者深入理解RL原理与应用。
2. **文章定位**：强化学习系列第8篇，前7篇已讲解RL核心概念、基于价值的Q-Learning和DQN算法，本文聚焦首个基于策略的算法——策略梯度（Policy Gradients），解析REINFORCE算法及与DQN的差异。
3. **发布信息**：原创作者Afunby，发布于“Afunby的 AI Lab”，2024年05月22日23:12，发布地广东。


## 二、策略梯度核心概念
1. **核心思想**：设计神经网络直接学习最优策略，输入为当前状态，输出为该状态下各可能动作的概率分布，而非动作的Q值，通过对概率分布抽样选取动作。
2. **策略网络（Policy Network）**：
    - 架构：标准神经网络，状态为数字变量时可用线性层+隐藏层；状态为图像/文本时，可结合CNN（卷积神经网络）或RNN（循环神经网络）。
    - 功能：接收当前状态，预测该状态下应采取动作的概率分布。


## 三、工作流程
策略梯度训练含多轮，每轮分两个阶段，且训练数据为（状态，动作，奖励）三元组：
1. **第一阶段：数据收集**
    - 策略网络作为Agent与环境互动，按当前状态预测的动作概率分布随机抽样选动作执行。
    - 从环境获取对应奖励和下一个状态，保存数据样本，此阶段策略网络配置固定，不做改进。
2. **第二阶段：网络训练**
    - 将收集的数据送入策略网络，网络根据样本中状态和动作预测执行该动作的可能性。
    - 提取样本回报计算折扣回报，结合动作概率算损失，通过学习让策略网络逐步预测更优动作，直至找到最优策略。


## 四、相较于基于价值方案的优势
|优势类别|具体说明|
|----|----|
|处理动作数量能力|基于价值方案输出所有动作Q值，动作极多（如连续动作）时受限；策略梯度直接输出动作概率分布，可高效处理大量动作问题|
|学习策略类型|基于价值方案选Q值最高动作，仅能学确定性策略；策略梯度可学确定性与随机策略，Agent抽样选动作，行为具多样性|
|训练效率|策略梯度直接确定策略，通常训练时间比基于价值方法更高效|


## 五、探索机制
1. 策略梯度通过抽样预测的概率分布选动作，能自然实现探索与利用平衡，最终会采取多种不同动作。
2. 训练初期：动作概率分布均匀，以“探索”为主，因不知哪些动作更优。
3. 训练过程中：概率分布逐渐集中到最佳动作，转向“利用”；若某状态仅一个最佳动作，概率分布成为退化分布，最终形成确定性政策。


## 六、损失函数
1. **设计目标**：让策略多采样高Q值动作，少采样低Q值动作。
2. **关键构成**：损失函数为 **-log (P(a)) * Rt**，其中P(a)是动作a的概率，Rt是动作a的折扣回报。
    - 用-log (P(a))：避免数值溢出，且P(a)接近1时其值接近0，可鼓励网络调整权重提高优质动作概率，效果等同于1 - P(a)。
    - 引入Rt（折扣回报）：为不同动作分配不同权重，回合中获更多奖励的动作权重更高，按折扣回报比例更新动作概率。


## 七、详细操作（以REINFORCE算法为例）
1. **数据收集细节**：从初始状态开始，策略网络输动作概率分布→抽样选动作→环境生成下一状态和奖励→下一状态反馈给网络，保存样本，此阶段网络权重固定，直至回合结束。
2. **训练批次类型**：
    - 单回合批次：训练样本仅来自一个完整回合，从初始状态到终止状态，数据组成一个批次，梯度更新与单一策略结果关联紧密，但回合长或差异大时训练方差高。
    - 多回合批次（更常见）：样本来自多个完整回合，汇总数据形成大数据集做一次梯度更新，能提高学习稳定性，平均单个回合的噪声或异常。
3. **损失计算步骤**：
    - 输入回合（单/多回合）中每个状态到策略网络，预测所有可能动作的概率分布。
    - 获取每个状态下实际采取动作的概率（同一状态不同动作需单独计算）。
    - 基于奖励加权总和计算每个样本的折扣回报。
    - 计算动作概率对数与折扣回报的乘积并取负，累加所有状态的负值得到损失，用于训练策略网络。
4. **循环训练**：下一轮重复“数据收集-概率预测-损失计算-策略梯度更新”流程。


## 八、策略梯度重要思想总结
### （一）作者总结
1. 策略梯度神经网络输入环境状态集合s，输出状态s下各动作概率p（a），Agent依概率随机选动作（非选概率最高动作），借随机性探索，助力找到长期累计奖励最大化的策略。
2. 损失函数设计与梯度更新：使高回报低概率动作概率增加，低回报高概率动作概率减少；通过动作概率对数加权（权重为动作回报）实现，让网络逐渐学到推断任意状态最佳动作的函数。
3. 一轮训练含数据收集、概率预测、损失函数计算、策略梯度更新四个主要过程。

### （二）GPT4.0总结
1. 直接优化策略：不依赖价值函数估计，直接优化策略参数，最大化环境总体回报，通过调整策略改进行为。
2. 策略表示：策略由参数化模型（如神经网络）表示，输出给定状态下各动作概率，适配连续动作空间或高自定义决策场景。
3. 梯度估计：靠样本估计，收集当前策略生成的轨迹（状态、动作、奖励序列），算轨迹回报，结合动作概率对数估算策略梯度。
4. 探索与利用：依概率分布随机选动作，自然平衡探索（试不同动作获更多信息）与利用（选已知最佳动作），避免局部最优，助于学更广泛策略。
5. 方差减少技术：因梯度估计方差高，常用基线（如状态值函数）、优势函数（动作相对平均优势）及Actor-Critic等复杂变体降低方差，提升学习稳定性与效率。
6. 应用灵活性：直接优化策略，适用于奖励信号稀疏/延迟的环境，可处理高维/连续动作空间，用于机器人控制、游戏玩耍等复杂决策任务。


## 九、后续内容预告
下一篇文章将深入学习更先进的RL解决方案——Actor-Critic方法，该方法创新结合基于策略与基于价值两种策略的优势构建。
