「Test time scaling, Reasoning」论文：

What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models

LLM “Thinking” 时代，本质上可以说是 Test-Time Scaling（TTS） 的时代。

这是一篇非常出色的 survey，从 what、how、where、how well 四条线出发，梳理了 TTS 领域的发展脉络。

我个人非常喜欢关于what和how to scale的内容。其中附带的 mind map 信息量巨大，值得保存，作为查阅 TTS 方法与思路的工具。

- What to Scale

Test-Time Scaling 最核心的问题是：“在推理时，我们到底能扩展什么？” 作者将其归纳为四类：

Parallel scaling：并行生成，如 self-consistency、multi-agent、majority vote。
Sequential scaling：例如 ReAct、ReWOO、Plan-and-Solve，强调 step-by-step 的规划式推理。
Hybrid scaling：Tree-of-Thoughts 是代表，通过组合多个推理路径与规划节点，形成混合式决策图。
Internal scaling：如 DeepSeek R1、OpenAI o1 等模型，内部集成 Verifier、Evaluator 或 Planning Module。

- How to Scale 

这一部分是我个人非常喜欢的，尤其是 inference 分支，囊括了许多可以直接借鉴来构建 agent workflow 。比如prompt 、decode、self-repetition、MoA 等，都可以用在纯 prompting 的agent场景中。

Tuning 部分（如 SFT、DPO、RLHF），其实现在更常见的术语是 Post-training，其实叫tuning很好，能反映目前post training的task specific的基因属性和局限性。
paper：
http://arxiv.org/abs/2503.24235
