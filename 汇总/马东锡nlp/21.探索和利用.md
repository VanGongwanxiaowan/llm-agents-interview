让我对 Agent 决策中的两个基本核心策略有了更深的理解：Exploration (探索) 与 Exploitation (利用)。

Exploration 是主动尝试那些价值尚不明确的动作，意味着冒险去探索未知的可能性；
Exploitation 则是利用当前认定回报最高的动作，以稳定兑现已有的知识。

在 Agent 的决策过程中，这两者总是在相互博弈与平衡中。过度的 Exploration 会使 Agent 迷失于不断的试错而难以取得实质收益；而过于偏向 Exploitation，则可能困于局部最优，限制长期收益。

在 Test Time Scaling 方法中，训练阶段使用 RL 微调策略，使其在推理时更愿意展开探索。

实质上，TTS 是鼓励增加 Reasoning 或 Thought 空间中的 Exploration，利用奖励机制强化正确的推理路径，最终让 Agent 更有效地 Exploit，精准选择成功概率更高的推理轨迹。

Exploration 当然可以发生在不同的维度，既可以停留在 LLM 自身的推理空间里，也可以突破自身边界。

特别是在 Agent 场景中，通过与外部环境进行交互（如Tool Call），获得全新的环境反馈，从而在更真实的条件下完成 Exploration，动态优化决策路径，而这就是 Test Time Interaction Scaling 的思想。
