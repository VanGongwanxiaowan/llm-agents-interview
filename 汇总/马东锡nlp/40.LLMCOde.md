「ByteDance, LLM, Code」

Seed-Coder: Let the Code Model Curate Data for Itself

谁来定义“好数据”？答案不再是人，而是“小模型”——这就是 model-centric。

“a model-centric data pipeline that minimizes human involvement”。

一篇精彩的工作，不只是论文，核心贡献是 model-centric data pipeline。

把数据构建的中心，从人类（human-centric）换成轻量 LLM（model-centric）：让模型自己给原始数据打分、过滤、抽样，再把挑出的优质数据反过来训练大模型；整条流程几乎没有手写规则。

Pipeline 强调“工程流程”，把上述 model-centric 各环节嵌进一条端到端，可并行的 data pipeline。

在 training recipe 中，个人最喜欢作者设计的继续 pretraining。除了传统的 next-token prediction，又加入两项重要的预训练任务：

- Fill-in-the-Middle (FIM)：将一段代码随机切成后缀 + 前缀，要求模型还原中间内容。
- Commit 预测：给 commit message + 上下文  -> 预测修改文件路径与 patch,  显式教会代码演化知识。

最后结果，8 B 级别即达到 SOTA！

Link:

https://github.com/ByteDance-Seed/Seed-Coder/tree/master
