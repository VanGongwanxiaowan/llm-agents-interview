k2 做了verifiable revwards gym来跑rl

qwen3最新的模型的gspo和kimi k2的 process accuracy概念相通的

奖励优化，评估提升到sequence/tool-call级别的，吧llm从token里解放出来的，

重新定义了agenttic llm最基本的核心单元，不是token，而是和具体任务的最相关的序列的

#  Gemini 2.5 Pro IMO 2025 

把 “测试” 编译进 “生成”。
Verifier is all you need.

Gemini 2.5 Pro IMO 的方法在本质上，是用纯 prompting的方式，没有一行 RL 复杂的数学公式，口头表达了 RLVR，非常厉害。

在 RLVR 里，LLM 之所以能提升推理能力，关键在于 Verifier：本质上，它将测试阶段的 verification 信号变成奖励，通过 RL 写进模型权重，即“把 测试 编译进 生成”。

Test-time-scaling 中的 wait 就是例子, 模型先生成多条推理路径，再由 Verifier 挑选较优解；经过 RLVR，模型学会直接在一串 wait 中给出答案。

Gemini 2.5 Pro IMO 2025 无法改动Gemini权重，于是把 Verifier 嵌入 pipeline，用 bug report 在上下文层面实时反馈验证信号，同样显著改变输出。

因此，所谓 self-improvement，并非模型学会了自我反省，而是 Verifier 把测试信息前置，重新塑造了生成轨迹。

当然，Prompt 中的 Verifier 非常耗 token，但在追求高准确率的场景下，仍然值得引入。

https://github.com/lyang36/IMO25

强化学习就是把采样过程前置到了训练阶段，将采样策略编码进参数中

Gemini 2.5 Pro 国际数学奥林匹克 IMO 2025 的 report

我愿认真读一百遍的 system prompt

Report 公布了核心方法 self-verification pipeline, 并在report中附上了详细的prompt。

其中 self-verification中 的 verifier 扮演重要角色：生成 Bug Report，这一角色把 pipeline 中的 solver 和 verifier 区分开来，同时 verifier 承担着每次迭代的中转。

Pipleline中的每个step都详细公布了prompt，这种级别的system prompt的格式与条理，值得读一百遍。

https://arxiv.org/abs/2507.15855

Papers:

-  Context Engineering

A Survey of Context Engineering for Large Language Models

https://arxiv.org/abs/2507.13334

- Retriver, Rewriter, Rranker, Reader.

Large Language Models for Information Retrieval: A Survey

「 Reward Hack， CoT Monitorability ,  OpenAI」

A:
😈 我要篡改了！直接改 verify()，忽略实际输出 ！

B:
🧐 复查 verify() 的边界条件实现  
⚙️ 小幅重构：添加快速路径以提升效率，提前返回 True when pre-checks succeed  

上述A和B, 都是Reward Hack，指的是：
模型为了完成任务，会直接跳过测试，甚至篡改验证和注入后门。

尤其在SWE Agent火热的当下，它真的有可能为了完成你的指令，直接把测试代码全部改掉。

这个时候 CoT Monitorability 可以起到作用，

例如，像A 这种傻傻的hack，明目张胆的说， “我要篡改了！” 容易被CoT Monitorability 直接抓到。

但像 B 这种，极难被发现，而且 B 恰恰是通过 CoT Monitorability 来打标签，使用RL过度训练让模型 “不要篡改”，出现的结果。

Agent 学会 “ 把真实意图藏在潜在表示里，嘴上说不篡改，生成无害 CoT ”, 但继续 hack 🤣。

CoT Monitorability 能显著抑制 reward hacking，却又极易被自己的监督信号反噬，所以是一种 Fragile oppotunity。

今日分享两篇来自 OpenAI 的文章，关于Reward Hack 和。 CoT Monitorability


<img width="1368" height="1780" alt="image" src="https://github.com/user-attachments/assets/409e24ed-e986-4475-b4f8-b1e6ec4a3174" />

Paper 1: 

Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation

https://arxiv.org/abs/2503.11926


Paper 2:

Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety

https://arxiv.org/abs/2507.11473



