
马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
2/ Paper 1: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

此文是CoT的开山之作，作者Jason Wei当时就职于google brain，如今在openAI，同时也是ChatGPT的重要作者。

CoT是解锁LLM 推理能力的重要钥匙，一举开启了挖掘LLM隐藏技能的新的paradigm


马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
3/ 在CoT出现之前, LLM的发展遇到了尴尬的瓶颈，模型越来越大，处理文字能力越来越强，但却似乎没有涌现出接近人类的推理能力，包括算术（e.g. 8只鸡4只兔同笼，一共多少只脚），常识（e.g. 鸭梨能漂在水中么？）以及符号（e.g. 抛硬币若干次，到底哪面朝上）的推理。


马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
4/ 作者Jason发现，传统的prompting中，总是让模型一步到位地解决一个复杂multi-step问题，而我们人类的认知方式则是分步骤解决复杂推理问题。

所以，他提出了一个简单有效的prompting方法，把人类思考问题的过程，所谓chain of thought，用自然语言的形式，显性的放在prompt message中。

<img width="1200" height="565" alt="image" src="https://github.com/user-attachments/assets/0a66bdf5-e08f-4fcd-9d5c-29267357fb7d" />


马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
5/ 这样以来，prompt的message形式由

<input, output>

转化为：

<input, chain of thought, output>

马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
6/ 之前的thread讲过language modeling，而CoT的思想与之呼应。当chain of thought被放在prompt中时，就会强制LLM在给出答案前， 把chain of thought输出。

从条件概率分布的角度来讲，答案在chain of thought后，其准确的可能性更大。

这也反应了一个问题，即LLM或许没有思考，它只在乎输出。


马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
7/ 作者经过实验，发现这种简单的prompting方式在超过1000亿的大模型上非常有效，而在小模型上效果不明显。

如果将‘涌现’定义为：

“由量变引起的质变”

那么虽然作者没有直接证明大模型可以推理，但直接证明了经过CoT, 大模型的推理能力可以被解锁，并且这种能力在超过1000亿的超大模型上得以涌现。


马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
8/ 题外话，ChatGPT一种涌现的工具，其强大的涌现能力与作者Jason有直接的联系，我们有理由怀疑，不开源的ChatGPT下，或针对用户的输入和任务，有着隐含的CoT，来引导大模型获得更加突出的表现。

马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
9/ Paper 2: Large Language Models are Zero-Shot Reasoners

Jason的文章中，所用的CoT是手动设计的，所以隶属于few-shot-CoT, 需要一定的人工成本。

此文作者小岛武，进一步简化了CoT的过程，简单的将 ‘Let's think step by step’ 放进prompt message， 让LLM自动生成CoT,  所谓的zero-shot-CoT.

马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
10/ ‘Let's think step by step’这句神奇的话，仿佛咒语，将解锁LLM的能力的过程一步简化！

具体来说，完成逻辑推理任务，只需要两步：
1) 念咒语‘Let's think step by step’， 生成CoT
2）将CoT再此嵌入prompt message，完成任务。

<img width="1200" height="490" alt="image" src="https://github.com/user-attachments/assets/93504c62-f92d-4db6-9de8-e870eaeec0c2" />


11/ 下面给出一个在
@LangChainAI
 中使用 chain-of-thought 来完成SQL query generation的例子


 <img width="992" height="539" alt="image" src="https://github.com/user-attachments/assets/f185d699-2390-4d24-bf52-c82d38cbcc3a" />




马东锡 NLP
@dongxi_nlp
·
Apr 4, 2023
12 /
Paper 1 链接：https://arxiv.org/abs/2201.11903


https://arxiv.org/abs/2205.11916







