LRM论文选读：START: Self-taught Reasoner with Tools

这篇文章介绍了一种构建特定任务、私域Large Reasoning Model的有效方法，主要方法是微调，而且没有使用reinforcement learning，但却有非常类似reinforcement learning with human feedback的特征。

关键词有二：

Test-time scaling
Rejection sampling fine-tuning（反思微调）

<img width="1200" height="731" alt="image" src="https://github.com/user-attachments/assets/932a4266-0b30-4593-a3f2-5b14603842fd" />

关键词一： Testing time scaling

简言之，就是通过强行增加LLM在testing或inferencing阶段的计算力，说白了就是强行增加reasoning长度，来增强LLM的reasoning能力。

具体关于Testing time scaling可以参考我之前的分享。


马东锡 NLP
@dongxi_nlp
·
Mar 20
该文章的testing time scaling，同样运用了大量强行让llm继续输出的词汇，如‘wait’， ‘alternatively’， 来构建多reasoning路径的微调数据集

<img width="971" height="481" alt="image" src="https://github.com/user-attachments/assets/717ab229-71bf-4be6-a5c8-2434b250be03" />




马东锡 NLP
@dongxi_nlp
·
Mar 20
插播一条，有读者问我，在large reasoning model时代，prompting的最佳实践是什么，是像以往那样写CoT，还是直接vanilla prompting不影响大模型本身的reasoning过程。

我的目前答案是，prompting需要跟LRM的post-training过程一致。如果LRM的post-training中人为地大量添加以「wait」分割的不同路径，那么我们的few-shot也要显性地包含以「wait」分割的多个CoT，以此增强LRM对few-shot的理解。


关键词二： Rejection sampling fine-tuning 反思微调

提示微调（Hint-RFT）
首先对代码/数学数据生成推理结果；再通过规则评分与筛选，构建数据集，微调得到START-0，使其具备自我感知和工具使用能力。

反思微调（RFT）
使用START-0自生成多样化的推理轨迹，构建数据集DSTART，并进一步微调模型，最终得到START模型。

这个过程非常像RLHF, 首先需要SFT, 只不过第二步把RL换为RFT. 

关于RLHF，未来会专门介绍给大家。


https://arxiv.org/abs/2503.04625
