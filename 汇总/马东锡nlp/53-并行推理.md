「Reasoning, Agent」论文

Learning Adaptive Parallel Reasoning with Language Models

当 prompt 成了 Launch Kernel ？APR 让 LLM 学会何时分裂多线程、何时回收串行，使推理摆脱线性束缚。

为什么要“并行推理”？
串行 CoT：一步一步写思路 -> 长 token 序列既拖慢推理，又挤爆 context。
多进程（best-of-N / self-consistency）：多个进程各算各的 -> 重复劳动，没有协作。

而APR 目标就是解决上述问题：
边生成、边决定“并行 or 串行”，动态编排reasoning搜索树。

在思想方法上，APR制定了两个核心指令， spawn() 和 join()：

- spawn()：把当前上下文切成多份，平行开子线程
- 子线程并行解题子线程独立探索子问题
- join()等待子线程返回 “结果摘要” 再继续

在这里，不禁感叹，这多像 OS 多线程：父线程把 CPU 时间分配出去，等子线程回来汇报，再沿着最优路线往下走。

在训练方法上：使用SFT+GRPO RL
SFT: 先教模型学会写“何时 spawn / join”的范例。
GRPO RL：在线环境里滚动执行，奖励 = 任务成功率 + 计算效率。

APR 不只加速，还在 token、time、context 多维资源上取得良好表现。

一些思考：

论文实验集中在 数学 Countdown；其他复杂任务（planning, coding）仍待验证。真正跨任务泛化时需要
额外示例或 RL 微调。

这篇论文的最大意义，让我联想到: 如果 spawn() / join() 能像 CUDA <<<grid, block>>> 那样随手就发，LLM 就像一颗操作系统内核，负责调度、隔离、回收算力与上下文。

LLM 推理，从串行单核的冯诺伊曼，踏进并行共享内存的 SMP，再跃迁到多节点协同的分布式，而所有硬件几十年的范式迁移，如今被折叠进一行 prompt 的 spawn() / join() 中，由语言本身驱动完成。


https://arxiv.org/abs/2504.15466

