
Post

See new posts
Conversation
马东锡 NLP
@dongxi_nlp
这条推特解释一下动辄几千亿参数的语言模型中的参数是什么。很多人以为每个参数都是不一样，所以觉得千亿非常浩渺。但实际，现在的语言模型都是基于transformer基本模型，而每一层transformer是由基础的attention，以及前馈神经网络组成。
我尽力在这条长推特中解释GPT千亿参数中每个参数的具体位置


GPT大概的架构layers 如下：
input embedding layer  
->positional embedding  
->transformer layers ( GPT3 has 96 layers)
(attention -> norm ->attention->norm->ffn->norm)


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
Input embedding layer：
当我们用向量代表一个词的时候，需要多维度才能抓到这个词的语言信息。
如经典的300维word2vec公式,：
vec(king)-vec(man)=vec(queen)-vec(woman)
在GPT-3中，这个纬度很高，让我们记为2048维。我们需要学习所有词汇量（记5万词汇），所以此层需要 2048*50000=102400000

马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
其次positional embedding layer:
一句话中的词汇之所以有含义是由于其位置决定，或者说词汇的语言功能和词性由其在句子中的位置决定。
于是，在input embedding之后，positional embedding用来描绘词汇的时间序列特性。
此层维度需要跟input一致，因为总参数为2048*2048=4194304


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
接下来，是transformer的核心层，在此层，有两个attention 层，外加一个前馈神经网络层FFN。基本架构是：attention -> norm ->attention->norm->ffn->norm


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
通过attention层，词与词之间的关联被抓取。这种抓取是通过三个向量qkv做向量相乘以及加权和。
此层需要参数3*2048*（12288+1)=75503616个参数。其中12288是qkv的向量权重维度，1是bias维度。算好的attention还经过12288*12288的linnear层送往下一层FFN。

马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
此时，总结attention层的参数为
3*2048*（12288+1)+12288*12288=352333824。


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
FFN层：分别是12288*（4*12288+1）以及 4*12288*（12288+1）。此处第一个FFN的4是为了再次扩大维度，增加信息量。此层需要参数为1208020992。


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
前提到，transformer层需要2个attention层+1个FFN层：
故需要2*352333824 + 1208020992=1912688640个参数。


马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
另外transformer层中在算完attention和FFN之后分别要加layer norm，所以需要参数为12288*2*3为73728个参数。
所以，整个transformer block需要1912688640+73728=1912762368个参数。



马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
而GPT-3共有96层transformer，所以1912762368*96，我们得到183625187328个参数。



马东锡 NLP
@dongxi_nlp
·
Mar 13, 2023
我们把所有transformer层加上开始介绍的embedding和position的参数，183625187328 + 102400000 + 4194304=183731781632个参数。
简化数字，183billion参数。





















