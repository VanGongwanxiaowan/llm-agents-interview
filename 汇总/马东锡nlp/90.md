
马东锡 NLP
@dongxi_nlp
·
Nov 7, 2023
OpenAI目前放开的唯一缺口是function calling，给开源的专家模型一丝机会。
但在开源模型的reasoning和调度能力匹配OpenAI模型之前，除非深度定制，不然技术上不存在完全private的enterprise GPT，无法摆脱OpenAI的控制。
开源大模型的reasoning能力，只能依赖大厂。
抛开眼花缭乱的prompting engineering，普通AI从业者要牢牢握住简单几样能力，即CoT/ReAct Prompting, 大语言模型Lora微调，小语言模型传统微调，云能力。


马东锡 NLP
@dongxi_nlp
·
Nov 7, 2023
OpenAI的assistant直接把Augmented Language Models（Agent, Tool + Retrival）从比拼benchmarking的学术届解放出来，直给到开发者手中；同时把原本帮助LLM更好完成生成任务中间产物CoT变为直接输出再次直给到用户。
作为一个商业公司，能做到牵着学术届不得不用它的api，到把agent从学术届挤出，把未来直接怼进现实，牛逼。

回看nlp的最新paradigm，无论是Tool 还是Retrival，本质还是在做reasoning，即reasoning在合适的时间选用合适的工具来增强LLM，而此处的LLM几乎就只是OpenAI。

一个意识reasoning，两个维护Tool + Retrival， 唯一核心OpenAI，构建AI命运共同体。


马东锡 NLP
@dongxi_nlp
·
Apr 22, 2023
最近上HPC的课程，思路打开了一个缝，重新思考了transformer的架构。

之前，一直觉得Q,K,V的加权平均是transformer的表现好的核心，或者跟风评论transformer是四肢发达头脑简单的大力出奇迹的始作俑者。

但其实transformer的paper里直接讲了它就是为算力而设计的，只不过它的题目让我出现幻觉，认为QKV is all you need。

transformer就是为大力出奇迹而生。这句话其实有个context，即transformer之前的RNN, 大力也出不了奇迹。因为RNN这种有时间序列的架构，在处理sequence的时候，它的complexity跟序列长度成线性关系，严重影响了训练时的parallelization。

而transformer没有任何循环和卷积网络，对序列处理的complexity是一个常量（只是引入了postional encoding来给数据序列特征），这使得它迎合了算力，也被算力选中。

算力的迅猛发展与transformer形成了良性的互动。这种互动使得transformer从NLP出圈到了CV，GPU的算力越来越强，nvidia的股价一路走高，语言模型的transformer层数越来越多并一路推动SOTA，最终ChatGPT这种形态的产品将NLP从学术界带到大众。

这种良性互动，还会继续。

