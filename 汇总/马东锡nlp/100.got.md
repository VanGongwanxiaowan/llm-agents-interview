chatGPT或者其他GPT是不是nlp的全部或者未来？
答案是否定的，GPT family models的局限性在与generative。
一个语言模型，如何拥有了generative的能力，关键在于其预训练过程。
GPT的预训练过程，使用的是autoregressive的方式，换成人话，就是如何让GPT会接话茬，给出上句，接下句。

马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
这使得GPT的具有非凡的接话茬能力，但其巨大的limitation也再次，仅仅会接话茬。
而我们看另外一个种语言模型BERT，在预训练过程中，采用了masked language modeling的方式，即完形填空。所以BERT具有对句子段落文章整体理解的能力，因此BERT常用于分类，回归等整体语言理解的任务，而生成文字能力不强

马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
那么，有没有BERT+GPT的模型呢？答案是肯定的，即BART，encoder是BERT, decoder是GPT。此encoder-decoder架构获得了NLP多任务的成功。

马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
所以总结来说，目前NLP大型语言模型LLM中，大概有三类，encoder（如bert）, decoder (如gpt)，以及encoder-decoder（如bart）。而无论是什么那一类，都逃不开谷歌17年开源的基础模型，transformer。所以再次总结，目前所有LLM都是transformer的堆叠。


马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
结果，如果是现在下结论说chatgpt就是ai的未来或者顶峰，那就等同于说transformer+autoregressive预训练就是ai的终结，这显而易见是错误。
未来，nlp的发展的突破，在于打破transformer的垄断，以及新的更加接近人类认知的预训练方法的提出。
chatgpt不是神，未来的nlp模型一定比chatgpt更加优秀。


马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
所以总结来说，目前NLP大型语言模型LLM中，大概有三类，encoder（如bert）, decoder (如gpt)，以及encoder-decoder（如bart）。而无论是什么那一类，都逃不开谷歌17年开源的基础模型，transformer。所以再次总结，目前所有LLM都是transformer的堆叠。

马东锡 NLP
@dongxi_nlp
·
Mar 12, 2023
结果，如果是现在下结论说chatgpt就是ai的未来或者顶峰，那就等同于说transformer+autoregressive预训练就是ai的终结，这显而易见是错误。
未来，nlp的发展的突破，在于打破transformer的垄断，以及新的更加接近人类认知的预训练方法的提出。
chatgpt不是神，未来的nlp模型一定比chatgpt更加优秀。

ChatGPT为何成功？
1: zero-shot。用户不必train，fintune，只需inference。
2: api。用户不必mlops，不必上云部署，不必自己写api，不必做load balancing。只需调用API。

就具体NLP任务而言，chatGPT无法评估，但定不是SOTA。
所以，这不是AI的成功，这是商业的成功。

马东锡 NLP
@dongxi_nlp
·
Mar 9, 2023
忘加一条： instruct LM。直接将inference由code转到自然语言，再次降低api使用门槛。




















