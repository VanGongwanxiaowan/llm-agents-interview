「 Interleaved Reasoning 」

Interleaved Reasoning for Large Language Models via Reinforcement Learning

Interleaved Reasoning 让 LLM “思考-回答交错”，迅捷反应 “首token” 。

人脑对 “无响应” 极为敏感,  首个 token 越早流出，用户越感到系统在交互。

Time-to-First-Token （TTFT）决定了用户看到系统在回应所需的第一眼时间。它直接影响用户对是交互式 LLM 的感知，好用还是难用。

作者提出 Interleaved Reasoning (IR)，让 LLM 在 <think> 和 <answer> 段之间来回切换，解决传统 “整段CoT” 带来的高延迟问题, 平均 TTFT 降 80 %。

方法上，纯文本，没有tool call，简单直接地设计三项 rule-based 奖励：
 - r_format 检查<think> 和 <answer>标签交错；
- r_final 看最终答案；
- r_intermediate 按条件给中间关键节点奖励

值得思考的是，通过RL, Interleaved Reasoning 似乎学到一种轻量版  Agentic ReAct 循环：虽然没有tool call，模型把上一步的 <answer> 当作隐性的Observation 继续思考。

一些思考：

在一片硬核的LLM RL氛围中，作者从用户和体验出发，聚焦“首token”反应， inspiring！

Interleaved Reasoning 目前仍需真标签；特别是r_intermediate， 是否可以结合 RLIF/INTUITOR 的内在置信奖励？

https://arxiv.org/abs/2505.19640

Transformer 带来的paradigm就是scaling。
Transformer 的本质不是self attention，而是迎合算力的存在。

...
Data Scaling
Model Size Scaling
Compute Scaling
Context Length Scaling
Modality Scaling
Test-Time Scaling
Parallel Scaling
...

以及，RL Scaling

https://arxiv.org/pdf/2505.24864
