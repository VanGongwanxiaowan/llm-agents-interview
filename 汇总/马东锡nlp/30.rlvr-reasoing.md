「RLVR, Reasoning」

Spurious Rewards: Rethinking Training Signals in RLVR

当随意的奖励信号仍可以大幅提升模型性能，就得重新思考：到底是RL在学习，还是在放大某种“先验”行为。

"RLVR must somehow be surfacing useful reasoning representations learned during pretraining."

预定本周最佳论文！

对 Qwen2.5-Math 系列，作者使用与正确性几乎无关、甚至负相关的“随意奖励”也能把 MATH-500 的准确率大幅提升。

读完论文，觉得精彩，更觉得作者非常 “蓄意” 😆。

“蓄意”地挑选“先验”模型和“非先验”模型：
- 选 Qwen 2.5-Math（自带大量 Python 链式推理）
- 选 Llama 3 / OLMo 2（通用模型、少或劣代码）

“蓄意”地验证并凸显两件事：

- RLVR 像放大器：在 Qwen 上，即便奖励随机或错误，GRPO 的裁剪也会“放大”如 coding 这一高概率、 高正确率的行为，准确率随之增长。

- 先验缺失就失效：Llama/OLMo 没有或只会 Bad-Code，被放大的只是噪声，成绩持平或下降。

严谨的实验支持了 RLVR 主要在 “放大” 预训练潜能而非教授新能力的观点。

作者同样建议跨模型、跨任务验证与深入理解模型先验，不要只盯着单一模型做漂亮数值提升的工作，因为那可能根本没有意义。

https://rethink-rlvr.notion.site/Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f
