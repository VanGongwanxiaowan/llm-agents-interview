https://jax-ml.github.io/scaling-book/gpus/

# 《How to Think About GPUs | How To Scale Your Model》网页总结
## 一、文档基础信息
- **类型**：普通网站，属于《How To Scale Your Model》系列的第12部分（聚焦GPU），需结合该系列第2章（TPU介绍）和第5章（训练相关）阅读以更好理解内容。
- **作者与 affiliations**：Jacob Austin、Swapnil Patil、Adam Paszke（均来自Google DeepMind），Reiner Pope（来自MatX）。
- **发布时间**：2025年8月18日。
- **核心定位**：深入解析GPU的工作原理、网络连接方式，及其对大语言模型（LLMs）的意义，并与TPU进行对比，重点围绕NVIDIA GPU展开。


## 二、GPU核心概念与硬件结构
### 1. 基本定义与核心组件
- **本质**：现代机器学习GPU（如H100、B200）是由大量专注于矩阵乘法的计算核心（**流式多处理器SM**）和高速内存（**HBM**）组成的集合。
- **SM（Streaming Multiprocessor）**：类似TPU的Tensor Core，是GPU的核心“单元”，每个SM包含：
  - **Tensor Core（张量核心）**：专用矩阵乘法单元，贡献GPU绝大部分的浮点运算能力（FLOPs/s），例如H100的bf16精度Tensor Core算力达990 TFLOP/s，而CUDA核心仅66 TFLOP/s；不同代际GPU的Tensor Core算力递增，且支持低精度（如fp8）以提升吞吐量，Blackwell架构的B200因Tensor Core规模过大，新增**TMEM（张量内存）** 用于数据输入。
  - **Warp Scheduler（ warp调度器）**：控制单元，管理一组CUDA核心的任务分配，负责向量算术运算（如ReLU、逐点操作、归约求和），采用**SIMT（单指令多线程）** 编程模型，每个线程有独立指令指针，可灵活编程但存在“warp分化”性能损耗风险。
  - **SMEM（共享内存/L1缓存）**：每个SM的片上高速缓存（如H100的SMEM为256kB），可由程序员控制或硬件自动用作缓存，用于存储激活值和Tensor Core的输入数据。
- **CUDA Core（CUDA核心）**：Warp Scheduler下的算术逻辑单元（ALU），每个SM子分区含32个fp32 CUDA核心，支持FMA（融合乘加）指令（每周期2个FLOPs），历史上曾是GPU核心组件，现游戏GPU中仍承担大量渲染工作。

### 2. 内存层级结构
GPU内存从大到小呈层级分布，各层级功能与性能差异显著：
| 内存类型 | 位置与归属 | 容量（以H100为例） | 功能与特点 |
|----------|------------|--------------------|------------|
| HBM（高带宽内存） | 主GPU内存 | 80GB | 存储模型权重、梯度、激活值等，H100带宽3.35TB/s，B200达9TB/s |
| L2缓存 | 所有SM共享 | 约50MB（分两部分，每部分25MB） | 减少主内存访问，带宽约5.5TB/s（全双工），但比TPU的VMEM慢且不可编程控制 |
| SMEM（L1缓存） | 单个SM私有 | 256kB/SM | 片上高速缓存，可手动控制，用于快速存储Tensor Core输入 |
| 寄存器内存 | 单个SM子分区私有 | 16384个32位寄存器/SM子分区（256kB/SM） | CUDA核心直接访问，寄存器数量限制同时运行的“驻留warp”数量 |
| TMEM（张量内存） | 单个SM私有（B200新增） | 256kB/SM | 解决Blackwell架构下Tensor Core输入数据无法放入SMEM的问题 |

### 3. 关键规格参数（主流NVIDIA GPU对比）
#### 基础硬件规格
| GPU型号 | 架构代际 | 时钟频率 | 每芯片SM数量 | 每SM SMEM容量 | 每芯片L2容量 | 每芯片HBM容量 |
|---------|----------|----------|--------------|---------------|--------------|--------------|
| V100    | Volta    | 1.25GHz/1.38GHz | 80           | 96kB          | 6MB          | 32GB         |
| A100    | Ampere   | 1.10GHz/1.41GHz | 108          | 192kB         | 40MB         | 80GB         |
| H100    | Hopper   | 1.59GHz/1.98GHz | 132          | 256kB         | 50MB         | 80GB         |
| H200    | Hopper   | 1.59GHz/1.98GHz | 132          | 256kB         | 50MB         | 141GB        |
| B200    | Blackwell | 未公布 | 148          | 256kB         | 126MB        | 192GB        |

#### 算力与带宽规格
| GPU型号 | 架构代际 | 每芯片HBM带宽 | 每芯片FLOPs/s（bf16/fp16） | 每芯片FLOPs/s（fp8/int8） | 每芯片FLOPs/s（fp4） |
|---------|----------|---------------|----------------------------|---------------------------|----------------------|
| V100    | Volta    | 9.0e11 B/s     | —                          | —                         | —                    |
| A100    | Ampere   | 2.0e12 B/s     | 3.1e14                     | 6.2e14                    | —                    |
| H100    | Hopper   | 3.4e12 B/s     | 9.9e14                     | 2.0e15                    | —                    |
| H200    | Hopper   | 4.8e12 B/s     | 9.9e14                     | 2.0e15                    | —                    |
| B200    | Blackwell | 8.0e12 B/s     | 2.3e15                     | 4.5e15                    | 9.0e15               |

### 4. GPU与TPU的芯片级差异
| 对比维度 | GPU（以H100为例） | TPU（以v5p为例） | 核心影响 |
|----------|-------------------|------------------|----------|
| 核心单元数量与规模 | 132个小型SM（独立） | 2个大型Tensor Core | GPU灵活性高（可同时处理数百任务），TPU结构简单、成本低 |
| 向量运算单元 | 528个小型独立SIMD单元（Warp Scheduler） | 8个大型VPU（每VPU含1024个ALU） | GPU编程灵活，TPU依赖编译器优化流水线以避免 stalls |
| 高速缓存 | SMEM+L2共约66MB（H100），L2不可编程 | 128MB VMEM（高速且可编程） | TPU缓存性能更优，适合存储模型权重以加速推理 |
| 单芯片性能与成本 | H200算力约为TPU v5p的2倍，HBM容量1.5倍；Google Cloud单价约$10/小时 | 算力与内存规模较小；单价约$4/小时 | GPU单芯片更强但更贵，TPU依赖多芯片组网扩展 |
| 编译器依赖 | 对编译器依赖低，新任务易适配 | 高度依赖编译器优化，需手动调度内存与计算 | GPU“即插即用”性好，TPU需优化才能接近峰值性能 |


## 三、GPU网络连接架构
GPU采用**分层树形交换网络**，与TPU的2D/3D环面网络（仅邻居连接）差异显著，分为节点内（Intra-node）和节点外（Cross-node）两层：

### 1. 节点级网络（Intra-node）
- **定义**：由8个GPU（GB200 NVL72支持72个GPU）组成“节点”（NVLink域），通过**NVLink（NVIDIA高速互连）** 实现全连接、低延迟、高带宽通信，节点内含多个**NVSwitch（NVLink交换机）** 转发数据包。
- **关键参数（按代际演进）**：
| NVLink代际 | NVSwitch代际 | GPU架构 | NVLink带宽（全双工，GB/s） | 每GPU NVLink端口数 | 节点内GPU间带宽（全双工，GB/s） | 节点规模（GPU数量） | 每节点NVSwitch数量 |
|------------|--------------|---------|----------------------------|--------------------|--------------------------------|--------------------|--------------------|
| 3.0        | 2.0          | Ampere  | 25                         | 12                 | 300                            | 8                  | 6                  |
| 4.0        | 3.0          | Hopper  | 25                         | 18                 | 450                            | 8                  | 4                  |
| 5.0        | 4.0          | Blackwell | 50                        | 18                 | 900                            | 8/72               | 2/18               |
- **拓扑特点**：H100节点含4个NVSwitch，每个交换机64个NVLink端口，GPU与交换机以“5+4+4+5”链路模式连接，节点内总带宽达3.6TB/s（全双工），远超节点外带宽。

### 2. 节点外网络（Cross-node）
- **架构**：多个节点通过**InfiniBand（IB）或以太网**连接成更大单元，采用“胖树（Fat Tree）”拓扑，确保全二分带宽（任意分区间带宽充足），主要层级包括：
  - **Scalable Unit（SU，可扩展单元）**：32个节点（256个GPU）组成，由8个InfiniBand叶交换机连接，每个节点通过8个400Gbps IB网卡与叶交换机通信，SU级带宽12.8TB/s（全双工）。
  - **SuperPod（超级集群）**：4个SU（1024个GPU）组成，由16个InfiniBand spine交换机连接所有叶交换机，总带宽51.2TB/s（全双工），节点间通信带宽400GB/s（全双工）。
- **特殊案例（GB200 NVL72）**：单个NVLink域含72个GPU，节点出带宽达3.6TB/s（是H100节点的9倍），可进一步组成更大SuperPod，大幅提升跨节点通信性能。


## 四、GPU上的集合通信（Collectives）
集合通信（如AllGather、ReduceScatter、AllReduce、AllToAll）由NVIDIA的**NVSHMEM**和**NCCL（“nickel”）** 库实现，性能随通信层级（节点内/外）变化：

### 1. 节点内集合通信（Intra-node）
- **AllGather/ReduceScatter**：采用“环形算法”，每个GPU将数据分片发送给其他GPU，通信成本约`B / W_GPU_egress`（B为数据量，W_GPU_egress为GPU出带宽），例如H100节点内AllGather 512MB数据约需1.04ms（理论值），实际因延迟可能达1.5ms。
- **AllReduce**：组合ReduceScatter + AllGather，成本为AllGather的2倍；若启用**SHARP（可扩展分层聚合归约协议）**（Hopper及后续架构支持），交换机可直接执行归约操作，理论成本减半，但实际带宽仅提升30%（如H100从370GB/s升至480GB/s）。
- **AllToAll**：因节点内全连接，直接点对点通信，成本约`B / (8 * W_GPU_egress)`；若为稀疏AllToAll（如MoE模型仅k个专家非零），成本降至`(B * k) / (64 * W_GPU_egress)`，灵活性高于TPU。

### 2. 节点外集合通信（Cross-node）
- **AllGather/ReduceScatter**：因胖树拓扑，成本约`B / W_node_egress`（W_node_egress为节点出带宽，H100节点为400GB/s），与节点数量无关，例如1024 GPU集群的AllGather带宽仍达400GB/s。
- **AllReduce**：成本为AllGather的2倍，启用SHARP可降低成本，但需SHARP兼容的IB交换机；跨节点AllToAll性能下降显著，例如2个H100节点的AllToAll带宽从50GB/s降至25GB/s，比节点内慢4倍以上。
- **分片数组的归约**：若数组沿多轴分片（如Y轴跨多个节点），归约成本可降低`1 / Y`（Y为分片数），但仅当Y超过节点规模时才有明显效果。


## 五、GPU上LLM扩展的屋顶线模型（Rooflines）
屋顶线模型用于分析不同并行策略下的计算（T_math）与通信（T_comms）瓶颈，核心是判断`T_math > T_comms`（计算瓶颈，性能最优）的条件，以下基于H100参数分析：

### 1. 数据并行（DP）与ZeRO分片
- **核心逻辑**： backward pass需AllReduce权重或ReduceScatter + AllGather，计算与通信成本分别为：
  - `T_math = 8 * B * D * F / (X * C)`（B为全局token批次，D为输入维度，F为feed-forward维度，X为分片数，C为GPU算力）
  - `T_comms = 8 * D * F / W_collective`（W_collective为集合通信带宽）
- **计算瓶颈条件**：单GPU token批次需>2500（节点内）或>2475（节点外）；MoE模型因权重规模扩大（E个专家，k个激活专家），条件变为>2500 * (E/k)（如E=128、k=4时需>79200），对批次规模要求极高。
- **优化点**：小批量数据并行（如2节点）可降低临界批次（如H100降至1237），启用SHARP理论上可减半临界批次，但实际提升有限。

### 2. 张量并行（TP）
- **核心逻辑**： forward pass需AllGather激活值，backward pass需ReduceScatter梯度，成本分别为：
  - `T_math = 4 * B * D * F / (Y * C)`（Y为TP分片数）
  - `T_comms = 4 * B * D / W_collective`
- **计算瓶颈条件**：TP分片数Y < F / 2475（节点外），例如F=28000（LLaMA-3）时Y<11，实际因节点规模限制，TP通常不超过8（单节点）或16（2节点），否则通信瓶颈显著。

### 3. 专家并行（EP，MoE模型专用）
- **核心逻辑**：专家权重沿专家维度分片，需2次AllToAll传输激活值，成本分别为：
  - `T_math = 4 * B * k * D * F / (Z * C)`（Z为EP分片数）
  - `T_comms = 4 * B * D * (Z-8) / (W * Z) * min(8k/Z, 1)`
- **计算瓶颈条件**：若F < 8*C/W_node（如H100中F<8*990e12/400e9≈19800），EP仅支持1-2节点；若F>19800，可支持E个节点的EP（E为专家总数），灵活性高于TP。

### 4. 流水线并行（PP）
- **核心逻辑**：将模型层拆分到不同节点，仅传输小批量激活值，通信成本极低（约`1.5*2*B*D/(W*N_layers)`，N_layers为总层数），理论上“通信免费”。
- **挑战**：代码复杂度高（需零气泡调度）、与FSDP/ZeRO-3兼容性差（微批次需重复AllGather权重）、存在流水线气泡（计算空闲），但仍是大模型扩展的重要补充策略。

### 5. 实例与总结
- **DeepSeek V3**：2048个H800 GPU，采用64-way EP（8节点）+16-way PP+2-way ZeRO-1 DP，批次规模62.9M tokens/GPU，通过多并行策略降低通信瓶颈。
- **LLaMA-3 70B**：16k GPU，8-way TP（单节点）+16-way PP+128-way ZeRO-1 DP，批次规模16M tokens，依赖TP和PP减少DP通信成本。
- **核心结论**：
  1. 数据并行需单GPU批次>2500 tokens，MoE模型需更高；
  2. 张量并行限于8-16-way（单/双节点），否则通信瓶颈；
  3. 专家并行适合F大的MoE模型，可跨多节点；
  4. 流水线并行通信成本低，但需解决代码复杂度与兼容性问题。


## 六、附录与扩展资源
### 1. 附录内容
- **附录A（GB200变化）**：Blackwell架构的B200节点仍为8 GPU，但GB200 NVL72节点含72 GPU，NVLink 5带宽900GB/s，节点出带宽3.6TB/s，跨节点通信性能大幅提升，屋顶线模型更倾向于节点级瓶颈。
- **附录B（网络细节）**：NVLink 4交换机含64个端口，节点内总带宽受GPU出带宽限制（H100节点3.6TB/s），SU级和Spine级带宽分别受叶交换机和Spine交换机限制；同时提供了GPU与TPU集合通信带宽的实测对比（如H100节点内AllReduce带宽约370GB/s，TPU v5p更高且低数据量时性能更优）。

### 2. 推荐阅读资源
- 《SemiAnalysis’ History of the NVIDIA Tensor Core》：解析GPU从游戏引擎到ML加速器的演变。
- 《DeepSeek-V3 Technical Report》：大模型训练中GPU并行策略的实践案例。
- 《H100 DGX SuperPod Reference》：GPU集群网络架构的详细文档。
- 《How to Optimize a CUDA Matmul》：CUDA核心实现高效矩阵乘法的教程。


## 七、引用格式
- **学术引用**：Austin et al., "How to Scale Your Model", Google DeepMind, online, 2025.
- **BibTeX引用**：
```bibtex
@article{scaling-book,
  title = {How to Scale Your Model},
  author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},
  publisher = {Google DeepMind},
  howpublished = {Online},
  note = {Retrieved from https://jax-ml.github.io/scaling-book/},
  year = {2025}
}
```
