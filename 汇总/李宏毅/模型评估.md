https://zhuanlan.zhihu.com/p/1913693232002495712


# 跟着台大李宏毅老师学：评估模型的方式 - 网页总结
## 一、模型评估核心逻辑
1. **基础流程**：先准备含输入与标准答案的评估数据集，将输入输入不同语言模型获取输出，最后对比输出与标准答案，正确率高的模型在数据集对应能力上更强。
2. **核心挑战**：语言模型输出无限制，判断其是否正确存在难度，且评估角度、方式等均会影响评估结果。

## 二、模型回答正确性判断方式
### （一）选择题对比选项
1. **理想状态**：让模型仅输出选项，直接与正确答案对比，但实际中模型可能输出“答案是b”等非预期内容，部分可通过正则表达式解析，部分难以判断；也可限制模型输出为几率分布中概率最高的token，但判断该结果对错无统一标准。
2. **影响因素**：选项位置（如正确选项移到A后，llama-30B分数提升15分）、选项表示方式（大写/小写、1/2/3/4、加括号等）均会影响模型正确率。

### （二）计算与答案的相似性
1. **适用场景**：翻译、摘要等无唯一标准答案的任务。
2. **常用指标**：翻译用BLEU指标，摘要用ROUGE指标，均通过字面比对判断，无需完全一致，只需部分一致。
3. **局限性**：无法识别语义相近的表述，如“诙谐”和“幽默”会被判定为全错，存在不合理性。

### （三）人类评估
1. **典型案例**：Chatbot Arena网站，用户输入问题后会收到两个模型回复，用户选择更优回复，被选次数多的模型（实际算法更复杂）排名更高。
2. **优势**：相对准确，能结合人类主观判断；劣势：耗时间、耗人力。

### （四）模型自我评估
1. **原理**：让语言模型替代人类，判断模型输出与标准答案是否一致，或比较两个输出优劣。
2. **典型案例**：MT-Bench是此类Benchmark，题目无标准答案（如写作），其与人类评估的Chat Bot Arena相关性达0.94，效果较好。
3. **局限性**：模型可能存在偏好（如偏好长答案），AlpacaEval 2.0考虑输出长度后，与Chat Bot Arena相关性有所提升。

## 三、模型评估角度与相关Benchmark
### （一）评估全方位能力
1. **代表Benchmark**：BIG-Bench，包含200多种任务，由400多位作者编写，还含多种“奇葩”任务。
2. **特色任务示例**
    - Emoji Movie：根据表情猜电影名，小模型常答非所问，大模型能答对如“海底总动员（finding nemo）”。
    - Checkmate In One Move（西洋棋一步将军）：大模型答题符合规则但可能答错，小模型连规则都不符合。
    - ASCII word recognition：识别用ASCII码呈现的单词（如“BENCH”），考验模型图像类识别能力。

### （二）评估长文能力
1. **测试方法——“大海捞针”**：在超长文章不同位置（开头、结尾、中间）插入特定信息，询问模型与该信息相关的问题，判断其长文理解能力。
2. **模型表现**
    - GPT-4：文本长度64k以下时，无论插入位置，正确率均较高；超过64k后，正确率未达100%。
    - Claude：初始表现差，正确率低，但修改Prompt（如加入“Don't give information outside the document or repeat your findings”）后，正确率接近100%，体现Prompt对评估结果的影响。

### （三）评估推理能力
1. **传统测试局限**：模型解出数学题等可能是因记忆题目而非真有推理能力，如GSM8K题目换名字、数字或调整句子顺序后，模型正确率下降。
2. **优化测试方式——去污染**：训练模型时剔除Benchmark数据集，可缓解记忆问题，但无法完全避免（如题目被翻译后难检测）。
3. **代表Benchmark——ARC-AGI**
    - 特点：图形智力测验题目，将图像转为数字形式输入模型，模型需靠推理答题，不易因记忆题目得分。
    - 发展情况：2019年出现，五年内模型进展小，现o3模型正确率介于普通人类和数理科学生之间，但仍可通过训练类似题目“hack”。

### （四）其他有趣Benchmark
1. **MACHIAVELLI Benchmark**：让模型玩文字冒险游戏，从“达成目标的reward分数”和“符合人类道德规范程度”两方面评估。如DRRN模型reward高但道德水准低，GPT4道德水准较高但reward较低，提示模型注意道德规范后其道德分数更高。
2. **Theory of Mind（心智理论评估）**：测试模型揣摩他人想法的能力，人类平均87.5分，最强模型仅12.3分，且模型可能因见过题目答对，换表述则不会，心智理论远不如人类。

## 四、评估的“hack”问题与应对
1. **hack现象**：固定出题方向的Benchmark易被hack（如用类似测试集数据训练模型）；Chatbot Arena虽让全球用户出题缓解出题方向hack，但人类喜好有倾向性（如偏好含emoji、格式清晰的回复），仍可被hack。
2. **Chatbot Arena评比机制**：采用竞赛常用的Elo Score，模型有战力分数，可计算胜率并反推战力，但需考虑与实力无关的因素（风格、emoji、长度等），加入这些因素后模型排名会变化（如Claude排名上升）。
3. **应对困境**：“一项指标一旦被当作目标，就不再是好指标”，无绝对无法被hack的Benchmark，唯一办法是不断推出新Benchmark，在被hack前利用其有效评估。

## 五、总结
1. **回答正确性判断维度**：涵盖选择题对比、相似性计算、人类评估、模型自我评估四种方式，各有优劣与适用场景。
2. **评估角度维度**：包括全方位能力、长文能力、推理能力、心智理论、道德与目标达成平衡等，可根据需求选择对应Benchmark。
3. **核心难题**：评估易受多种因素干扰，且Benchmark存在被hack的风险，模型评估仍是待持续优化的难题。

## 六、参考资料
1. 【生成式AI導論 2024】第12講：淺談檢定大型語言模型能力的各種方式
2. 【生成式AI時代下的機器學習(2025)】第九講：你這麽認這個評分系統幹什麽啊？談談有關大型語言模型評估的幾件事
