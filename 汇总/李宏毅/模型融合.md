https://zhuanlan.zhihu.com/p/1925272685186904605


# 跟着台大李宏毅老师学：模型融合-网页总结
## 一、模型融合核心定义
模型融合（Model Merging）是基于相同基础模型（Foundation Model），无需训练数据和额外训练，通过参数操作融合不同微调模型技能的技术，核心依赖**Task Vector（任务向量）**——即微调模型与基础模型的参数差值，代表模型新增的特定技能。


## 二、Task Vector三大核心应用
### 1. 相加：融合多模型技能
- **适用条件**：所有参与融合的模型需源自**相同基础模型**（结构、参数均一致），可给不同模型参数设置权重（权重越高影响越大）。
- **典型案例**：
  - 打造“中文+安全对齐”LLaMA模型：基于LLaMA-2-base训练中文模型，将其与LLaMA-2-Chat的Task Vector相加，解决直接微调中文数据导致的“丢失安全对齐能力”问题。
  - 生成专项Reward Model：将“评价输出好坏但不评代码”的Reward Model与“写代码模型”融合，得到能评代码质量的Reward Model；将“评文字输出”的Reward Model与“视觉模型”融合，得到能通过图片评质量的Reward Model。

### 2. 相减：实现机器遗忘（Machine Unlearning）
- **原理**：从目标模型参数中减去某技能的Task Vector，让模型丢失该技能。
- **典型案例**：净化中文模型（TAIDE-LX）：先用脏数据（脏话数据）训练LLaMA-2-base，得到“说脏话”的Task Vector，再从TAIDE-LX中减去该向量，使模型不再会说脏话（甚至无法识别脏话）。

### 3. 类比：无数据学会新任务
- **原理**：若“Task A之于B = Task C之于D”，可通过A、B、C模型的Task Vector推导D的Task Vector，无需D的训练数据即可得到D模型。
- **典型案例**：构建法律领域ASR模型（无真实语音数据）：
  - 定义任务：A（旧领域文字+合成语音）、B（旧领域文字+真实语音）、C（新领域/法律文字+合成语音）、D（新领域/法律文字+真实语音，无数据）。
  - 操作：训练A、B、C模型，推导“合成语音转真实语音”的Task Vector（Synthesic2Real Vector），加到C模型上，得到接近真实法律语音训练效果的ASR模型，且在不同基础模型、TTS模型下均有效（WER指标越低越好）。


## 三、模型融合成功的关键条件
### 1. 成功判定标准
融合后模型需保留原模型核心能力：若原模型A对输入X₁输出Y₁、模型B对输入X₂输出Y₂，融合后模型对X₁仍输出Y₁、对X₂仍输出Y₂，即视为成功（暂不考虑衍生新能力场景）。

### 2. 核心影响因素
- **参数更新范围与重叠度**：各微调模型对基础模型的**更新参数越少、重叠度越低**，融合成功率越高（避免参数冲突导致原能力丢失）。
- **基础模型规模**：模型越大，融合成功的可能性越高（实验数据支撑）。


## 四、领域展望
模型融合目前仍属较新领域，未来若技术成熟、成功率提升，有望形成“Task Vector商店”——开发者可专注单一领域任务，通过“装备”对应Task Vector，快速为模型添加所需能力。


## 五、参考资料与补充信息
- 核心参考：【生成式AI时代下的机器学习(2025)】第十一讲：今天你想为Foundation Model装备哪些Task Vector？浅谈神奇的Model Merging技术。
- 相关延伸：文中提及模型融合可用于“防止模型遗忘”，具体细节可参考对应论文；同时提到机器学习比赛中常见类似融合思路（如“1/2*(theta_A + theta_B) = theta + 1/2*(tau_A + tau_B)”）。
- 作者信息：文章来自“台大李宏毅课程笔记”专栏，作者为codingling，公众号“L的AI实验室”，发布于2025年7月6日。
