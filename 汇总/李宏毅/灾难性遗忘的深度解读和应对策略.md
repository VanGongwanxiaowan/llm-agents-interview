https://zhuanlan.zhihu.com/p/1898093959705784684


# 跟着台大李宏毅老师学：灾难性遗忘的深度解读与应对策略-知乎文章总结
## 一、灾难性遗忘核心定义
灾难性遗忘指对Foundation模型进行post-training（含pretrain、SFT、RLHF等）后，模型虽获得针对目标任务的能力，却丢失Foundation模型原有部分能力的现象，具体案例如下：
1. **LLaMA-2-Chat学中文**：经中文数据Post-training后，模型能中文回复，但丢失部分Alignment能力，面对攻击性prompt不再拒绝，有害输出概率升高；部分情况下还会重复输出同一句话。
2. **专项能力训练**：在函数使用、数学、代码数据集上训练模型，模型对应专项能力增强，其他能力却下降。
3. **LLaMA听声音训练**：训练1个Epoch时，模型能按指令输出json格式（答案错误）；训练3个Epoch时，模型能正确识别声音情绪，却无法输出json格式。


## 二、Post-training面临的挑战与相关研究
### （一）灾难性遗忘成因
Post-training过程中，模型仅针对目标任务学习，未涉及其他任务，导致过拟合到目标任务，进而出现灾难性遗忘。

### （二）相关研究结论
1. **模型大小影响**：针对1B-7B大小模型的研究显示，模型大小未显著改善灾难性遗忘问题，且模型在目标任务上学习效果越好，遗忘越严重。
2. **不同训练方式对比**：LoRA训练导致的灾难性遗忘程度最低，但代价是模型在目标任务上的学习效果也相对较差；Dropout、weight decay等方法防止遗忘的效果不如LoRA。


## 三、缓解灾难性遗忘的策略
### （一）Experience Replay（经验回放）
1. **核心原理**：训练新任务（任务2）时，混入少量（5%即可）旧任务（任务1）数据，可大幅缓解遗忘；若缺乏旧任务训练数据，可让LLM自生成数据作为replay数据。
2. **案例支撑**：GPT-2学习SQuAD（阅读理解）后正确率上升，再学WikiSQL、SST时正确率下降，而学习与SQuAD类似的SRL任务时，正确率又回升；混入旧任务数据的Replay实验也验证了该方法的有效性。
3. **数据生成方法**：如Magpie论文中，让Llama-3-Instruct生成数据——先输入“user”token让其输出query，再拼接“user”token、query、“assistant”token让其输出回复，也可自行准备问题让模型仅输出回答，实现针对性数据收集。

### （二）Paraphrase & self-output（改写与自输出）
1. **具体方法**
    - **Paraphrase**：不直接使用原始输出，让模型用自身语言改写输出内容。
    - **self-output**：让模型直接输出回答，若回答正确则采用，可多次采样，类似RL训练方法，能减少灾难性遗忘。
2. **数据来源对比研究**
    - 用人类答案训练模型效果通常最差，用其他语言模型（如GPT-4、Claude）输出来训练效果更好，但在HumanEval等场景中，用GPT-4答案训练效果不佳。
    - **优化方法（Minimum Change）**：先让Foundation Model生成回复，再用GPT-4仅修改回复中的错误部分，尽量保留原内容，此方法效果优于直接使用GPT-4的答案。
3. **跨模态应用**：训练“听声音的LLM”时，将声音讯号的语音特征（长度、说话人性别、情绪、口音等）用文字描述，与query一同输入LLM，用LLM输出作为训练Target，可防止模型遗忘LLM原有能力，BLSP、DeSTA2、DiVA等模型均采用类似训练方式，其中DeSTA2仅用“what can you hear？”一个指令训练，就能回答多种问题，在声音相关Benchmark上表现最优且数据用量少。

### （三）Selective Token Masking (STM，选择性token屏蔽)
1. **核心逻辑**：研究发现，Ground Truth（真实标签）中低概率token更多，且整体perplexity（困惑度）最高；训练时过滤掉难预测的token，不计算其loss。
2. **效果验证**：过滤部分难预测token后，模型在in-domain（领域内）和out-of-domain（领域外，代表遗忘程度）任务上的表现均有所提升，横轴显示token过滤比例（从最难预测的开始过滤），进一步证明该方法有效。


## 四、参考资料与文章信息
1. **核心参考**：李宏毅老师《生成式AI时代下的机器学习(2025)》第六讲：生成式人工智能的后训练(Post-Training)与遗忘问题。
2. **文章基本信息**：发布于2025年4月22日，收录于“每天学点NLP知识”专栏，作者为“codingling”（公众号：L的AI实验室），获10人赞同。
