https://zhuanlan.zhihu.com/p/1903544577467605414

# 跟着台大李宏毅老师学：别让推理模型想太多 - 网页总结
## 一、核心问题与实验结论
1. **核心疑问**：推理（思考）过程越长，模型输出结果正确率越高吗？
2. **初步实验局限**：早期实验显示推理长度与正确率呈负相关，但该结论不严谨——二者实际均受**问题难度**影响（问题越难，模型推理越长、正确率越低），无法直接证明推理长度与正确率的因果关系。
3. **严谨实验结论**：同一问题输入模型5次，按推理长度分Group 1（最短）至Group 5（最长），结果表明：推理长度越长，正确率未必越高，冗长的推理过程多为不必要。


## 二、避免模型“想太多”的方法（对应4种教模型思考的方式）
|教模型思考的原方法|避免“想太多”的具体策略|关键细节|
|----|----|----|
|CoT方法（思维链）|修改CoT的prompt（参考《Chain of Draft》论文）|在Claude 3.5 Sonnet上实验，可大幅缩短推理长度，且对正确率影响小|
|给定推理工作流程|人为控制流程参数|如减少采样数量、缩小beam search的N值、简化推理树状结构，直接限制推理长度|
|Imitation Learning（模仿学习）|筛选教师模型输出数据|多次采样教师模型输出，选取“答案正确且推理长度最短”的数据，供学生模型学习|
|From Explicit CoT to Implicit CoT（显式→隐式思维链）|分阶段训练，逐步去除推理过程|1. 初始用（问题+完整计算过程+答案）训练；2. 逐步删除计算过程中的部分token；3. 最终直接用（问题+答案）训练；在数学乘法、GSM8K等简单任务上，有无推理过程效果相近|


## 三、RL方法（强化学习）的特殊处理
1. **问题痛点**：RL训练的模型（如DeepSeek-R1）推理过程通常冗长，因训练中仅要求“答案正确”，未限制长度，模型可能反复验证、尝试多种方法。
2. **优化策略**
    - **动态长度控制（主流方法）**：先通过多次输入问题，统计模型答对该问题的“平均推理长度”，以该长度为基准——超过则判定为冗余，低于则更优，据此训练模型。
    - **指定长度+奖励机制**：在问题后添加“推理长度设定为n”的提示，将奖励（reward）设为“正确率 - 目标与实际长度差异”，差异过大则给予负面奖励。
    - **效果**：在数学任务的in-domain（域内）测试集上，长度控制误差仅2%-6%；out-of-domain（域外）测试集效果较弱，但仍有控制作用。


## 四、长度控制对模型推理能力的影响
1. **对比对象**：不可控长度模型（如DeepSeek-R1-1.5B、Agentica-24K，长度长、正确率高）、粗暴控制模型（S1，超长度截断/不足补“wait”，短长度时正确率低）、RL控制模型（L1-Exact：必须符合目标长度；L1-Max：不超过目标长度）。
2. **结论**：RL控制模型在“长token输出”时，正确率与不可控长度模型相近；“短token输出”时，正确率优于S1，证明可训练出“能控制推理长度且不损失能力”的模型。


## 五、网页元信息与参考资料
1. **基础信息**：发布于2025年5月7日，所属专栏“每天学点NLP知识”，作者为“codingling”（公众号：L的AI实验室），获3人赞同。
2. **参考资料**：核心内容源自【生成式AI时代下的机器学习(2025)】第八讲：大型语言模型的推理过程不用太长、够用就好。
3. **推荐阅读**：含《Done is Better than Perfect! MinD 多轮拆解推理过程，加速大模型推理》《笔记：学习推理加速半年之总结与迷思》等相关文章。
