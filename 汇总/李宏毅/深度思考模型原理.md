https://zhuanlan.zhihu.com/p/1901388487082612076


# 跟着台大李宏毅老师学：打造深度思考模型方法总结
## 一、深度思考语言模型核心概念
1. **定义与典型案例**：深度思考语言模型（又称推理模型）会先输出长思考过程再给答案，如GPT o1/o3/o4、DeepSeek R1、Gemini 2 Flash Thinking、Claude 3.7 Sonnet，其思考过程（Reasoning）包含验证、探索、规划，且需用``和``标注。
2. **关键关联概念**
    - **Test-Time Compute**：测试阶段投入更大算力以获更好结果，Reasoning是其一种，通过增加输出长度弥补“深度不足”，AlphaGo测试时用MCTS（蒙特卡洛树搜索）也属此范畴。
    - **Test-Time Scaling**：思考越多（算力投入越多）结果越好。相关棋类论文实验显示，达到同等效果，训练阶段算力投入远大于测试阶段，故Test-Time Scaling是潜力方向。


## 二、打造推理模型的四类方法
### （一）无需微调的方法
1. **CoT（Chain of Thought，思维链）**
    - **适用范围**：仅适用于较强模型。
    - **核心逻辑**：让模型先列解题过程再给答案，输出形式与深度思考模型类似，思考过程极长时称为“Long CoT”。
    - **获取方式**：
        - Few-shot CoT：给模型示例，使其知晓先写过程再给结果。
        - Zero-shot CoT：无需示例，仅告知模型“Let's think step by step”，模型即可自动列过程。
        - Supervised CoT：将思考方式和流程写入prompt，给模型更精确指示，如对GPT-4o采用该方法，模型会先分析题目、分解运算，再给出验算步骤，但需模型能力较强。
2. **推理工作流程**
    - **核心逻辑**：直接为模型设定推理流程，如让模型尝试多种方法，可通过让模型对同一问题多次回答（利用输出随机性）实现，尝试次数越多，获正确答案概率越高（参考论文“Large Language Monkeys”）。
    - **答案筛选方式**：
        - 投票法（Majority Vote/Self-consistency）：选出现次数最多的答案，对Llama 3.1 1B模型，该方法可提升效果，但仍不及8B模型。
        - 置信度（Confidence）：选产生几率更高的答案。
        - Best-of-N：训练或使用Verifier模型验证答案，选分数最高的答案，训练Verifier模型需先收集已知答案问题，让模型输出推理过程与结果，对结果正确/错误的输出分别标1/0分作为训练数据，训练后Verifier效果更佳。
    - **答案生成模式**：
        - 并行（Parallel）：同时生成多个答案。
        - 串行（Sequential）：基于前一解法生成后一解法，两种模式可结合（参考论文2408.03314）。
    - **中间步骤验证方案**：
        - 让模型先输出step 1（用`<step>`和`</step>`限定，输出第一个`</step>`即停止），再用Process Verifier验证step 1正确性。
        - Process Verifier训练：让模型分步骤解题，对每个step多次生成后续步骤，统计正确结果占比作为该step正确率，以此为数据训练模型，使其对易获正确结果的步骤打分更高。
        - 后续步骤选择：可设阈值选超阈值步骤，或用Beam Search保留分数最高的N条路径继续；MCTS（蒙特卡洛树搜索）是当前流行的Beam Search变形，DVTS也是Beam Search变形，但效果与Beam Search差距不大。

### （二）需要微调的方法
1. **Imitation Learning（模仿学习）**
    - **核心逻辑**：用含“输入、推理过程、正确答案”的训练数据，让模型学习输出推理过程与正确答案，可采用有监督学习或类强化学习（告知模型步骤对错）。
    - **训练数据构造方式**：
        - 让模型输出推理过程与答案，若答案正确，可认为推理过程正确，用作训练数据。
        - 用Process Verifier验证每个步骤，仅保留步骤正确且最终答案正确的数据（参考rStar-Math论文）。
        - 加入错误路径并插入Verifier反馈，训练模型纠错能力，如SoS方法（插入错误路径与反馈）、Journey learning（构建含错误路径的树状推理过程）。
        - 知识蒸馏（Knowledge Distillation）：用已有推理模型生成的推理过程与答案作为训练数据，如DeepSeek-R1用该方法训练Qwen和Llama，效果可媲美顶尖模型。
    - **关键认知**：无需要求模型推理过程每步都正确，重点是最终答案正确，需训练模型“知错能改”的能力，仅给正确推理过程会让模型无法学会纠错。
2. **RL（强化学习）**
    - **核心逻辑**：以结果为导向，仅根据答案是否正确给奖励，推理过程形式与正确性不影响奖励，依赖基础模型（Foundation Model）能力，RL仅强化模型原有能力，需模型本身有生成正确答案的潜力。
    - **典型案例（DeepSeek-R1系列）**：
        - DeepSeek-R1-Zero：用“问题+正确答案”训练，答案正确给正奖励，错误给负奖励，还加入format reward要求输出含think token，可逼近GPT-o1正确率，结合Majority Vote效果更佳，但推理过程不易读且混多语言，非最终版本。
        - DeepSeek-R1训练流程：
            1. 冷启动数据收集：用R1-Zero生成数据后人工修改、用few-shot CoT让其他模型生成数据、用Supervised CoT让模型生成含反思验证的详细答案，共几千条数据训练DeepSeek-v3得模型A。
            2. RL训练：对模型A做RL得模型B，除结果导向奖励外，加入语言一致性奖励提升易读性（牺牲少量性能）。
            3. 扩展数据训练：收集推理类（来自模型B，用DeepSeek-v3验证答案并按规则过滤糟糕推理过程，获60万条）与非推理类（DeepSeek-v3自生成，20万条）数据，共80万条训练模型A得模型C，再对模型C做RL（加入基于奖励模型的RL，因非推理数据无法用结果导向训练），最终得DeepSeek-R1。
    - **注意事项**：DeepSeek尝试Process Verifier和MCTS未成功；RL效果依赖基础模型，如基于Qwen-32B做RL效果提升有限，但Imitation Learning更有效。


## 三、总结与预告
1. **方法汇总**：打造推理模型的四类方法可结合使用（如DeepSeek-R1融合CoT、推理工作流程、Imitation Learning、RL），各类方法核心要点如下表：
|方法|是否需微调|核心逻辑|关键要点|
|----|----|----|----|
|CoT|否|让模型先列过程再给答案|分Few-shot/Zero-shot/Supervised三类，需较强模型|
|推理工作流程|否|为模型设定推理流程，多次生成答案后筛选|有投票法/Confidence/Best-of-N等筛选方式，支持并行/串行生成，可验证中间步骤|
|Imitation Learning|是|用“输入+推理过程+正确答案”数据训练|可通过多种方式构造数据，需训练模型纠错能力|
|RL|是|以结果为导向给奖励，强化模型能力|依赖基础模型，DeepSeek-R1系列为典型案例|
2. **下期预告**：将讲解推理模型的挑战与未来发展，重点探讨如何在保证效果的前提下缩短冗长的推理过程，降低算力与成本消耗。


## 四、参考资料
台大李宏毅老师【生成式AI時代下的機器學習(2025)】第七講：DeepSeek-R1 這類大型語言模型是如何進行「深度思考」（Reasoning）的？
