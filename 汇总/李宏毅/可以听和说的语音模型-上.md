https://zhuanlan.zhihu.com/p/1930382940283568997

# 跟着台大李宏毅老师学：可以听和说的语音语言模型（上）- 知乎网页总结
## 一、语音语言模型核心定位与实例
1. **核心差异**：相较于普通语言模型，语音语言模型需同时处理文本/语音的输入输出，还需识别说话人身份、情绪及语音环境，技术挑战更大。
2. **典型实例**：包括ChatGPT voice mode、Gemini Live、Moshi（arxiv:2410.00037）、GLM-4-Voice（arxiv:2412.02612）、Step-Audio（arxiv:2502.11946）、Qwen2.5-Omni（arxiv:2503.20215）、Kimi-Audio（arxiv:2504.18425）、SpeechGPT（GitHub:OpenMOSS/SpeechGPT-2.0-preview）、Sesame（sesame.com/research/crossing_the_uncanny_valley_of_voice），其中李宏毅老师认为Sesame是当前最流畅的语音语言模型。
3. **核心逻辑**：与语言模型类似，语音模型通过“输入Token→输出Token”流程工作，输入声音经tokenization生成speech token，输出token经detokenization转化为人类可听懂的声音；训练流程分三步——无监督语音数据预训练、人类标注语音数据SFT（监督微调）、可选的RLHF（基于人类反馈的强化学习）。


## 二、语音生成基本单位：Speech Token
1. **核心问题**：需在“保留语音关键信息”与“压缩Token序列长度”间平衡，避免两种极端情况：
   - 极端1：先将声音转文本（ASR）、对文本分词生成Token，再通过TTS转声音——本质是“Text LLM”，会丢失语气等语音关键信息。
   - 极端2：直接输入声音取样点（每秒至少8000个），1分钟语音对应50万Token，序列过长不切实际。
2. **Tokenization方法评估基准**：
   - **Codec-SUPERB**：先对声音做tokenizer生成Token，再经de-tokenizer还原声音，通过对比输出音质、输入输出声音在现有语音模型（如语音辨识系统）中的识别结果损失，评估方法优劣。
   - **DASB**：将声音转Token后，分别训练语音辨识系统（验证Token是否含文字信息）、情绪识别系统（验证Token是否含情绪信息），以训练效果评估方法。


## 三、语音生成Token的两大核心方式
### （一）SSL（Speech Self-supervised Learning Model）
1. **流程**：
   1. 输入声音至预训练的SSL模型，生成连续的vector sequence（通常0.02秒对应1个vector），SSL模型相关可参考李宏毅老师2022年综述及课程。
   2. 对vector做Quantization（向量聚类），用ID/Token表示每个聚类类别。
   3. 可选步骤：用Deduplicate去除重复ID、用BPE合并高频ID，进一步缩短序列，最终得到离散Token序列。
2. **关键特点**：tokenization无需训练，但对应的detokenization需训练；生成的Token常被称为“Sematic Token”，虽名称含“语义”，但更偏向捕捉发音基本单位（音速），也可包含情绪信息。

### （二）Neural Speech Codec
1. **核心逻辑**：tokenizer与detokenizer联合训练，流程类似autoencoder（自编码器）——声音经tokenizer生成Token序列，Token序列经detokenizer还原为声音讯号，支持将latent representation（ latent表示）转化为离散形式的autoencoder可作为该方法模型。
2. **关键特点**：
   - “Codec”意为“compression（压缩）+ decompression（解压）”。
   - 生成的Token常被称为“Acoustic Token”，不仅含情绪信息，还包含内容、音速等信息。
   - 一段声音可抽出多个Token，分别对应声音不同维度信息（如用1个Token学习SSL的“Sematic Token”，其他Token捕捉其他信息），具体可参考RVQ（Residual vector Quantization）。

### （三）Token结合与流式输出优化
1. **多Token层级结合**：热门方向是结合不同粗细层级的Token，如LLM模型依次生成“粗Token（内容）→细Token（韵律）”，AudioLM、VALLE均采用类似思路；LLM可单模型生成多类Token，也可多模型分工（如VALLE用non-autoregressive模型生成细Token，速度快于autoregressive模型）。
2. **流式输出解决方案**：传统多Token生成需全部Token完成后才能detokenize，无法流式输出（边生成边输出），优化方法包括：
   - 生成首个粗Token后，同步生成首个细Token，直接detokenize首个Token。
   - 采用Acoustic Delay方法：先生成第1个粗Token→再生成第2个粗Token+第1个细Token→接着生成第3个粗Token+第2个细Token+第1个更细Token，依次类推。
   - 双Transformer架构：Temporal Transformer输出vector给Depth Transformer，后者同步生成同一位置的粗、细、更细Token。

### （四）离散Token与连续向量的选择
1. **输入端**：离散Token与连续向量差异不大，且连续向量因无信息丢失，效果可能更优。
2. **输出端**：
   - 离散Token优势：生成时需支持“同一输入对应不同合理输出”，离散Token通过概率分布输出明确类别（如要么Token1、要么Token2），避免连续向量的“平均化错误”（如模型试图同时接近两个正确连续向量，最终输出错误平均值）。
   - 连续向量潜力：可通过改造loss函数，让模型输出仅接近正确答案，该思路在图片生成中可行，语音领域已开始相关研究。


## 四、网页其他信息
1. **发布与作者**：发布于2025年7月20日（北京），作者“codingling”，公众号为“L的AI实验室”，收录于“台大李宏毅课程笔记”专栏（专栏含7篇内容，获49赞同），本文获3人赞同。
2. **延伸推荐**：包括《本周值得关注的语音方向论文【2022/08/09】》《历史最全语音增强必读论文、数据集、工具包、书籍及应用整理分享》《LLaSO 横空出世：逻辑智能推出全球首个完全开源语音大模型框架》等语音领域相关内容。
