{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 如何在ReAct智能体中管理对话历史\n",
        "\n",
        "!!! info \"先决条件\"\n",
        "    本指南假设您熟悉以下内容：\n",
        "\n",
        "    - [预构建的create_react_agent](../create-react-agent)\n",
        "    - [持久化](../../concepts/persistence)\n",
        "    - [短期内存](../../concepts/memory/#short-term-memory)\n",
        "    - [消息修剪](https://python.langchain.com/docs/how_to/trim_messages/)\n",
        "\n",
        "消息历史可能会快速增长并超过LLM上下文窗口大小，无论您是在构建具有多次对话轮次的聊天机器人，还是构建具有大量工具调用的智能体系统。有几种管理消息历史的策略：\n",
        "\n",
        "* [消息修剪](#keep-the-original-message-history-unmodified) — 删除历史记录中的前N条或后N条消息\n",
        "* [摘要](#summarizing-message-history) — 总结历史记录中的早期消息并用摘要替换它们\n",
        "* 自定义策略（例如，消息过滤等）\n",
        "\n",
        "要在`create_react_agent`中管理消息历史，您需要定义一个`pre_model_hook`函数或[可运行对象](https://python.langchain.com/docs/concepts/runnables/)，该函数接受图状态并返回状态更新：\n",
        "\n",
        "\n",
        "* 修剪示例：\n",
        "    ```python\n",
        "    # highlight-next-line\n",
        "    from langchain_core.messages.utils import (\n",
        "        # highlight-next-line\n",
        "        trim_messages, \n",
        "        # highlight-next-line\n",
        "        count_tokens_approximately\n",
        "    # highlight-next-line\n",
        "    )\n",
        "    from langgraph.prebuilt import create_react_agent\n",
        "    \n",
        "    # 此函数将在调用LLM的节点之前每次被调用\n",
        "    def pre_model_hook(state):\n",
        "        trimmed_messages = trim_messages(\n",
        "            state[\"messages\"],\n",
        "            strategy=\"last\",\n",
        "            token_counter=count_tokens_approximately,\n",
        "            max_tokens=384,\n",
        "            start_on=\"human\",\n",
        "            end_on=(\"human\", \"tool\"),\n",
        "        )\n",
        "        # 您可以在`llm_input_messages`或`messages`键下返回更新的消息\n",
        "        # （请参见下面的注释）\n",
        "        # highlight-next-line\n",
        "        return {\"llm_input_messages\": trimmed_messages}\n",
        "\n",
        "    checkpointer = InMemorySaver()\n",
        "    agent = create_react_agent(\n",
        "        model,\n",
        "        tools,\n",
        "        # highlight-next-line\n",
        "        pre_model_hook=pre_model_hook,\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "    ```\n",
        "\n",
        "* 摘要示例：\n",
        "    ```python\n",
        "    # highlight-next-line\n",
        "    from langmem.short_term import SummarizationNode\n",
        "    from langchain_core.messages.utils import count_tokens_approximately\n",
        "    from langgraph.prebuilt.chat_agent_executor import AgentState\n",
        "    from langgraph.checkpoint.memory import InMemorySaver\n",
        "    from typing import Any\n",
        "    \n",
        "    model = ChatOpenAI(model=\"gpt-4o\")\n",
        "    \n",
        "    summarization_node = SummarizationNode(\n",
        "        token_counter=count_tokens_approximately,\n",
        "        model=model,\n",
        "        max_tokens=384,\n",
        "        max_summary_tokens=128,\n",
        "        output_messages_key=\"llm_input_messages\",\n",
        "    )\n",
        "\n",
        "    class State(AgentState):\n",
        "        # 注意：我们添加此键来跟踪先前的摘要信息\n",
        "        # 以确保我们不会在每次LLM调用时都进行摘要\n",
        "        # highlight-next-line\n",
        "        context: dict[str, Any]\n",
        "    \n",
        "    \n",
        "    checkpointer = InMemorySaver()\n",
        "    graph = create_react_agent(\n",
        "        model,\n",
        "        tools,\n",
        "        # highlight-next-line\n",
        "        pre_model_hook=summarization_node,\n",
        "        # highlight-next-line\n",
        "        state_schema=State,\n",
        "        checkpointer=checkpointer,\n",
        "    )\n",
        "    ```\n",
        "\n",
        "!!! Important\n",
        "    \n",
        "    * 要在图状态中**保持原始消息历史不变**并**仅将更新的历史作为LLM的输入**传递，请在`llm_input_messages`键下返回更新的消息\n",
        "    * 要用更新的历史**覆盖图状态中的原始消息历史**，请在`messages`键下返回更新的消息\n",
        "    \n",
        "    要覆盖`messages`键，您需要执行以下操作：\n",
        "\n",
        "    ```python\n",
        "    from langchain_core.messages import RemoveMessage\n",
        "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
        "\n",
        "    def pre_model_hook(state):\n",
        "        updated_messages = ...\n",
        "        return {\n",
        "            \"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *updated_messages]\n",
        "            ...\n",
        "        }\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 设置\n",
        "\n",
        "首先，让我们安装所需的包并设置API密钥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain-openai langmem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"admonition tip\">\n",
        "    <p class=\"admonition-title\">为LangGraph开发设置 <a href=\"https://smith.langchain.com\">LangSmith</a></p>\n",
        "    <p style=\"padding-top: 5px;\">\n",
        "        注册LangSmith以快速发现问题并提高LangGraph项目的性能。LangSmith让您使用跟踪数据来调试、测试和监控使用LangGraph构建的LLM应用程序——阅读更多关于如何开始的信息 <a href=\"https://docs.smith.langchain.com\">这里</a>。 \n",
        "    </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 保持原始消息历史不变\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们构建一个带有管理对话历史步骤的ReAct智能体：当历史长度超过指定的令牌数量时，我们将调用[`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)实用程序，该程序将减少历史记录，同时满足LLM提供商的约束。\n",
        "\n",
        "在ReAct智能体内部应用更新的消息历史有两种方式：\n",
        "\n",
        "  * [**保持原始消息历史不变**](#keep-the-original-message-history-unmodified)在图状态中，并**仅将更新的历史作为LLM的输入**传递\n",
        "  * [**用更新的历史覆盖原始消息历史**](#overwrite-the-original-message-history)在图状态中\n",
        "\n",
        "让我们从实现第一个开始。我们首先需要为我们的智能体定义模型和工具：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"使用此工具获取天气信息。\"\"\"\n",
        "    if any([city in location.lower() for city in [\"nyc\", \"new york city\"]]):\n",
        "        return \"纽约市可能多云，有下雨的可能，温度高达80度。\"\n",
        "    elif any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
        "        return \"旧金山总是阳光明媚\"\n",
        "    else:\n",
        "        return f\"我不确定{location}的天气情况\"\n",
        "\n",
        "\n",
        "tools = [get_weather]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在让我们实现`pre_model_hook`——一个将作为新节点添加并在调用LLM的节点（`agent`节点）**之前**每次被调用的函数。\n",
        "\n",
        "我们的实现将包装`trim_messages`调用并在`llm_input_messages`下返回修剪后的消息。这将**在图状态中保持原始消息历史不变**并**仅将更新的历史作为LLM的输入**传递\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# highlight-next-line\n",
        "from langchain_core.messages.utils import (\n",
        "    # highlight-next-line\n",
        "    trim_messages,\n",
        "    # highlight-next-line\n",
        "    count_tokens_approximately,\n",
        "    # highlight-next-line\n",
        ")\n",
        "\n",
        "\n",
        "# 此函数将作为ReAct智能体图中的新节点添加\n",
        "# 在调用LLM的节点之前每次运行。\n",
        "# 此函数返回的消息将是LLM的输入。\n",
        "def pre_model_hook(state):\n",
        "    trimmed_messages = trim_messages(\n",
        "        state[\"messages\"],\n",
        "        strategy=\"last\",\n",
        "        token_counter=count_tokens_approximately,\n",
        "        max_tokens=384,\n",
        "        start_on=\"human\",\n",
        "        end_on=(\"human\", \"tool\"),\n",
        "    )\n",
        "    # highlight-next-line\n",
        "    return {\"llm_input_messages\": trimmed_messages}\n",
        "\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = create_react_agent(\n",
        "    model,\n",
        "    tools,\n",
        "    # highlight-next-line\n",
        "    pre_model_hook=pre_model_hook,\n",
        "    checkpointer=checkpointer,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
