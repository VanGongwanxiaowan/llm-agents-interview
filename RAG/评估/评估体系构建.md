# RAG评估体系搭建


## 一、嵌入模型选择
不同嵌入模型在多语言支持、中文表现、适用场景等方面各有特点，具体如下：

|模型名称|主要特点|中文领域表现|适用场景|
| ---- | ---- | ---- | ---- |
|Qwen3-Embedding-8B|8B参数量，多语言SOTA，支持100+语言，长文本理解，指令支持，可自定义维度|目前MTEB多语言榜单第一，中文性能极佳|对性能要求极高的通用中文任务，RAG，需要灵活维度和指令控制的场景|
|BGE-M3|多语言（100+），多功能（稠密/多向量/稀疏检索），多粒度（8192 token），指令微调|在中文RAG和通用检索任务中表现出色|需要处理多语言、长文本、多种检索方式（混合检索）的中文RAG和信息检索系统|
|BCEmbedding|双语（中/英）和跨语言能力强，RAG基石，双编码器+交叉编码器，领域适应性强，无指令|在中英文双语和跨语言任务上表现优异，尤其适合RAG|中英文混合或跨语言场景，RAG强调高效和精确检索|
|BGE-Large-Zh|中文专属优化，1024维，512 token最大长度|中文语义理解和检索的强大选择|专注于中文的语义搜索、相似计算、文本分类等任务|
|M3E-base|中英文混合训练，轻量级（110M参数），高效推理，指令数据训练|中文文本分类和检索方面性能优异|资源受限但需要中英文混合处理的场景，快速推理、通用中文语义任务|
|Nomic Embed Text V1.5|多语言（100+），长序列（8192 token），Matryoshka灵活维度，多模态（与Vision对齐），任务类型优化，开源|在中文通用任务上有竞争力，但可能不如中文专属模型|需要处理长文本、多语言、灵活维度选择的通用语义任务，多模态需求场景|


## 二、黄金评估数据集构建
黄金评估数据集是RAG评估的基础，构建流程及最佳实践如下：

### 1. 构建步骤
- **确定业务领域**：明确数据集的应用场景（如法律问答、金融资讯、企业文档助手等），聚焦核心业务以保证数据集意义。
- **准备问题答案**：
  - 方法：手动构造（与业务专家合作，确保内容清晰权威并标明出处）、寻找开源数据集（如Natural Questions、HotpotQA、SQUAD等）。
  - 样例：`{"question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？", "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年”}`
- **标注支持文档**：这是关键步骤，需明确答案的出处。数据结构示例：
  ```json
  {
    "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？",
    "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年”
    "supporting_docs": [
      "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’尹天仇听后愣住",
      "尹天仇从床上坐起，刚想说话，却被柳飘飘打断。她刻意用轻松的语气掩饰自己的复杂情绪。",
      "这个桥段展现了两人情感的错位，尹天仇认真投入，而柳飘飘则用防备和自嘲隐藏内心。"
    ],
    "doc_ids": ["scene_26", "scene_26_note", "analysis_emotion_26"],
    "metadata": {
      "movie": "喜剧之王",
      "characters": ["尹天仇", "柳飘飘"],
      "scene": "一夜情后",
      "emotion": "情感错位 / 冷漠掩饰",
      "difficulty": "较高"
    }
  }
  ```
- **数据质量校验**：多角色（客户、业务专家、算法专家等）参与，检查内容准确性、逻辑合理性、完整性、冗余性等，可结合人工和大模型评估。


### 2. 黄金数据特点与扩展
- **黄金数据特点**：问题真实自然、答案准确、支持文档完整、可评估性强。
- **数据扩展**：基于已有黄金数据，扩展相关场景（如同一主题的不同问题），形成上下文相关或推理型数据。


### 3. 最佳实践
- **数量**：先收集10-50条样本，每条包含问题、标准答案、2-3条支持文档、文档索引ID、元数据。
- **样本选择**：优先高频问题，问题自然符合真人口吻，文档片段不宜过长，答案需从支持文档推出，避免常识题。
- **工具使用**：Excel/Notion手动标注小批量数据；Label Studio开源平台标注；GPT-4/Claude生成初步QA对后人工审核；Haystack/LlamaIndex半自动获取数据集。


## 三、检索评估
检索评估用于评估从知识库检索结果的有效性，核心是衡量检索的“准”“全”及结果排序质量。

### 1. 核心指标
|指标名|中文解释|举例|用途|
| --- | --- | --- | --- |
|Recall@k|前k个检索结果中是否有相关文档|检索前5个段落，1个是答案出处→Recall@5=1（命中）|检查是否返回相关文档|
|Precision@k|前k个检索结果中“真正相关”的比例|检索5个段落，2个相关→Precision@5=0.4|衡量检索准确性|
|Hit@k|前k个结果中是否至少有1个命中正确文档（命中=1，否则=0）|正确段落排第4名→Hit@5=1|粗略统计检索命中率|
|MRR (Mean Reciprocal Rank)|正确文档排名越靠前，分数越高|正确答案排第1名得1分，第2名得0.5分→排名第4得0.25|衡量正确答案排序质量|
|NDCG@k（选用）|结合相关性和排名的加权评价|比MRR更细致，适用于多相关文档场景|大型系统中常用|


### 2. 评估流程
1. 构建检索黄金数据集；
2. 运行RAG检索系统，输入问题返回前k个结果；
3. 计算评估指标，对比系统返回文档与真实答案。


### 3. 提升策略
- **提高召回率**：
  - 优化分块策略（重叠分块、语义/句子分块、多粒度分块）；
  - 改进检索器（尝试不同嵌入模型、微调模型、混合检索（如BM25+向量检索））；
  - 扩大检索范围（增加top-k、降低相似度阈值）；
  - 进行查询扩展/改写。
- **提升准确率**：
  - 重排序（用Cross-Encoder等模型对候选文档精细排序）；
  - 优化检索器（更好的嵌入模型或微调）；
  - 收紧检索范围（提高相似度阈值、减少top-k）；
  - 元数据过滤；
  - 优化分块策略（保证语义完整性）。


## 四、生成评估
生成评估关注基于检索文档生成的答案是否准确、忠实、流畅、完整。

### 1. 评估目标
- 准确性：生成结果是否正确、基于事实、无偏差；
- 忠实性：是否基于知识库回答，无胡编乱造；
- 表达性：语言是否流畅、易理解；
- 完整性：是否答全，无关键信息遗漏。


### 2. 评估方法
|方法|说明|特点|
| ---- | ---- | ---- |
|机器评估指标|基于统计或向量计算生成内容与参考答案的匹配度| - BLEU/ROUGE/METEOR：快但偏字面；<br>- BERTScore：基于语义相似度，更准确；<br>- QA-based eval：适合问答系统|
|LLM-as-a-Judge|用GPT-4、Claude等大模型对生成结果打分（需设计提示词）|方便、经济、智能，缺点是结果可能不稳定（可多模型评审取平均）|
|人工评估|人工从正确性、忠实性、表达性、完整性等维度打分（1-5分）|最权威但成本最高|


### 3. 评估流程
1. 构建黄金数据集；
2. 运行RAG生成答案；
3. 用语义相似度（如BERTScore）、LLM评分、人工抽样评估；
4. 记录错误类型（幻觉、漏答、答非所问等）。


### 4. 最佳实践
|问题|原因|解决方案|
| ---- | ---- | ---- |
|编造信息|检索内容未覆盖，或模型太自由|限制Prompt：只允许基于文档回答|
|回答太短|Prompt不完整|明确要求“解释原因+引用原文”|
|幻觉|检索+Prompt双问题|优化问题匹配并进行问题指引|


## 五、端到端评估（E2E Evaluation）
从用户视角整体评估RAG系统的最终输出，关注用户体验和产品价值。

### 1. 评估目标
|维度|解释|举例问题|
| ---- | ---- | ---- |
|正确性|回答是否客观真实，有无事实错误|“是不是在胡编乱造？”|
|忠实性|回答是否基于检索内容|“是不是脱离材料自由发挥了？”|
|有用性|回答对用户是否有价值|“这回答对我有没有用？”|
|完整性|有无漏答关键点|“说全了吗？有没有跳过重要信息？”|
|可读性|回答是否流畅、易读|“像不像一个正常人写的？”|


### 2. 评估方法
- **人工打分**：适合高质量、小批量数据集，按上述维度1-5分打分（如用Excel、问卷等工具），精准但成本高。
- **LLM-as-a-Judge**：用GPT-4/Claude等模型评分（需设计提示词），如Ragas工具内置评估模块，快且可扩展但需抽样校验。
- **用户反馈评分**：系统上线后收集用户👍/👎，实用价值最高但不系统、偏主观。


### 3. 最佳实践
|问题类型|解决方式|
| ---- | ---- |
|没有命中支持内容|提升检索Recall，加入reranker|
|胡说八道/编故事|Prompt限制只基于文档生成|
|回答不完整|Prompt引导“逐点展开”|
|语言怪异/风格不自然|模型fine-tune或prompt精调|


## 六、RAG评估全流程代码实战（基于《喜剧之王》示例）
```python
from typing import List, Dict
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from difflib import SequenceMatcher

# --- 1. 准备数据集（黄金数据集） ---
golden_data = [
    {
        "question": "《喜剧之王》中柳飘飘一夜情后对尹天仇说了什么？",
        "answer": "在电影《喜剧之王》中，一夜情之后，柳飘飘对尹天仇说：“你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。”",
        "supporting_docs": [
            "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
            "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
        ]
    }
]

# --- 2. 检索模拟 ---
encoder = SentenceTransformer('all-MiniLM-L6-v2')
# 模拟知识库
document_store = [
    "柳飘飘第一次见尹天仇是在酒吧的后台。",
    "第二天清晨，柳飘飘看着尹天仇说：‘你不会真的以为我喜欢你吧？大家都是成年人，玩玩而已。’",
    "尹天仇梦想成为演员，坚持每天排练台词。",
    "她刻意用轻松的语气掩饰自己的复杂情绪，试图拉开距离。"
]

def retrieve(question: str, top_k=3) -> List[str]:
    """通过语义相似度检索top_k文档"""
    q_emb = encoder.encode([question])
    doc_embs = encoder.encode(document_store)
    scores = cosine_similarity(q_emb, doc_embs)[0]
    top_indices = scores.argsort()[::-1][:top_k]
    return [document_store[i] for i in top_indices]

# --- 3. 生成模拟 ---
def generate_answer(retrieved_docs: List[str]) -> str:
    """拼接检索文档作为生成结果（实际需替换为LLM调用）"""
    return "基于资料回答：" + " | ".join(retrieved_docs)

# --- 4. 评估 ---
def evaluate_retrieval(gt_docs: List[str], retrieved_docs: List[str]) -> float:
    """评估检索是否命中（文本相似度>0.8视为命中）"""
    for gt in gt_docs:
        for r in retrieved_docs:
            if SequenceMatcher(None, gt, r).ratio() > 0.8:
                return 1.0  # Hit@k
    return 0.0

def evaluate_generation(golden: str, generated: str) -> float:
    """计算生成答案与真实答案的文本相似度"""
    return SequenceMatcher(None, golden, generated).ratio()

# --- 5. 端到端执行 ---
if __name__ == "__main__":
    for sample in golden_data:
        question = sample["question"]
        gt_answer = sample["answer"]
        gt_docs = sample["supporting_docs"]
        
        print("\n=== 问题 ===")
        print(question)
        
        # 检索
        retrieved = retrieve(question)
        print("\n检索结果：")
        for d in retrieved:
            print("-", d)
        
        # 检索评估
        hit_score = evaluate_retrieval(gt_docs, retrieved)
        print(f"\n检索 Hit@k: {hit_score}")
        
        # 生成
        gen_answer = generate_answer(retrieved)
        print("\n生成答案：")
        print(gen_answer)
        
        # 生成评估
        gen_score = evaluate_generation(gt_answer, gen_answer)
        print(f"\n生成相似度得分：{round(gen_score, 3)}")
```

### 代码说明
1. **数据准备**：定义包含问题、答案、支持文档的黄金数据集；
2. **检索模拟**：用`SentenceTransformer`编码文本，通过余弦相似度实现语义检索；
3. **生成模拟**：拼接检索结果作为生成答案（实际需替换为LLM调用）；
4. **评估逻辑**：检索评估通过文本相似度判断是否命中；生成评估计算文本相似度；
5. **端到端执行**：依次执行检索、检索评估、生成、生成评估，输出全流程结果。
