### 字节大模型应用一面面试题目详细解答

#### 自我介绍 + 项目介绍（30-40分钟）
- **重点突出**：大模型相关项目经验、技术难点、个人贡献
- **建议准备**：项目架构图、关键技术选型、性能优化点

#### 手撕和八股同时进行

##### 1. 写一下交叉熵公式
**交叉熵损失函数**：
```
CrossEntropy = -Σ(y_i * log(ŷ_i))
```
其中：
- `y_i` 是真实标签的one-hot编码
- `ŷ_i` 是预测的概率分布
- 求和是对所有类别进行

**二分类交叉熵**：
```
L = -[y * log(ŷ) + (1-y) * log(1-ŷ)]
```

##### 2. SGD，Adam和AdamW说一下有什么不同

**SGD（随机梯度下降）**：
- 最基本的优化算法：`θ = θ - η * ∇J(θ)`
- **优点**：简单，内存占用小
- **缺点**：容易陷入局部最优，学习率难选择

**Adam（Adaptive Moment Estimation）**：
- 结合动量（Momentum）和RMSProp
- 计算一阶矩（均值）和二阶矩（未中心化方差）的指数移动平均
- 公式：
  ```
  m_t = β1 * m_{t-1} + (1-β1) * g_t
  v_t = β2 * v_{t-1} + (1-β2) * g_t²
  θ_t = θ_{t-1} - η * m_t / (√v_t + ε)
  ```

**AdamW（Adam with Weight Decay）**：
- 在Adam基础上改进了权重衰减（Weight Decay）的实现
- **区别**：
  - Adam：权重衰减与梯度更新耦合
  - AdamW：权重衰减与梯度更新解耦，直接加到参数更新中
- **优势**：更好的泛化性能，训练更稳定

##### 3. Transformer结构说一下，写一下transformer伪代码

**Transformer核心结构**：
- Encoder-Decoder架构
- 自注意力机制（Self-Attention）
- 前馈神经网络（FFN）
- 残差连接和层归一化

**Transformer伪代码**：
```python
class Transformer:
    def __init__(self, d_model, n_heads, d_ff, n_layers):
        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff)
        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff)
    
    def forward(self, src, tgt, src_mask, tgt_mask):
        enc_output = self.encoder(src, src_mask)
        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)
        return output

class MultiHeadAttention:
    def __init__(self, d_model, n_heads):
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.W_q = Linear(d_model, d_model)  # Q投影
        self.W_k = Linear(d_model, d_model)  # K投影  
        self.W_v = Linear(d_model, d_model)  # V投影
        self.W_o = Linear(d_model, d_model)  # 输出投影
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # 线性变换并分头
        Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 缩放点积注意力
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn_weights = F.softmax(scores, dim=-1)
        
        # 注意力加权
        context = torch.matmul(attn_weights, V)
        
        # 合并多头
        context = context.transpose(1, 2).contiguous().view(
            batch_size, -1, self.n_heads * self.d_k)
        
        output = self.W_o(context)
        return output
```

##### 4. GRPO原理说一下

**GRPO（Group Policy Optimization）**：
- 基于分组策略的强化学习优化方法
- **核心思想**：
  - 将策略网络分成多个组（Group）
  - 每个组独立学习和探索
  - 通过组间竞争和合作提升整体性能
- **优势**：
  - 增强探索能力，避免早熟收敛
  - 提高训练稳定性和样本效率
- **应用场景**：大语言模型强化学习、多智能体系统

#### 最后两道力扣

##### 第一道：DP题目
**典型DP问题分析框架**：
```python
def dp_solution(self, params):
    # 1. 定义dp数组含义
    n = len(params)
    dp = [0] * (n + 1)
    
    # 2. 初始化边界条件
    dp[0] = base_case
    
    # 3. 状态转移方程
    for i in range(1, n + 1):
        for j in range(i):
            dp[i] = max/min(dp[i], dp[j] + transition)
    
    # 4. 返回结果
    return dp[n]
```

##### 第二道：二分查找
**二分查找模板**：
```python
def binary_search(self, nums, target):
    left, right = 0, len(nums) - 1
    
    while left <= right:
        mid = left + (right - left) // 2
        
        if nums[mid] == target:
            return mid
        elif nums[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1  # 或返回插入位置
```

---

**面试准备建议**：
1. **大模型基础**：深入理解Transformer、注意力机制、优化算法
2. **数学基础**：掌握损失函数、优化算法的数学原理
3. **编码能力**：熟练DP、二分、图论等高频算法题型
4. **项目表达**：能用简洁语言说清项目价值和技术难点
