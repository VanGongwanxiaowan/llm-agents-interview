大模型应用开发面经 （5年经验）
时间范围

近半年

实际面过的公司

阿里、腾讯、美团、字节、快手、同程、京东、360、keep、滴滴、印象笔记、作业帮、彩云科技、蓝色光标、江城互娱、Aviagames、Hungry Stdios、深言科技、即时科技、RockFlow、格灵深瞳、百融云创、印象笔记、网龙、 HiDream.ai、昆仑万维、数驱互动、Authing

先说总结

面试内容 &amp; 难度

个人觉得，在llm应用的面试题上，没有太多复杂、高深的问题，不像上来让你说一下分布式锁怎么设计然后死扣设计细节或是描述一下MVCC原理这种偏高难度的八股文问题（当然也遇到了一两次），究其原因以下几点，一是大模型应用目前仍没有很成熟且被广泛接纳的方案，都还在探索；二是很多公司今年刚开始all in AI（我司all进去的比较早点），面试官也懂得不多，例如RAG这个东西，大部分的面试题无非是“你觉得RAG中最难的是什么？（文档切割喽）”、“你是怎么解决幻觉问题的？”，“微调和RAG的区别是啥？”等等，如果你做过RAG加上你经常看技术文章结合你的“侃侃而谈”，基本面试官都觉得ok。但这里着重说一下我觉得当前非常重要且极大概率提升面试通过率的的一个技术点，就是掌握微调原理并且做过动手做过微调工作再加上动手部署过大模型，这是我面试中最常被问到而又只能说没做过的问题，当然大部分公司都有专门的算法团队去做这件事，自己到没机会参与其中，也是可以理解的。

算法题：一半是DP问题，还有一部分难度是easy的问题，总体上都是“老熟人”，但是，你即使写出来，面试不一定就能过，有的干脆就不考算法题。

八股文：明显比之前少很多，这个和面试的岗位有关系，LLM应用的岗位更偏实践，所有很多一面就是leader面，直接问项目，除非一面也不懂LLM的东西，就会考八股文，但总的来说，八股少了，但是绝对不可以不准备，好几次挂在这上面，别小瞧它。

岗位内容：

游戏公司：基本上是LLM + AB test for 游戏策划；BI 分析；游戏社区客服助手；

toC: Agent 个人助手

toB: Agent for 解决方案

other: 通用 Agent 平台；公司内部AI助手、平台；Agent for 运维

offer

会有很多横向对比，如果你期望薪资比较高，对方说要在等等，基本上凉了。

大部分涨幅基本是不到20%的，但我的期望是30%左右，最后还是拿到了（要有一点点耐心，还要有一定的运气）。

不要眼高手低，先拿一个低于自己预期的offer，再慢慢谈，前提是公司想要你。

规划好时间，集中面试，集中对比，由于我时间线拉的过长，后面安排的很乱。

再总结

每次面完都要复盘，没答好的问题，一定要重新梳理答案。

没把握问题的可以直接说不会，别给个你自己都听不懂的答案。

简历一定要让大模型润色，但自己要check一遍，别吹过头了。

多看技术文章，扩展技术视野，提高二面面试官对你的印象。

表达一定要流畅清晰，不要断断续续的，面试官会觉得你思路不清晰。

项目效果评估是个很重要的问题，不管你的技术多炫酷，终究还是要看效果，看落地效果。

面试题

这里想到多少写多少

LLM 基础

大模型是怎么训练出来的？

Transform 的架构，Encoder 和 Decoder 是什么？

Function Call 是怎么训练的？

微调的方案有哪些？自己做过没有？

大模型分词器是什么？

Embedding 是什么？你们用的那个模型？

Lib

介绍一下 langchian

介绍一下 autogen

有没有用过大模型的网关框架（litellm）

为什么手搓agent，而不是用框架？

mcp 是什么？和Function Call 有什么区别？有没有实践过？

A2A 了解吗？

Prompt

ReAct 是啥？怎么实现的？

CoT 是啥？为啥效果好呢？有啥缺点？

Prompt Caching 是什么？

温度值/top-p/top-k 分别是什么？各个场景下的最佳设置是什么？

RAG

你介绍一下RAG 是什么？最难的地方是哪？

文档切割策略有哪些？怎么规避语义被切割掉的问题？

多路召回是什么？

文档怎么存的？粒度是多大？用的什么数据库？

为啥要用到图数据库？

向量数据库的对比有没有做过？Qdrant 性能如何？量级是多大？有没有性能瓶颈？

怎么规避大模型的幻觉？

微调和RAG的优劣势？

怎么量化你的回答效果？ 例如检索的效果、回答的效果。

Workflow

怎么做的任务拆分？为什么要拆分？ 效果如何？怎么提升效果？

text2sql 怎么做的？怎么提高准确率？

如何润色query，目的是什么？

code-generation 是什么做的？如何确保准确性？

现在再让你设计你会怎么做？（replan）

效果是怎么量化的？

Agent

介绍一下你的 Agent 项目

长短期记忆是怎么做的？记忆是怎么存的？粒度是多少？怎么用的？

Function Call 是什么做的？

你最大的难题是什么？你是怎么提高效果的？怎么降低延迟的？

端到端延迟如何优化的？

介绍一下single-agent、multi-agent的设计方案有哪些？

反思机制是什么做的？为什么要用反思？

如何看待当下的LLM应用的趋势和方向

为什么要用Webrtc？它和ws的区别是什么？

agent服务高可用、稳健性是怎么保证的？

llm 服务并发太高了怎么办？

系统设计题

短链系统

分布式锁的设计

给你一部长篇小说，怎么做文档切割？

怎么做到论文翻译，并且格式尽可能和原来的统一

游戏社区客服助手设计。如何绑定游戏黑话，如何利用好公司内部的文档

结合线上问题快速定位项目工程代码有问题的地方

有很多结构化和非结构化数据，怎么分析，再怎么得出我要的结论。

八股

go的内存分配策略、GMP、GC

python 的内存分配策略、GC

redis 用过那些？mget 底层什么实现的？、zset怎么实现的？

mysql 索引怎么设计最好？数据库隔离级别？mvcc是怎么实现的？

分布式锁是什么实现的？

kafka的 reblance 是什么？会产生那些问题？怎么保证数据不丢?

fastapi 设计原理？

go 中 net/http 如何处理的tcp粘包问题

http2 是什么？比http1.1有什么优势？

Linux 网络性能调优的方式

如何定位Linux中的pid、端口号等等

个人

在每个项目的里的角色是什么？承担那些工作？项目是几个人在做？

为什么离职、每次离职的原因是什么？

平常怎么学习的？怎么接触到大模型的最新进展的？

对大模型将来的应用发展有什么看法？

你将来的职业规划是什么？


作者：大模型小坛
链接：https://www.nowcoder.com/feed/main/detail/129eaa1c20444651ac3b932e200d3da4?sourceSSR=search
来源：牛客网


好的，这是一场非常典型的大模型/AI应用开发岗位的面试。我们来逐一详细拆解这些问题，帮你构建一个系统化的认知。

---

### 1. 大模型是怎么训练出来的？

大模型的训练是一个庞大且耗资巨大的系统工程，通常分为几个核心阶段：

**1. 预训练 (Pre-training)**
*   **目标：** 让模型学习语言的通用表示、语法、 facts 和一定的推理能力。这是最耗时、最费钱的阶段。
*   **数据：** 使用超大规模、高质量的网络文本、书籍、代码等数据集（如The Pile, C4）。
*   **方法：** **自监督学习**。不需要人工标注。
    *   **对于类似GPT的解码器模型：** 使用**因果语言建模 (Causal Language Modeling)** 目标。给定前文，预测下一个最可能的词（Token）。这就像是“完形填空”，模型学会了语言的概率分布和上下文关系。
    *   **对于类似BERT的编码器模型：** 使用**掩码语言建模 (Masked Language Modeling)** 目标。随机掩盖输入中的一些词，让模型根据上下文来预测被掩盖的词。
*   **结果：** 产出一个**基座模型 (Base Model)**，如 LLaMA、GPT、ChatGLM-base。

**2. 有监督微调 (Supervised Fine-Tuning, SFT)**
*   **目标：** 教会模型如何遵循人类的指令，并以期望的格式（如对话、摘要、推理链）进行回应。让基座模型变得“有用”。
*   **数据：** 高质量的**指令-回答对** 数据集，由人类专家编写或从高质量资源中收集（如Alpaca、ShareGPT数据）。
*   **方法：** **监督学习**。输入指令，模型生成回答，其输出与人类编写的标准答案进行对比，通过计算交叉熵损失来更新模型权重。
*   **结果：** 产出一个**SFT模型**，如 Alpaca、ChatGLM-SFT。它能听懂指令，但回答可能不稳定或不安全。

**3. 对齐微调 (Alignment Tuning) / 人类反馈强化学习 (RLHF)**
*   **目标：** 进一步对齐模型的输出与人类的偏好（ helpful, honest, harmless）。让模型变得“好用且安全”。
*   **方法：** 通常使用 **RLHF**，这是一个多步骤的过程：
    *   **a. 训练奖励模型 (Reward Model, RM)：** 收集人类对多个模型回答的偏好排序数据（A回答 > B回答 > C回答）。训练一个单独的模型（RM）来学习人类的偏好，使其能够对任何回答给出一个偏好分数。
    *   **b. 强化学习优化：** 使用**近端策略优化 (PPO)** 等强化学习算法，以RM的评分作为奖励信号，去优化SFT模型。模型尝试生成各种回答，好的回答会获得高奖励从而被鼓励，坏的回答会获得低奖励从而被抑制。
*   **替代方案：** **直接偏好优化 (DPO)**。一种更简单、更稳定的方法，无需训练单独的奖励模型，直接利用偏好数据来优化模型。
*   **结果：** 产出入ChatGPT、Claude这样的**对话模型**。

**简单总结：** `海量无标注数据预训练` -> `指令数据有监督微调` -> `人类偏好数据对齐`。

---

### 2. Transform 的架构，Encoder 和 Decoder 是什么？

**Transformer** 是谷歌在2017年《Attention is All You Need》论文中提出的革命性模型架构，它是几乎所有现代大模型的基础。

它的核心是**自注意力机制 (Self-Attention Mechanism)**，可以并行处理序列中的所有元素，并计算它们之间的相关性权重，从而高效地捕获长距离依赖关系。

**1. Encoder（编码器）**
*   **职责：** **理解和编码** 输入序列的丰富语义表示。它为输入序列中的每个词（Token）生成一个考虑了**全局上下文**的向量表示。
*   **结构：**
    *   **自注意力层：** 让输入序列中的每个词都能与其他所有词进行交互，从而理解上下文。
    *   **前馈神经网络层：** 对每个位置的表示进行非线性变换。
    *   每一层都包含残差连接和层归一化。
*   **典型模型：** BERT、RoBERTa。它们适用于**理解类**任务，如文本分类、命名实体识别、情感分析。

**2. Decoder（解码器）**
*   **职责：** **生成** 输出序列。它根据编码器的输出和**已经生成的部分**，自回归地（一个一个词地）预测下一个词。
*   **结构：** 比编码器多一个“编码器-解码器注意力层”。
    *   **掩码自注意力层：** 为了防止在训练时“偷看”未来的答案，它使用掩码确保当前位置只能关注到它之前的位置。
    *   **编码器-解码器注意力层：** 这是连接编码器和解码器的桥梁。解码器通过这一层去“关注”编码器输出的哪些部分与当前生成步骤最相关。
    *   **前馈神经网络层。**
*   **典型模型：** GPT系列。它们适用于**生成类**任务，如文本生成、翻译、对话。

**3. Encoder-Decoder（编码器-解码器）**
*   **职责：** 处理**序列到序列 (Seq2Seq)** 的任务。编码器理解输入，解码器基于编码器的信息生成输出。
*   **典型模型：** T5、BART。它们适用于需要同时理解和生成的任务，如机器翻译、文本摘要、问答。

---

### 3. Function Call 是怎么训练的？

Function Calling（函数调用，或Tool Calling）是让大模型学会根据用户请求，决定是否需要调用外部工具/API，以及生成符合API要求的结构化参数。

**它的训练本质上是一种有监督微调 (SFT)：**

1.  **数据构造：**
    *   需要人工构造大量的**对话数据**，其中包含了用户请求、模型思考、以及是否要调用函数、调用哪个函数、参数是什么的**标注**。
    *   数据格式通常如下：
        ```json
        {
          "messages": [
            {"role": "user", "content": "今天北京的天气怎么样？"},
            {"role": "assistant", "content": null, "function_call": {"name": "get_weather", "arguments": "{\"location\": \"北京\"}"}},
            {"role": "function", "name": "get_weather", "content": "{\"weather\": \"晴\", \"temperature\": 25}"},
            {"role": "assistant", "content": "北京今天天气晴朗，气温25摄氏度。"}
          ]
        }
        ```
    *   关键是要有 `function_call` 和 `function` 角色的消息。

2.  **训练方法：**
    *   在已有的对话模型基础上，使用上述构造的数据进行**有监督微调 (SFT)**。
    *   训练时，模型学习在合适的时机（当用户问题需要外部工具解决时）生成特定的格式（`function_call` 的 JSON 结构）。
    *   这个过程教会了模型两件事：**a) 规划 (Planning):** 判断是否需要调用函数；**b) 格式化 (Formatting):** 如何将自然语言请求准确地映射到预定义函数的参数上。

3.  **推理过程：**
    *   用户提问。
    *   模型生成一个 `function_call` 请求（如果认为需要）。
    *   系统拦截这个请求，实际去调用对应的外部函数/API。
    *   将函数返回的结果（如JSON）以 `function` 角色的身份再次喂给模型。
    *   模型根据函数返回的结果，组织最终的自然语言回答给用户。

---

### 4. 微调的方案有哪些？自己做过没有？

**（这个问题一定要诚实回答，如果做过就详细讲，没做过就说明你了解的理论知识）**

微调的核心思想是：**用特定领域或任务的数据，在预训练模型的基础上进行继续训练，使其适应新任务。**

| 微调方案 | 简介 | 参数量 | 效率 | 效果 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **全参数微调 (Full Fine-Tuning)** | 更新模型的所有参数。 | 全部 | 低 | 最好 | 数据量大、计算资源充足、追求极致性能 |
| **参数高效微调 (PEFT)** | **只更新极少部分新增参数**，冻结原始模型绝大部分参数。 | 少量 | 高 | 接近全量 | **目前的主流方案**，资源有限、快速迭代 |
| **- LoRA (Low-Rank Adaptation)** | 在模型旁路增加低秩矩阵来模拟参数更新。 | 0.1%~1% | 很高 | 很好 | **最流行**，通用性强，效果稳定 |
| **- QLoRA** | LoRA的量化版本，将模型以4bit加载，进一步节省显存。 | 极少 | 极高 | 很好 | 在**消费级显卡**上微调大模型（如用1张24G卡微调70B模型） |
| **- Adapter** | 在Transformer层中插入小型神经网络模块。 | 少量 | 高 | 好 | 早期方案，可能会增加推理延迟 |
| **- P-Tuning v2** | 将可学习的连续提示向量插入到每一层输入中。 | 少量 | 高 | 好 | 适用于指令微调 |

**如何回答“是否做过”：**
*   **如果做过：** “是的，我使用过 [**LoRA/QLoRa**] 方案在 [**具体任务，如客服问答/领域知识库**] 上对 [**基座模型，如LLaMA-2-7B/ChatGLM3-6B**] 进行过微调。我们使用了 [**工具，如Unsloth/XTuner/LLaMA-Factory**] 框架，在 [**硬件，如单卡A100/3090**] 上完成了训练，取得了不错的效果，使模型在特定任务上的准确率提升了 [**量化指标**]。”
*   **如果没做过但了解：** “我目前还没有亲手进行微调的项目经验，但我深入理解其原理。我知道目前主流的方案是参数高效微调PEFT，尤其是LoRA和QLoRA，因为它们能极大地降低计算成本。我了解整个数据构造、训练和评估的流程，并已经搭建好了环境（如配置了CUDA、阅读了XTuner的文档），非常期待在接下来的项目中实践。”

---

### 5. 大模型分词器是什么？

分词器 (Tokenizer) 是大模型处理文本的**第一步和最后一步**，负责在**字符串**和模型能理解的**数字Token ID**之间进行转换。

*   **为什么需要？** 模型无法直接处理文本（“hello”），只能处理数字（[1, 2, 3]）。并且需要将词汇拆分成更小的单元。
*   **工作原理：**
    1.  **分词 (Tokenization):** 将输入文本拆分成一个个的Token（词元）。这些Token可能是单词（“hello”）、子词（“ing”、“est”）甚至是字符。
    2.  **编码 (Encoding):** 根据一个预设的**词表 (Vocabulary)**，将每个Token映射成一个唯一的整数ID。
    3.  **解码 (Decoding):** 模型输出一系列Token ID后，分词器根据同样的词表，将这些ID转换回文本字符串。

*   **主流分词算法：**
    *   **Byte-Pair Encoding (BPE)：** **GPT、LLaMA系列使用**。从字符开始，逐步将最常连续出现的字节对合并成新的Token。
    *   **WordPiece：** **BERT使用**。与BPE类似，但合并策略是基于概率（可能性最大），而不是频率。
    *   **SentencePiece：** **ChatGLM、T5使用**。它将输入视为原始字节流，可以处理任何语言而不需要预处理，并且将空格也作为一个字符处理，对中文更友好。

*   **重要特点：**
    *   词表大小通常在几万到十几万。
    *   不同模型的分词器一般**不能混用**。
    *   对于中文，一个汉字通常会被分成1-2个Token。

---

### 6. Embedding 是什么？你们用的那个模型？

**1. Embedding（嵌入）是什么？**
*   **定义：** 一种将**离散的符号**（如单词、句子、图片、用户ID）映射到**连续的高维向量空间**的技术。这个向量就是Embedding。
*   **核心思想：** **语义相近的符号，其在向量空间中的位置（即向量）也相近。** 例如，“国王”和“王后”的向量距离，与“男人”和“女人”的向量距离应该相似。
*   **在大模型中的作用：**
    *   **输入表示：** 模型的第一层通常就是一个Embedding层，它将输入的Token ID转换为稠密向量。
    *   **文本表示：** 可以将一段文本（句子、段落）通过模型转换成一個固定大小的向量，这个向量代表了文本的语义。这是构建**向量数据库**和实现**检索增强生成 (RAG)** 的基础。

**2. 用的哪个模型？**
*   **（这是一个考察你是否真的有实践经验的问题，需要结合你的项目/学习经历回答）**
*   **通用Embedding模型：**
    *   **OpenAI: `text-embedding-ada-002`**： 非常流行，效果很好，但需要调用API。
    *   **Sentence-BERT (SBERT)：** 一系列基于BERT的双塔编码模型，专门为生成句向量而微调过。
    *   **M3E (Moka Massive Mixed Embedding)：** **中文领域非常强大的开源模型**，在中文语义相似度计算、检索任务上表现优异，是很多中文RAG项目的首选。
    *   **BGE (BAAI General Embedding)：** 智源研究院推出的系列模型，有中英文版本（如`BGE-large-zh`），同样非常强大，在MTEB等榜单上排名靠前。
*   **如何回答：**
    *   “在我们的项目中，主要处理的是中文文本检索任务。经过调研，我们选择了**M3E-base**作为我们的Embedding模型。它在中文任务上的表现优于同等参数量的SBERT模型，并且是开源的，可以部署在本地，避免了数据泄露的风险。我们使用Hugging Face的 `sentence-transformers` 库来加载和使用它，效果符合我们的预期。”
    *   （如果没用过）：“我个人在学习RAG项目时，使用过 `text-embedding-ada-002` 和开源的 `all-MiniLM-L6-v2` 模型。我了解到在中文领域，**M3E**和**BGE**是当前效果最好的开源模型，如果有实际项目需求，我会优先选择它们进行本地部署。”

希望这份详细的解答能帮助你更好地准备下一次面试！
