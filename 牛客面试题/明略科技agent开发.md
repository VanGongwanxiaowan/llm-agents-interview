https://www.nowcoder.com/discuss/787075820568297472?sourceSSR=users


关键词检索的算法 有什么

rerank模型的架构

rerank为什么慢

长期记忆，短期记忆

长期记忆需要保存什么东西

长期记忆的压缩方案

知识库构建 PDF数据是怎么解析的。 -- 用ocr和 VL模型 有没有调研过一些开源的库之类的

OCR用的什么模型

PDF中的表格和图片是怎么处理的。 -转成markdowm

HTML是怎么解析的 有没有优化方案   BS4。 有调研过其他方案吗

为什么要先embbding再rerank 你先rerank不行吗

manus
了解过manus，你能给我讲讲manus的技术特点嘛

你怎么看出来manus是多智能体方案的

如何不用多智能体方案给1000个tools还能正常work

如果一个agent工具的某一个节点出现了问题 如何解决

大模型是怎么返回的做函数调用的

是谁去做fuctioncall

有了解过MCP嘛  MCP是什么

如何让大模型格式化输出一个消息  PE   ----  Pydanic

为什么Pydanic 要比 PE 工程要好

有遇到过Json输出格式不对的情况无法解析的情况嘛 这种情况是怎么处理的

你是在哪里遇到过这种问题 有考虑过我什么会发生这种情况嘛

langchain如何去做长期记忆和短期记忆

如何构建一个工具去做memory的记忆，让大模型能够在长期对话中学习到，用户的使用习惯

了解过cursor是怎么做memory的管理的吗

服务端怎么向客户端通信有了解过吗。 SSE websocket

算法：
二分查找

二分查找的变种

把中间截断以后接到前面 需要保证O(log n )时间复杂度

反问
完善的AI产品的样子

简历和知识点的不足。 找一个地方深挖 而不是泛泛学习

需要找一个好实习 让你去学习到这些反馈和各种奇怪的问题

作者：愚蠢的猫
链接：https://www.nowcoder.com/discuss/787075820568297472?sourceSSR=users
来源：牛客网

好的，这是一份针对您提供的面试题目的详细解答，我将按照问题的逻辑分组进行回答。

---

### 一、检索与重排序（Retrieval & Rerank）

**1. 关键词检索的算法有什么？**
关键词检索是信息检索的基础，核心算法包括：
*   **布尔模型（Boolean Model）：** 使用AND, OR, NOT等逻辑运算符进行精确匹配。简单快速，但无法排序，相关性判断粗糙。
*   **向量空间模型（Vector Space Model, VSM）：** 将文档和查询表示为向量（通常使用TF-IDF权重），通过计算余弦相似度等度量进行排序。考虑了词频，效果优于布尔模型。
*   **BM25（Best Matching 25）：** 是TF-IDF的改进和优化，被认为是关键词检索的“终极”算法。它考虑了：
    *   词频（TF）的饱和性：一个词出现多次，其重要性不会无限增加。
    *   文档长度归一化：惩罚过长的文档。
    *   逆文档频率（IDF）：降低常见词的重要性。
    *   BM25及其变种（如BM25+）在ES、Lucene等搜索引擎中广泛应用，是当前最主流的关键词检索算法。

**2. Rerank模型的架构**
Rerank模型通常是一个**交叉编码器（Cross-Encoder）** 架构。
*   **输入：** 将`[CLS]` + `Query` + `[SEP]` + `Document` + `[SEP]`拼接成一个长序列输入到Transformer模型中。
*   **处理：** Query和Document在模型的注意力层进行**充分、深度的交互**，从而更精确地理解两者的相关性。
*   **输出：** 取`[CLS]`标签对应的输出向量，接一个线性分类层，输出一个相关性分数（如0-1之间的值）。
*   **代表模型：** BERT、RoBERTa等预训练模型微调后的模型，如Sentence-BERT的交叉编码模式、专门用于重排的MonoBERT等。

**3. Rerank为什么慢？**
*   **计算复杂度高：** 交叉编码器需要将Query和**每一个**候选文档都拼接成一个序列，然后进行前向推理。假设有K个候选文档，就需要进行K次完整的模型计算。
*   **无法预处理：** 与双塔编码器（Bi-Encoder）可以预先计算好所有文档的向量嵌入不同，Rerank模型必须在拿到Query后，才能进行Query和文档的交互计算，无法利用缓存。
*   **长文本处理：** 如果文档很长，输入的序列会非常长，Transformer的自注意力机制计算复杂度是序列长度的平方（O(n²)），会急剧增加计算时间。

**4. 为什么要先Embedding再Rerank，不能先Rerank吗？**
*   **效率原因：** 绝对不能先Rerank。假设知识库有100万条数据，对一个Query进行Rerank需要计算100万次交叉编码，延迟高到无法接受。
*   **Pipeline设计：** 标准的检索-重排管道（Retrieval-Rerank Pipeline）是：
    1.  **召回（Retrieval）**：使用**快速**的方法（如关键词检索BM25或双塔模型向量检索）从海量数据中召回Top K（例如100或200个）最相关的候选文档。这一步追求高召回率（Recall）。
    2.  **重排（Reranking）**：使用**精准但慢速**的交叉编码器模型对这100个候选文档进行精细排序，选出Top N（例如3或5个）最相关的文档。这一步追求高精度（Precision）。
*   这个“快-慢”两级结构在保证效果的同时，兼顾了效率。

---

### 二、记忆（Memory）

**1. 长期记忆（Long-term Memory） vs 短期记忆（Short-term Memory）**
*   **短期记忆：** 通常指**对话上下文（Conversation Context）**。即当前对话窗口内（受模型上下文长度限制，如128K）的聊天历史。模型可以基于这些历史进行回复，但一旦对话超过窗口长度，最早的信息就会被“遗忘”。
*   **长期记忆：** 指存储在模型外部（如数据库、向量库）的、关于用户或世界的持久化信息。它突破了模型上下文长度的限制，需要时通过检索等方式被唤醒并放入短期记忆中使用。

**2. 长期记忆需要保存什么东西？**
长期记忆的内容取决于应用场景：
*   **用户个性化信息：** 用户的姓名、职业、喜好、习惯、家庭情况等。
*   **关键对话摘要：** 对过去重要对话的总结性描述（例如：“用户于10月1日决定购买一款游戏笔记本，预算8000元”）。
*   **私有知识：** 用户上传的文档、笔记中的关键知识片段。
*   **智能体状态：** 对于Agent，可能包括其目标、计划、已执行的动作及其结果等。

**3. 长期记忆的压缩方案**
*   **摘要（Summarization）：** 最核心的方案。定期或基于触发条件（如对话轮次、关键信息出现），让大模型对一段对话或信息进行总结，将冗长的文本压缩成简洁的要点存入长期记忆。
*   **提取关键实体/事实（Key Fact Extraction）：** 使用模型或规则从文本中提取出关键实体（人物、地点）和事实关系，结构化地存储。
*   **向量化嵌入（Vector Embedding）：** 将文本转换为向量存入向量数据库。这本身是一种语义压缩，检索时通过相似度查找。通常与摘要结合使用，即对摘要进行向量化。

**4. LangChain如何做长期和短期记忆？**
*   **短期记忆：** 使用`ConversationBufferWindowMemory`、`ConversationSummaryMemory`等Memory类来管理和维护聊天历史，并在生成下一轮回复时自动将历史记录拼接到Prompt中。
*   **长期记忆：** LangChain本身不提供“长期记忆”的完整实现，但提供了构建的组件：
    *   使用`VectorStoreRetrieverMemory`可以将记忆片段向量化后存储到向量数据库（如Chroma），实现基于检索的长期记忆读写。
    *   通常需要开发者自己设计摘要、存储和检索的策略。

**5. 如何构建一个工具去管理Memory，让模型学习用户习惯？**
这是一个系统设计问题，一个可能的方案：
1.  **记录：** 在每一轮对话后，不仅保存原始对话，还记录元数据（如时间、对话主题标签）。
2.  **摘要与提取：** 定期（例如对话结束或每10轮对话）使用LLM对近期对话进行摘要，并**专门提取与用户习惯、偏好相关的信息**（例如：“用户更喜欢用Markdown格式接收代码答案”）。
3.  **结构化存储：** 将这些习惯和偏好以结构化的形式（如JSON）存入数据库或向量库。每条记忆可包含：`用户ID`，`习惯描述`，`置信度`，`上次更新时间`。
4.  **检索与触发：** 在对话开始时，或检测到当前对话可能涉及用户习惯时（例如用户说“像之前那样”），从长期记忆中检索该用户的相关习惯。
5.  **注入上下文：** 将检索到的习惯信息作为系统提示（System Prompt）的一部分提供给LLM，例如：“当前用户有以下偏好：1. ... 2. ... 请你在回答时充分考虑这些偏好。”

**6. 了解过Cursor的Memory管理吗？**
Cursor Editor的Memory管理是其核心卖点之一，虽然其实现细节未完全开源，但根据其表现，推测其结合了：
*   **超长上下文利用：** 直接利用GPT-4 Turbo等模型的128K长上下文，将当前工作区的大量代码文件作为“短期记忆”纳入上下文。
*   **向量化代码库记忆：** 必然对整个项目代码库建立了向量索引（长期记忆）。当用户提出模糊需求时（如“修改我们之前处理用户登录的函数”），它能通过语义检索找到相关代码。
*   **对话摘要与元数据：** 很可能在后台对对话进行摘要，并将摘要、代码位置（如GitHub Copilot的`@code`引用）等信息与项目文件关联存储。

---

### 三、数据解析与处理

**1. 知识库构建：PDF数据解析**
*   **通用文本PDF：** 使用像`PyMuPDF（fitz）`、`pdfplumber`、`pypdf`这样的库，可以较好地提取文本和其位置信息。
*   **扫描件/图片型PDF：** 必须使用**OCR（光学字符识别）**。
    *   **流程：** `PDF -> 逐页转成图像（PyMuPDF/ pdf2image） -> OCR模型识别图像中的文字`。
    *   **开源OCR模型：** **PaddleOCR**（中文场景效果极佳，开源首选）、**Tesseract**（老牌经典，但中文精度不如PaddleOCR）、**MMOCR**（商汤开源，功能强大）。
    *   **商业API：** 百度OCR、阿里云OCR、Google Cloud Vision等。

**2. PDF中的表格和图片处理**
*   **表格：**
    *   **目标：** 不仅提取文字，更要**保留表格结构**（行列关系）。
    *   **工具：**
        *   **`pdfplumber`**：提取简单表格效果很好，可直接转换为Pandas DataFrame。
        *   **`Camelot`**、**`Tabula`**：专门用于表格提取。
        *   **深度学习模型：** 使用**Table Transformer**等模型进行表格检测和结构识别，是当前最先进的方法。
    *   **输出：** 最终目标通常是转换为**Markdown表格**或**HTML表格**，这两种格式都能很好地保留结构信息。
*   **图片：** 通常无法理解图片内容。解决方案是：
    1.  提取图片本身，存储为文件，并在文本中留下一个引用链接（如`![image](path/to/image.png)`）。
    2.  使用**多模态大模型（VL Model）** 如GPT-4V、LLaVA、Qwen-VL对图片进行描述，将描述文本作为上下文。这对于后续的语义检索至关重要。

**3. HTML解析与优化**
*   **常用库：**
    *   **BeautifulSoup (BS4)：** 解析和遍历HTML/XML文档的利器，API简单易用，是Python最流行的解析库。
    *   **lxml：** 一个高性能的库，解析速度**远快于**BeautifulSoup。BeautifulSoup底层可以选用lxml作为解析器（`BeautifulSoup(html, 'lxml')`）来提升速度。
    *   **parsel：** Scrapy框架使用的选择器库，支持CSS和XPath，性能很好。
*   **优化方案：**
    *   **使用更快的解析器：** 用`lxml`替代纯Python的解析器。
    *   **减少搜索空间：** 尽量使用CSS选择器或XPath直接定位目标元素，避免大面积遍历。
    *   **编译正则表达式：** 如果使用正则表达式，预先用`re.compile()`编译。
    *   **异步请求：** 如果是网络爬虫，使用`aiohttp`+`asyncio`进行异步并发请求，避免I/O阻塞。

---

### 四、智能体（Agent）与工具调用

**1. 了解过Manus？它的技术特点？**
Manus（马努斯）是一个AI原生应用开发平台，其技术特点包括：
*   **多智能体（Multi-Agent）架构：** 核心特点，将复杂任务分解，由多个 specialized 的AI智能体协作完成。
*   **“大脑-手脚”模式：** 一个中心智能体（大脑）负责理解用户意图、规划和任务分配，多个工具智能体（手脚）负责执行具体的子任务（如写代码、查数据库、调用API）。
*   **强大的工具调用（Tool Calling）能力：** 智能体可以无缝地调用大量外部工具和函数来扩展能力。
*   **可视化工作流编排：** 提供低代码界面，允许开发者通过拖拽的方式设计和调试多智能体协作的工作流。

**2. 如何看出Manus是多智能体方案？**
从其宣传和演示中可以看出：它强调“**分工**”、“**协作**”、“**工作流**”，而不是一个单一的AI模型处理所有事情。例如，一个“开发一个网站”的任务，可能会被拆解给“产品经理Agent”、“前端工程师Agent”、“后端工程师Agent”协作完成，这就是典型的多智能体范式。

**3. 如何不用多智能体方案给1000个Tools还能正常Work？**
让一个单一的Agent直接管理1000个Tool几乎肯定会导致性能下降和混乱。替代方案是：
*   **分层路由（Hierarchical Routing）/ 元工具（Meta-Tool）：**
    1.  设计一个**“工具管理器”Agent**或一套**分类系统**。首先将1000个工具按照功能领域（如“数据库操作”、“图像处理”、“API调用”）进行分类。
    2.  用户请求到来时，先由“工具管理器”判断请求属于哪个类别（例如“查一下用户的订单” -> “数据库操作”类别）。
    3.  然后，只将该类别下的**少量相关工具**（如5个数据库查询工具）暴露给**执行Agent**。
    4.  执行Agent再从这5个工具中选择最合适的一个进行调用。
*   **动态工具选择（Dynamic Tool Selection）：** 使用一个小的神经网络或检索模型，根据用户查询的语义嵌入，快速从1000个工具中检索出最相关的Top-K个工具，再交给LLM做最终决策。

**4. 如果一个Agent工具的某一个节点出现了问题如何解决？**
这是一个关于**容错（Fault Tolerance）** 的问题。
1.  **重试（Retry）：** 对于瞬态错误（如网络波动），最简单的策略是自动重试1-2次。
2.  **超时（Timeout）：** 为每个工具调用设置超时时间，防止无限期等待。
3.  **备用方案（Fallback）：** 设计备用的工具或流程。如果工具A调用失败，尝试使用功能相似的工具B。
4.  **优雅降级（Graceful Degradation）：** 如果工具完全不可用，向用户坦诚地报告问题（“查询XX数据的服务暂时不可用”），并提供不含该数据信息的回答，而不是完全崩溃。
5.  **工作流回滚与状态管理：** 对于复杂的多步骤工作流，需要记录每个步骤的状态和结果。当一个步骤失败时，能够回滚到上一步的状态，并尝试替代路径或通知用户。

**5. 大模型是怎么返回函数调用的？是谁去做Function Call？**
*   **如何返回：** 当大模型（如GPT-4）决定要调用一个函数时，它不会直接执行代码。它会在回复中返回一个**特殊的结构化JSON响应**。
    *   **OpenAI格式示例：**
        ```json
        {
          "name": "get_current_weather",
          "arguments": "{\"location\": \"Beijing\"}"
        }
        ```
*   **谁去做Function Call：**
    1.  **大模型（LLM）** 的角色是**决策者**：它根据用户请求，决定**是否要调用函数**、**调用哪个函数**、以及**传入什么参数**。
    2.  **应用程序/Agent框架（你的代码）** 的角色是**执行者**：
        *   接收LLM返回的结构化JSON。
        *   在你的代码中**找到对应的本地函数**（`get_current_weather`）。
        *   解析参数（`{"location": "Beijing"}`）。
        *   **安全地执行**这个本地函数（或发起API调用）。
        *   将函数执行的结果**再次返回给LLM**，让LLM基于结果生成最终面向用户的回复。

**6. 有了解过MCP吗？MCP是什么？**
*   **MCP（Model Context Protocol）：** 是由Anthropic提出的一种**开放协议**，用于标准化LLM与外部工具、数据源和其他服务（称为“资源”）的交互方式。
*   **核心思想：** **解耦**LLM和工具。开发者无需为每个LLM供应商（OpenAI, Anthropic, Cohere...）重复编写工具集成代码。只需为你的工具或数据源编写一个MCP服务器，任何支持MCP协议的LLM客户端（如Claude Console、第三方应用）都可以立即使用这些工具。
*   **类比：** 类似于数据库的**ODBC/JDBC**协议，为LLM世界提供了统一的“工具驱动”标准。

**7. 如何让大模型格式化输出？Pydantic vs PE**
*   **PE（Prompt Engineering）：** 通过在提示词中详细描述输出格式（如“请用JSON输出，包含name和age字段...”），依赖模型的理解能力。**缺点：** 不稳定，容易格式错误或遗漏。
*   **Pydantic：** 是一个Python数据验证库。在LLM上下文中，它与`instructor`等库结合使用，实现**结构化输出（Structured Outputs）**。
    *   你定义一个Pydantic模型（`class User(BaseModel): name: str; age: int`）。
    *   框架会将这个模型的结构信息通过函数调用（或其它方式）传递给LLM，并**要求LLM严格遵循此结构**进行输出。
    *   输出后，再用Pydantic进行**验证和解析**。

**8. 为什么Pydantic比PE工程性好？**
*   **可靠性：** Pydantic通过框架强制约束输出格式，几乎完全消除了格式错误，而PE依赖模型的“自觉性”，不可靠。
*   **可维护性：** Pydantic模型是清晰的代码，结构一目了然。修改输出格式只需修改模型定义。而混在Prompt里的格式描述难以维护和版本控制。
*   **自动化验证：** Pydantic自动完成数据验证、类型转换（如字符串`"21"`转整数`21`），省去了手动编写验证代码的麻烦。
*   **开发者体验：** 享受IDE的自动补全、类型提示等好处。

**9. 有遇到过JSON输出格式不对的情况吗？如何处理的？**
**肯定遇到过。** 这是纯PE方式的常见问题。
*   **处理方式：**
    1.  **重试（Retry）：** 最简单的策略。捕获`json.decoder.JSONDecodeError`异常后，将错误信息和失败的原文本反馈给LLM，要求它纠正并重新生成。通常会成功。
    2.  **输出引导（Output Guiding）：** 在Prompt中提供更精确的示例（Few-shot），甚至要求模型“在生成JSON前先思考”。
    3.  **使用结构化输出库（根本解决方案）：** 迁移到使用`instructor`（OpenAI）或Anthropic的`tools` / `tool_choice`参数，利用LLM原生的函数调用功能来生成标准JSON，从而彻底避免此问题。

**10. 发生这种情况的原因？**
*   **LLM的本质：** LLM是生成文本的，不是生成代码的API。它生成的是“看起来像JSON”的文本，而不是精确的、无错误的JSON字符串。
*   **标点符号错误：** 缺少逗号、引号，尾随逗号。
*   **编码问题：** 包含无法解析的控制字符或Unicode问题。
*   **幻觉：** 有时模型会“解释”JSON而不是输出它，或者在JSON外加了其他标记。

---

### 五、通信技术

**服务端怎么向客户端通信？SSE vs WebSocket**
*   **SSE (Server-Sent Events)：**
    *   **协议：** 基于HTTP，是**单向通信**（只能服务器向客户端推送）。
    *   **特点：** 简单轻量，天然支持断线重连。非常适合**单向数据推送**场景，如新闻推送、实时状态更新、**LLM流式文本输出**（Token-by-Token）。
    *   **代码（客户端）：** 使用`EventSource` API，非常简单。
*   **WebSocket:**
    *   **协议：** 独立的基于TCP的协议，提供**全双工通信**（双方可以同时发送和接收）。
    *   **特点：** 功能更强大，开销也稍大。适合需要**高频、双向、低延迟交互**的场景，如聊天应用、实时协作编辑器、多人在线游戏。
*   **选择：** 对于LLM应用，如果只是需要将模型生成的Token流式地推送给前端，**SSE是更简单、更合适的选择**。如果应用还需要客户端高频地向服务器发送其他控制指令，则考虑WebSocket。

---

### 六、算法题

**1. 二分查找（Binary Search）**
标准算法，在**有序数组**中查找目标值。
```python
def binary_search(nums, target):
    left, right = 0, len(nums) - 1
    while left <= right:
        mid = left + (right - left) // 2
        if nums[mid] == target:
            return mid
        elif nums[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
```

**2. 二分查找的变种**
常见变种：
*   **查找第一个等于目标值的元素**
*   **查找最后一个等于目标值的元素**
*   **查找第一个大于等于目标值的元素**
*   **查找最后一个小于等于目标值的元素**
这些变种的核心在于**处理相等情况时，不立即返回，而是继续收缩边界**，并注意循环终止条件。

**3. 题目：将中间截断以后接到前面，需要保证O(log n)时间复杂度**
这道题通常指的是**在旋转排序数组（Rotated Sorted Array）中搜索**。

*   **示例：** 原数组 `[0,1,2,4,5,6,7]` 在索引3处旋转后变为 `[4,5,6,7,0,1,2]`。
*   **思路：** 虽然数组不是完全有序，但**一半总是有序的**。可以通过比较`nums[mid]`和`nums[right]`来判断哪一半是有序的，然后判断目标值是否在这个有序区间内，从而决定搜索方向。
*   **代码：**
```python
def search_in_rotated_array(nums, target):
    left, right = 0, len(nums) - 1
    while left <= right:
        mid = left + (right - left) // 2
        if nums[mid] == target:
            return mid
            
        # 判断哪部分是有序的
        if nums[mid] <= nums[right]:  # 右半部分 [mid..right] 有序
            if nums[mid] < target <= nums[right]:
                left = mid + 1
            else:
                right = mid - 1
        else:  # 左半部分 [left..mid] 有序
            if nums[left] <= target < nums[mid]:
                right = mid - 1
            else:
                left = mid + 1
    return -1
```
*   **时间复杂度：** 每次循环将搜索范围减半，为 O(log n)。

---

希望这份详细的解答能对您有所帮助！
