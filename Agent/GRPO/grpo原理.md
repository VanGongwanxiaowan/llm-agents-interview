https://medium.com/%40la_boukouffallah/how-to-fine-tune-llms-with-deepseeks-grpo-4aec203b83d8

# 网页总结：如何用DeepSeek的GRPO微调大语言模型（LLMs）
## 一、引言
大语言模型（LLMs）虽功能强大，但在生成准确、一致及具备推理能力的响应方面仍有不足。DeepSeek提出的**分组相对策略优化（GRPO）** 作为一种强化学习（RL）技术，为LLM微调提供了新方案，可提升模型响应质量，本文对其进行了详细解析。

## 二、LLM微调的必要性
LLMs（如GPT-4、Llama、DeepSeek）虽经大规模数据集训练，但存在以下问题，需通过微调改进：
- 生成错误或误导性信息
- 回答不一致
- 在推理类任务中表现不佳
常见微调方法如**基于人类反馈的强化学习（RLHF）** 存在局限性，GRPO正是为解决这些问题而生。

## 三、GRPO与传统方法的差异
传统强化学习方法（如**近端策略优化（PPO）** ）依赖独立的**评估模型（Critic Model）** 评估响应，存在计算资源消耗大、评估模型反馈未必可靠的问题。而GRPO无需评估模型，采用**分组比较法**，具体流程为：
1. 针对同一查询生成多个响应
2. 对比这些响应以筛选更优答案
3. 调整LLM，使其倾向生成优质响应、规避劣质响应
相比传统方法，GRPO具有**更高效率、更强稳定性、更优推理能力提升效果**的优势。

## 四、GRPO工作步骤详解
### 步骤1：生成多个响应
LLM针对同一问题生成**多个不同响应**，例如针对“法国首都是什么？”，模型可能生成“巴黎”“伦敦”“巴黎，欧洲的一座城市”“法国是欧洲的一个国家”等答案，为后续对比排序做准备。

### 步骤2：分配奖励
依据预设的**奖励函数**，从以下维度对每个响应打分：
- 正确性：响应是否符合事实
- 相关性：是否直接回答问题
- 一致性：是否与之前的响应保持一致
- 流畅性：语言是否清晰自然
以步骤1的例子为例，奖励分配可能为：“巴黎”（最优，奖励最高）＞“巴黎，欧洲的一座城市”（正确但不够简洁，奖励次之）＞“法国是欧洲的一个国家”（相关但未直接回答，奖励较低）＞“伦敦”（错误，奖励最低）。

### 步骤3：对比响应（优势计算）
GRPO在**组内对比**响应，将每个响应的奖励与组内平均奖励比较：若响应优于平均值，模型未来会倾向生成类似响应；若劣于平均值，模型则学习避免此类响应。

### 步骤4：更新模型（策略优化）
根据组内对比结果调整LLM的响应生成策略，同时通过**裁剪（Clipping）技术**避免模型变化过于剧烈，确保：
- 强化优质响应的生成
- 抑制劣质响应的产生

### 步骤5：保持模型稳定性（KL正则化）
为防止模型因过度调整而变得不可预测，GRPO采用**KL散度正则化（KL Divergence Regularization）** ，限制模型偏离原始行为的程度，确保模型在提升性能的同时，保留通用知识，不陷入过度专业化。

## 五、GRPO的核心优势
1. 无需评估模型：减少计算成本，提升训练效率
2. 基于分组的学习：助力模型提升推理能力，使训练更稳定
3. 鼓励自我反思：让模型通过对比自身响应实现持续改进
4. 微调效果更优：在复杂推理任务中表现更出色

## 六、GRPO实施参考
若需详细的GRPO实施步骤，可参考Hugging Face的教程（原文提供链接入口）。

## 七、结语
DeepSeek的GRPO通过**分组学习**，在无需独立评估模型的前提下，降低了计算成本，提升了LLM的推理能力，是一种高效的LLM微调方法，适合需优化LLM性能的相关实践场景。

## 八、其他相关内容
1. 作者信息：本文作者为BOUKOUFFALLAH ABDALLAH，是高等计算机科学学院四年级学生，对机器学习相关领域感兴趣，其账号还有“如何用Neo4j和LangChain构建知识图谱”“理解DeepSeek-R1中的强化学习”等文章。
2. 读者互动：有读者（Tim Wu）提问是否有参考代码，并对“步骤4：更新模型”提出疑问，认为LLM微调更多涉及注意力层更新。
3. Medium平台推荐：平台还推荐了“掌握LLM微调：GRPO、PPO和DPO对比”“微调SLM或LLM——以非实用主题为例的实践”等与LLM微调相关的文章。
