https://medium.com/%40g.anirudh15/fine-tuning-llms-a-look-at-group-relative-policy-optimization-grpo-8240cac48ebc

# 微调大型语言模型：群体相对策略优化（GRPO）网页总结
## 一、引言
大型语言模型（LLMs）虽在理解和生成类人文本上能力突出，但要生成有用、无害且符合人类目标的回应，需复杂训练技术。强化学习（RL）是实现这一目标的有效方法，而群体相对策略优化（GRPO）作为RL领域的方案，为LLM微调提供了新颖且灵活的思路。

## 二、强化学习（RL）基础与在LLM优化中的作用
### （一）RL核心原理
RL是智能体通过试错学习的范式，过程呈循环状：
1. 智能体观察环境，收集当前状态与策略信息；
2. 依据当前习得策略采取行动；
3. 环境根据行动以奖励或惩罚形式反馈；
4. 智能体结合反馈更新策略；
5. 迭代循环助力智能体持续提升决策能力。

### （二）RL在LLM优化中的价值
1. **传统训练方法局限**：传统预训练侧重下一词预测，有监督训练虽能生成结构化输出，但在培养模型有用性、无害性及与人类价值观一致性等细微特质上效果欠佳。
2. **RL的作用机制**：RL为微调LLM以实现期望特性提供机制，其中基于人类反馈的强化学习（RLHF）是重要技术。在RLHF中，人类对同一提示下LLM生成的不同响应比较并表明偏好，据此训练奖励模型，再以该模型为环境微调LLM。
3. **RL融入LLM训练的优势**：增强对LLM输出的控制，使其契合人类偏好；与常具主观性和复杂性的人类价值观保持一致；减少生成有害语言、虚假信息及表现出偏见等不良行为。

## 三、群体相对策略优化（GRPO）概述
### （一）GRPO与其他LLM微调技术的对比
|技术|核心特点|
| ---- | ---- |
|近端策略优化（PPO）|采用策略梯度方法，基于奖励模型更新策略|
|直接偏好优化（DPO）|直接应用偏好数据，绕过奖励模型，类似分类任务|
|群体相对策略优化（GRPO）|用奖励信号或模型比较相似样本组，无需复杂奖励模型，梯度更稳定，利于训练收敛|

### （二）GRPO的实践案例——DeepSeekR1
1. **“顿悟”时刻**：DeepSeekR1在RL训练中展现出“顿悟”行为，模型会先尝试解决问题，识别初始尝试中的错误或不一致，基于此自我修正方法，并解释更新后方法更优的原因，且该自我修正能力是RL过程自然产生，非明确编程。
2. **与DeepSeekR1-Zero的差异**：DeepSeekR1-Zero采用纯RL方法，无微调，推理能力强但可读性差；DeepSeekR1采用多阶段监督微调（SFT）+RL方法，推理能力增强，语言一致性和可读性更优。
3. **DeepSeekR1训练阶段**
    - **冷启动阶段（质量基础）**：用R1-Zero创建的小型高质量样本数据集微调v3-base模型，解决冷启动数据问题，奠定可读性和响应质量基础。
    - **推理RL阶段（能力构建）**：聚焦在数学、编程、科学、逻辑等领域构建能力，用基于规则的RL过程验证输出正确性，针对“语言混合”问题，引入奖励函数维持语言一致性。
    - **拒绝采样阶段（质量控制）**：让V3模型充当质量评判者，筛选输出数据用于监督微调（SFT）。
    - **多样化RL阶段（广泛对齐）**：使模型与人类偏好广泛对齐，确定性任务用基于规则的奖励系统，复杂任务借助LLM处理。

## 四、GRPO的工作原理与算法细节
### （一）GRPO工作步骤
1. **群体形成**：模型多次尝试解决问题或响应提示，整合不同尝试（含非最优、错误尝试）形成群体，为模型提供丰富学习样本。
2. **偏好学习**：通过组内分数归一化融入人类偏好学习，可用简单评分器评估响应，利用公式“优势=(奖励−均值(群体奖励))/标准差(群体奖励)”对组内奖励归一化，而非用绝对分数。
3. **优化**：依据从一组解决方案中获取的见解优化模型，遵循两大原则：一是鼓励模型生成与组内成功方案相似的解，远离不理想方案；二是融入KL散度惩罚项，避免模型变化过剧或过快，保留已有知识，逐步摆脱不良行为。

### （二）GRPO算法核心
1. **算法输入**：初始策略、奖励函数、训练提示、组大小（通常每个提示生成4-16个输出）。
2. **迭代过程**：每次训练迭代为每个提示生成多个输出，计算奖励并在组内归一化，通过最大化截断比率更新策略，同时考虑与参考策略的KL散度。
3. **高级GRPO算法核心步骤**
    - **群体采样**：对每个输入查询q，从旧策略πθold采样生成G个候选输出{ o1，…，oG }，为相对质量评估提供多样化输出，公式为{ oi }i=1G∼πθold(⋅|q)。
    - **优势计算**：分配原始奖励（如正确输出ri=1，否则ri=0）；计算组统计量（平均奖励rˉ=(1/G)∑j=1G rj，标准差σr=(1/G)∑j=1G (rj–rˉ)²）；将奖励标准化为优势Ai=(ri–rˉ)/σr，转化为相对“z分数”，聚焦显著偏离组平均的输出更新。
    - **策略更新**：目标函数JGRPO(θ)=∑i=1G min(ri(θ)⋅Ai, clip(ri(θ),1–ε,1+ε)⋅Ai)–β⋅KL(πθ∥πref)。其中ri(θ)=πθ(oi∣q)/πold(oi∣q)为重要性比率；clip(r,1–ε,1+ε)=max(1–ε,min(r,1+ε))防止策略大幅变动；β为KL惩罚权重（如0.04）；KL(P∥Q)=∑x P(x)⋅ln[P(x)/Q(x)]为KL散度。

### （三）GRPO目标函数关键组件
1. **重要性比率（ri(θ)）**：反映新策略选择特定输出oi的可能性与旧策略的差异，ri>1时新策略下该输出可能性更高。
2. **截断项（clip(r,1–ε,1+ε)）**：限制重要性比率对更新的影响，防止更新过大，稳定训练。
3. **标准化优势（Ai）**：体现特定输出oi相对组平均的优势，经组内变异性标准化。
4. **KL散度惩罚项（DKL(πθ∥πref)）**：衡量新策略与可信参考策略的差异，起正则化作用，保障安全并与已有学习行为保持一致。
5. **完整目标函数（JGRPO(θ)）**：整合各组件，奖励优于组平均的答案，限制策略突变，维持模型整体稳定。

## 五、GRPO的局限性
1. **生成成本高**：每个提示生成4-16个补全，比生成1-2个补全的方法计算需求更高。
2. **批量大小受限**：需成组处理补全结果，限制有效批量大小，增加复杂性，可能减慢训练速度。
3. **依赖奖励函数设计**：训练质量受奖励函数设计影响大，设计不佳会导致模型优化目标偏离或出现不良行为。
4. **群体规模难权衡**：选择最优群体规模需平衡解决方案多样性与计算成本，样本过少多样性不足，过多则增加训练时间和资源消耗。
5. **KL散度惩罚项难调优**：KL散度惩罚项（β）需仔细调优，过高模型学习效果差，过低模型易偏离初始稳定能力。

## 六、其他信息
1. 本文是作者对GRPO主题理解的精炼整合，欢迎读者反馈错误以修正。
2. 参考文献：含Hugging Face相关课程链接、YouTube视频链接及博客教程链接。
3. 作者信息：Anirudh Gokulaprasad，GenAI工程师，拥有约4年大规模构建全自动智能体GenAI平台的经验。
