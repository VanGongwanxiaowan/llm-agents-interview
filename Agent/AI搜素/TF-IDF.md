# 机器学习：生动理解TF-IDF算法


## 一、什么是TF-IDF？
TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，算法简单高效，常被工业界用于最开始的文本数据清洗。

TF-IDF包含两层核心含义：
1. **词频（Term Frequency，简称TF）**：统计词在单篇文档中的出现频率；
2. **逆文档频率（Inverse Document Frequency，简称IDF）**：衡量词在整个语料库中的常见程度，与常见程度成反比。


### 核心逻辑示例
假设存在长文《量化系统架构设计》：
- 用TF可统计出“的”“是”“了”等**停用词**（高频但无实际意义的词），并对其过滤；
- 过滤后若“量化”“系统”“架构”出现次数相同，仅靠TF无法区分重要性——此时需IDF：“系统”在其他文章中更常见，IDF会给其更小权重，最终“量化”“架构”的重要性会排在“系统”之前。

**关键结论**：将TF与IDF相乘，得到词的TF-IDF值。某词的TF-IDF值越大，通常在该文章中的重要性越高；按TF-IDF值降序排序，排在前列的词即为文章关键词。


## 二、TF-IDF算法步骤
### 第一步：计算词频（TF）
考虑到文章长短差异，需对“词频”进行标准化，避免因文档长度导致高频词统计偏差，便于不同文档间的比较。

<img width="621" height="79" alt="image" src="https://github.com/user-attachments/assets/901d05c1-67e9-4629-bdc2-581f443c1845" />

<img width="628" height="164" alt="image" src="https://github.com/user-attachments/assets/3666c937-873a-44d3-b7a0-6d0c37fda188" />

### 第二步：计算逆文档频率（IDF）
需依赖**语料库（corpus）** 模拟语言使用环境，计算公式隐含两个关键处理：
- 分母加1：避免“所有文档都不包含该词”导致的分母为0问题；
- 取对数（log）：对计算结果进行平滑，降低极端值影响。  
核心规律：词越常见，分母越大，IDF值越小（越接近0）。

<img width="653" height="148" alt="image" src="https://github.com/user-attachments/assets/76f7f25c-5237-4c8e-90dd-a78391341147" />


### 第三步：计算TF-IDF
TF-IDF值 = TF值 × IDF值，其与“词在文档中的出现次数成正比”“与词在整个语料库中的出现次数成反比”。  
自动提取关键词的逻辑：计算文档中每个词的TF-IDF值，降序排列后取前N个词。


## 三、TF-IDF的优缺点
### 优点
- 算法逻辑简单，易于理解；
- 计算效率高，适合工业界快速处理文本数据。

### 缺点
1. 用词频衡量词的重要性不够全面：部分重要词可能出现次数较少，导致TF-IDF值偏低；
2. 无法体现位置信息：忽略词在文档中的上下文关系（如标题、段落首句的词通常更重要）；
3. 无法处理语序问题：如“武松打虎”与“虎打武松”的TF-IDF统计结果一致，无法区分语义差异。  
（若需体现上下文结构或语义关系，可结合word2vec等算法补充）


```
from math import log
import os

def TF_IDF(path, dic):
    # 用于存储最终每个关键词的TF-IDF值
    tf_idf = {}
    # 获取path目录下的子文件夹列表
    fold_list = os.listdir(path)
    # 用于统计语料库中的文件总数
    count = 0
    # 用于存储每个关键词的逆文档频率相关计数
    idf = {}
    # 初始化idf和tf_idf字典，为每个关键词设置初始值
    for key in dic.keys():
        idf[key] = 1
        tf_idf[key] = 0
    # 遍历每个子文件夹
    for flod in fold_list:
        # 获取当前子文件夹下的文件列表
        file_list = os.listdir(path + '/' + flod)
        # 累加文件数量到count
        count += len(file_list)
        # 遍历当前子文件夹下的每个文件
        for file in file_list:
            # 打开文件并读取内容
            with open(path + '/' + flod + '/' + file) as f:
                text = f.read()
                # 遍历每个关键词，若关键词在文件内容中，更新idf中对应关键词的计数
                for key in dic.keys():
                    if key in text:
                        idf[key] += 1
    # 根据公式计算每个关键词的idf值
    for key, value in idf.items():
        idf[key] = log(count / value + 1)
    # 计算每个关键词的TF-IDF值，即词典中该词的词频（dic[key]）乘以对应的idf值
    for key, value in tf_idf.items():
        tf_idf[key] = dic[key] * idf[key]
    # 返回存储每个关键词TF-IDF值的字典
    return tf_idf
```

好的，我们来详细、深入地讲解一下**TF-IDF算法**。这是一个在信息检索和文本挖掘中非常基础且重要的技术，用于评估一个词语对于一个文件集或语料库中的其中一份文件的重要程度。

### 一、 TF-IDF 是什么？

**TF-IDF** 的全称是 **Term Frequency-Inverse Document Frequency**，即**词频-逆文档频率**。

它的核心思想非常直观：
1.  **一个词语在一篇文章中出现的次数越多（TF高），它对于这篇文章就越重要。**
2.  **但同时，如果一个词语在整个文档集合中出现的频率也很高（DF高），说明它是一个常见词（如“的”、“是”、“在”），其重要性就应该被削弱。**

TF-IDF就是将这两点结合起来，形成一个综合的权重指标。**TF-IDF值越高，意味着该词语对当前文档越独特、越重要。**

---

### 二、 TF-IDF 的组成部分

我们来拆解这个公式，它由两部分相乘组成：

**TF-IDF(t, d, D) = TF(t, d) × IDF(t, D)**

其中：
- `t`： 某一个**词语**（Term）
- `d`： 某一个**文档**（Document）
- `D`： 整个**文档集合**（Corpus），即所有文档的合集。

#### 1. 词频 - TF (Term Frequency)

**作用**：衡量一个词语在一篇文档中出现的频繁程度。
**直觉**：出现的次数越多，就越重要。

**计算方法（有多种变体，最常见的是）：**
- **基础词频：** `TF(t, d) = (词语 t 在文档 d 中出现的次数)`
- **标准化词频（更常用）：** `TF(t, d) = (词语 t 在文档 d 中出现的次数) / (文档 d 中所有词语的总数)`
    - 这样做是为了消除文档长度不同的影响。否则，长文档中的词频天然会更高。

**例子**：
假设文档 d 的内容是：“`苹果 公司 发布 了 新 一代 苹果 手机`”
- 文档总词数：8
- “苹果”的出现次数：2
- 所以，`TF(“苹果”, d) = 2 / 8 = 0.25`

#### 2. 逆文档频率 - IDF (Inverse Document Frequency)

**作用**：衡量一个词语在整个文档集合中的普遍重要性。如果包含某个词的文档越少，IDF越大，说明该词具有很好的类别区分能力。
**直觉**：如果一个词在很多文档中都出现，那它可能是一个常见词，区分度不高，应该降低其权重。

**计算方法（标准公式）：**
`IDF(t, D) = log( 文档集合 D 中的文档总数 / (包含词语 t 的文档数 + 1) )`

**说明**：
- **分子**：总文档数 `N`。
- **分母**：包含词语 t 的文档数。加 `1` 是为了防止分母为0（即当某个词不在任何文档中出现时），这被称为“平滑”处理。
- **取对数(log)**：这是因为如果某个词在所有文档中都出现（`DF = N`），那么 `N/DF = 1`，`log(1) = 0`，这个词的权重就变为0，这符合直觉。同时，取对数可以抑制IDF值的尺度，使其不会过大。

**例子**：
假设文档集合 D 共有 1000 篇文档 (N=1000)。
- 单词 “的” 在 900 篇文档中都出现了。
    - `IDF(“的”, D) = log(1000 / (900 + 1)) ≈ log(1.11) ≈ 0.045` (值非常小)
- 单词 “区块链” 只在 10 篇文档中出现过。
    - `IDF(“区块链”, D) = log(1000 / (10 + 1)) ≈ log(90.9) ≈ 4.51` (值非常大)

---

### 三、 TF-IDF 的计算过程与完整示例

现在我们通过一个完整的例子来计算一个词的TF-IDF值。

**文档集合 D**：
- 文档1 (d1)：`“这是 一个 苹果”`
- 文档2 (d2)：`“那个 苹果 是 我的”`
- 文档3 (d3)：`“这是 他的 手机”`

**任务**：计算词语 `“苹果”` 在 **文档1 (d1)** 中的 TF-IDF 值。

**步骤 1：计算 TF(“苹果”, d1)**
- 文档1的总词数：4
- “苹果”在文档1中出现的次数：1
- `TF(“苹果”, d1) = 1 / 4 = 0.25`

**步骤 2：计算 IDF(“苹果”, D)**
- 总文档数 N = 3
- 包含“苹果”的文档数：文档1和文档2，所以 `DF(“苹果”) = 2`
- `IDF(“苹果”, D) = log(3 / (2 + 1)) = log(3/3) = log(1) = 0`

**步骤 3：计算 TF-IDF(“苹果”, d1, D)**
- `TF-IDF = 0.25 × 0 = 0`

**分析和解释**：
这个结果很有意思。虽然“苹果”在文档1中出现了，但它的TF-IDF值却是0。这是因为“苹果”在总共3篇文档中出现了2次，太常见了，导致其IDF值为0。在这个小的文档集合里，“苹果”不是一个具有区分度的词。

**我们再计算一个词：“手机” 在 文档3 (d3) 中的 TF-IDF。**

- **TF(“手机”, d3)**： 文档3总词数4，“手机”出现1次。 `TF = 1/4 = 0.25`
- **IDF(“手机”, D)**： 总文档数3，包含“手机”的文档只有文档3，所以 `DF = 1`。 `IDF = log(3 / (1+1)) = log(1.5) ≈ 0.405`
- **TF-IDF(“手机”, d3, D)**： `0.25 × 0.405 ≈ 0.101`

可以看到，“手机”的TF-IDF值高于“苹果”，因为它只在一篇文档中出现，更具独特性。

---

### 四、 TF-IDF 的应用场景

TF-IDF的价值在于它能够将文本数据**数值化**，从而可以被计算机处理和计算。

1.  **搜索引擎**：早期的搜索引擎用TF-IDF来计算查询关键词与网页内容的相关性。对于搜索词“苹果手机”，会计算每个网页中“苹果”和“手机”的TF-IDF值，然后综合排序，将分值高的网页排在前面。
2.  **文档相似度计算**：
    - 将两篇文档中的词语分别计算TF-IDF，形成两个向量（Vector）。
    - 然后计算这两个向量的**余弦相似度**（Cosine Similarity）。值越接近1，说明文档越相似。
    - 广泛应用于推荐系统（如推荐相似新闻）、论文查重等。
3.  **文本摘要/关键词提取**：
    - 计算一篇文章中所有词的TF-IDF值。
    - 排序后，选取分值最高的几个词作为这篇文章的**关键词**。
4.  **作为机器学习的特征**：
    - 在文本分类（如垃圾邮件识别、情感分析）任务中，可以将每个词的TF-IDF值作为特征，输入到分类模型（如SVM、朴素贝叶斯）中进行训练。

---

### 五、 TF-IDF 的优缺点

#### 优点：
- **简单有效**：原理简单，计算不复杂，但在很多场景下效果不错。
- **易于理解**：背后的直觉非常清晰，符合人类对“重要词汇”的认知。
- **无监督**：不需要人工标注数据，完全基于统计。

#### 缺点：
- **无法捕捉语义信息**：TF-IDF纯粹基于词频统计，无法理解词语的语义。“汽车”和“轿车”虽然意思相近，但会被当作完全不同的词处理。这是其最大的局限。
- **无法捕捉词的位置信息**：标题、段首、段尾出现的关键词通常更重要，但TF-IDF无法利用这些信息。
- **假设词之间相互独立**：忽略了文本中词序和上下文关系（即“词袋模型”的缺陷）。
- **对罕见词不友好**：如果一个词在整个语料库中只出现一两次，它的IDF会非常高，可能会被赋予不合常理的高权重，这些词可能是拼写错误或无意义的噪音。

---

### 六、 与现代方法（如Word2Vec, BERT）的对比

- **TF-IDF**：代表的是**基于统计**的浅层文本表示模型。
- **Word2Vec, GloVe**：代表的是**基于浅层神经网络**的词嵌入模型，它能将单词映射到一个稠密向量空间中，语义相似的词在空间中的位置也接近。解决了TF-IDF“无法捕捉语义”的问题。
- **BERT, GPT**：代表的是**基于深度上下文感知**的预训练模型。它们能根据上下文动态地调整一个词的向量表示（例如，“苹果”在“吃苹果”和“苹果公司”中的向量是不同的）。这是目前最先进的技术，几乎在所有NLP任务上都超越了传统方法。

尽管如此，TF-IDF因其简单、高效、可解释性强，在处理小规模数据、作为基线模型或在计算资源受限的场景下，依然是一个非常实用的工具。
