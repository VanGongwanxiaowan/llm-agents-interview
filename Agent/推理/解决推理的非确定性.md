https://www.youtube.com/watch?v=eYq6Zc1M6pU

<img width="755" height="516" alt="image" src="https://github.com/user-attachments/assets/90f57825-6697-4d24-a313-dbe944149e72" />

<img width="932" height="232" alt="image" src="https://github.com/user-attachments/assets/f5f82a12-d9c0-4029-82a2-81e185330d1a" />

输入平方的均值

<img width="891" height="508" alt="image" src="https://github.com/user-attachments/assets/a83f855d-c1f5-4a72-8878-2b2067d5f51d" />

数据并行的策略

<img width="889" height="519" alt="image" src="https://github.com/user-attachments/assets/4fa71d7a-2682-4489-a9f0-ae4e1eb65600" />


<img width="889" height="519" alt="image" src="https://github.com/user-attachments/assets/d99f2462-870e-4065-882a-9b7b8d62a7d2" />

<img width="894" height="322" alt="image" src="https://github.com/user-attachments/assets/828cbf5f-aad1-4cb1-bc50-0b85315a2b49" />



<img width="708" height="507" alt="image" src="https://github.com/user-attachments/assets/e52ca94f-8349-4a1c-b26d-de0eb6851216" />

<img width="638" height="504" alt="image" src="https://github.com/user-attachments/assets/e683c3ed-5d6a-4174-83bd-ed420f82d48d" />

<img width="910" height="248" alt="image" src="https://github.com/user-attachments/assets/f3d7118c-37df-4051-b0e6-f04dc9840b06" />

固定大小分割

好的，这是对视频内容的一份极为详细的总结，涵盖了从问题现象、传统误解、核心真相、技术原理、解决方案到行业意义的全部重点。

---

### **《为什么大模型输出会“失控”？Thinking Machines Lab 揭晓核心真相》详细总结**

#### **一、 问题现象：令人困惑的“非确定性”**

*   **核心问题**：在进行大模型推理时，即使输入完全相同的提示词（Prompt），并且固定了随机种子（Random Seed），模型的输出仍然会不一致。
*   **普遍性**：这是一个“老大难”问题，困扰着所有从事大模型开发和应用的从业者。即使用户在自己的硬件上使用vLLM、SGLang等主流开源推理库，该问题依然存在。
*   **影响**：这种输出的不可重复性严重影响了模型的可靠性和在严谨场景下的落地。

#### **二、 传统误解与真相揭露**

*   **常见错误归因**：行业内部通常将问题归咎于：
    1.  **GPU并发执行**：认为GPU不同核心的计算完成顺序不同导致结果差异。
    2.  **浮点数运算误差**：认为浮点数计算本身固有的精度误差累积导致了不同结果。
*   **研究揭示的核心真相**：Thinking Machines Lab的研究表明，上述原因**并非问题核心**。真正的“罪魁祸首”是 **“批次不变性缺失”**。

#### **三、 核心真相：“批次不变性缺失”**

*   **什么是批次处理？** 大模型推理服务器为了效率，会将多个用户的请求打包成一个批次（Batch）进行处理。批次大小（Batch Size）会根据服务器负载动态变化（例如，低峰期批次大小为3，高峰期可能为16）。
*   **问题本质**：**相同的输入，在不同的批次大小下，会产生不同的输出**。这就像同一道菜，因为厨房同时炒的菜数量不同，味道就会变化一样荒谬。
*   **根本原因：浮点数的“非结合性”**
    *   **定义**：对于浮点数运算，`(a + b) + c ≠ a + (b + c)`。计算顺序的改变会导致结果不同。
    *   **例子**：`(0.1 + 1e20) - 1e20 = 0`，而 `0.1 + (1e20 - 1e20) = 0.1`。当大数和小数相加时，小数的精度会被“吞噬”。
    *   **与大模型的关联**：大模型的核心操作（矩阵乘法、RMSNorm、注意力机制）涉及海量的浮点数加法和乘法（即归约操作）。当批次大小变化时，GPU内核为了优化性能，会**改变这些归约操作的顺序**，从而导致最终计算结果出现差异。
*   **澄清误区**：实验证明，GPU本身的并发计算能力并不会导致非确定性。在固定计算任务下，GPU重复执行1000次的结果是完全一致的。问题在于**内核策略随批次大小而变化**。

#### **四、 解决方案：构建“批次不变内核”**

Thinking Machines Lab 针对三大核心操作提出了具体的解决方案，核心思想是**固定计算顺序，宁可牺牲部分性能也要保证一致性**。

1.  **RMSNorm（均方根归一化）**
    *   **问题**：计算均值（需归约求和）时，大批次用数据并行（每个核心处理一个样本，顺序固定），小批次用分割归约（一个样本分给多个核心，顺序改变）。
    *   **解决方案**：**强制采用数据并行策略**。无论批次多小，都保证每个样本的归约计算在单个核心内独立完成，固定顺序。

2.  **矩阵乘法**
    *   **问题**：为追求性能，内核会使用`Split-K`策略（拆分归约维度）或根据矩阵尺寸动态选择不同大小的张量核心指令，这些都改变了归约顺序。
    *   **解决方案**：
        *   **禁用`Split-K`策略**。
        *   **固定张量核心指令的尺寸**（如统一使用128x128x32的块大小）。

3.  **注意力机制**
    *   **问题**：最复杂。常规内核（如FlashAttention2）在编码和解码阶段会采用不同的并行策略（如Q维度并行、Split-KV），并且推理引擎（如vLLM）的KV缓存“分块预填充”也会导致归约顺序变化。
    *   **解决方案**：
        *   **固定KV缓存的分割大小**：不按分割数量，而是按固定元素数量（如每块256个元素）来拆分KV缓存，确保无论序列长度如何，拆分方式都一致。
        *   **统一KV缓存布局**：在注意力计算前，确保预填充和解码阶段的数据格式一致。

#### **五、 实验效果与性能权衡**

*   **实验设置**：使用Qwen2-72B模型，输入“介绍一下理查德·费曼”，温度设为0（贪婪采样），采样1000次。
*   **常规内核结果**：产生了**80种**不同的输出结果。前102个token一致（“费曼出生在1918年五月11日”），但从第103个token开始出现分歧（如“纽约皇后区” vs “纽约市”），这完美体现了归约差异的累积效应。
*   **批次不变内核结果**：1000次采样的输出**完全一致**，实现了真正的确定性。
*   **性能损耗**：
    *   未经优化的批次不变内核会使推理速度下降约2倍。
    *   经过优化（调整调度、内存访问等）后，性能损耗可控制在**20%左右**，对于需要高确定性的场景，这是一个可以接受的权衡。

#### **六、 重大意义与行业影响**

1.  **奠定可重复性与可靠性基石**：解决了大模型在科学研究和技术落地中“结果无法复现”的根本性问题。
2.  **解锁在线策略强化学习**：
    *   **之前困境**：训练（小批次）和推理（大批次）的输出不一致，导致策略偏移和训练崩溃。
    *   **解决后**：批次不变内核确保了训练与推理输出一致，KL散度可保持为0，使得**在线策略RL变得可行且稳定**。实验显示，无需重要性加权等校正，训练也能稳定进行。
3.  **推动AI在严谨领域的应用**：为金融、医疗、法律等需要高度可靠性和可审计性的领域铺平了道路。未来AI产品的输出将不再受服务器负载波动的影响。
4.  **转变行业焦点**：提醒从业者在追求性能（规模、速度）的同时，必须关注底层计算的**确定性**，这将成为未来技术竞争的新维度。

#### **七、 公司与背景信息**

*   **Thinking Machines Lab**：由OpenAI前CTO **米拉·穆拉蒂** 创立，自带“顶流”光环。
*   **惊人融资**：在**未发布任何产品**的情况下，于2024年7月完成**20亿美元**种子轮融资，估值高达**120亿美元**。投资方包括A16z、英伟达、AMD、思科等顶级机构。
*   **研究博客“联结主义”**：名字源于1980年代的AI子领域，寓意研究神经网络与生物大脑的相似性。此举旨在提升公众对AI的科学理解并与社区分享见解。其首款旗舰产品可能命名为“Connection Machine”。

#### **核心结论**

大模型推理的非确定性，其核心并非浮点数特性或GPU并发本身，而是**由动态变化的批次大小导致的内核计算顺序（归约顺序）不一致**。Thinking Machines Lab 通过构建 **“批次不变内核”** ，从底层解决了这一问题，为实现大模型的**可重复性、可靠性和高级训练范式（如在线策略RL）** 提供了关键性的技术突破。

