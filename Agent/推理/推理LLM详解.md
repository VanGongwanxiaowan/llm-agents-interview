https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms

https://mp.weixin.qq.com/s/8DHjWUAIjfuIjqcoJYCotA

# 《A Visual Guide to Reasoning LLMs》网页总结
## 一、文章基础信息
- **作者**：Maarten Grootendorst（机器学习工程师，GenAI领域创作者，开源项目BERTopic、PolyFuzz、KeyBERT开发者，《Hands-On Large Language Models》作者）
- **发布时间**：2025年2月3日
- **核心主题**：探索推理型大型语言模型（Reasoning LLMs）、测试时计算（Test-Time Compute）技术，深入解析DeepSeek-R1模型
- **特色**：含40余张定制可视化图表，帮助读者直观理解相关概念，支持中文、韩语、法语等多语言翻译

## 二、核心概念解析
### （一）推理型LLM（Reasoning LLMs）
- **定义**：与普通LLM相比，推理型LLM在回答问题前，会将问题拆解为更小的推理步骤（或称思维过程、思维链），核心是让模型学习“如何回答”而非仅“回答什么”。
- **本质**：虽暂无法确定LLM是否能像人类一样“思考”，但这些结构化的推理步骤能帮助模型进行更严谨的推断。

### （二）训练时计算（Train-Time Compute）
- **核心构成**：2024年中期前，提升LLM预训练性能的关键在于扩大三大要素规模，三者共同构成训练时计算：
  - 模型规模（参数数量）
  - 数据集规模（token数量）
  - 计算资源（FLOPs数量）
- **相关概念**：还包含训练和微调过程中所需的计算资源，曾是提升LLM性能的核心焦点。
- **缩放定律**：
  - 本质：研究模型规模（计算、数据集、参数）与性能的关联，属“幂律”关系，通常用对数坐标展示（呈直线），知名的有Kaplan定律和Chinchilla定律。
  - 差异：Kaplan定律认为固定计算下，扩大模型规模比扩大数据更有效；Chinchilla定律认为模型规模和数据同等重要。
  - 局限：2024年起，计算、数据集、参数持续扩大，但性能提升呈边际递减，引发“是否已达性能天花板”的疑问。

### （三）测试时计算（Test-Time Compute）
- **产生背景**：训练时计算成本高昂且性能提升放缓，促使研究者转向测试时计算。
- **核心逻辑**：无需持续增加预训练预算，而是让模型在推理（Inference）阶段“更长时间地思考”，通过生成更多含信息、关系和新想法的token，提升最终答案质量，而非直接输出答案。
- **缩放定律**：较新，有两大关键研究支持其有效性：
  - OpenAI研究：测试时计算可能遵循与训练时计算相似的性能提升趋势，预示范式转移。
  - 《Scaling Scaling Laws with Board Games》论文：以AlphaZero玩Hex游戏为例，发现训练时计算与测试时计算紧密相关，且测试时计算可按长度缩放。
- **分类**：分两大方向，对应不同优化焦点：
  | 分类 | 核心思路 | 具体技术 |
  |----|----|----|
  | 基于验证器的搜索（Search against Verifiers） | 先生成多个推理过程和答案样本，再用验证器（Reward Model）评分，属“输出导向” | 1. 多数投票（Self-Consistency）：选生成次数最多的答案<br>2. Best-of-N样本：用结果验证器（ORM）评答案质量，或用过程验证器（PRM）评推理步骤质量，还可加权选最高分<br>3. 带PRM的束搜索（Beam Search）：跟踪Top3“束”（高分路径），终止无效推理路径<br>4. 蒙特卡洛树搜索（MCTS）：分选择、扩展、滚动、反向传播四步，平衡“探索”与“利用” |
  | 修改提议分布（Modifying Proposal Distribution） | 训练模型生成更优推理步骤，调整token采样分布，属“输入导向” | 1. 提示工程（Prompting）：通过示例（上下文学习）或指令（如“Let’s think step-by-step”）引导模型推理，但模型未真正学会推理，且无法自我修正错误<br>2. STaR（Self-Taught Reasoner）：让模型生成推理数据，正确答案的推理直接加入训练集，错误答案则提供正确答案提示后让模型补全推理，再用于监督微调 |

## 三、DeepSeek-R1模型深度解析
### （一）模型定位
- 开源推理型LLM，权重可获取，与OpenAI o1直接竞争，基于DeepSeek-V3-Base模型优化，核心靠强化学习（非验证器）实现推理能力，无监督微调蒸馏推理行为。

### （二）关键前置模型：DeepSeek-R1 Zero
- **训练逻辑**：以DeepSeek-V3-Base为基础，仅用强化学习（无推理数据监督微调），通过简单提示要求推理过程置于特定标签内（不规定具体形式）。
- **奖励机制**：设两大规则奖励：
  - 准确性奖励：基于答案正确性评分。
  - 格式奖励：基于是否正确使用推理标签评分。
- **算法**：采用Group Relative Policy Optimization（GRPO），调整导致答案正确/错误的token和推理步骤的概率。
- **成果与局限**：模型自主发现最优思维链行为（含自我反思、自我验证），但可读性差、易混语言。

### （三）DeepSeek-R1训练流程（五步法）
1. **冷启动（Cold Start）**：用约5000token的高质量推理数据集微调DeepSeek-V3-Base，解决可读性差的冷启动问题。
2. **推理导向强化学习**：沿用DeepSeek-R1 Zero的RL流程，新增语言一致性奖励，确保目标语言稳定。
3. **拒绝采样（Rejection Sampling）**：用RL训练后的模型生成合成推理数据，结合规则奖励和DeepSeek-V3-Base验证，得到60万高质量推理样本；同时用DeepSeek-V3生成20万非推理样本。
4. **监督微调（Supervised Fine-Tuning）**：用80万样本（60万推理+20万非推理）微调DeepSeek-V3-Base。
5. **全场景强化学习**：对微调后的模型进行RL训练，沿用DeepSeek-R1 Zero的方法，新增“有用性”“无害性”奖励以对齐人类偏好，还要求模型总结推理过程提升可读性。

### （四）推理能力蒸馏
- **需求**：DeepSeek-R1含6710亿参数，消费级硬件难运行，需将其推理能力蒸馏到小模型（如Qwen-32B）。
- **方法**：以DeepSeek-R1为“教师模型”，小模型为“学生模型”，让两者接收相同提示并生成token概率分布，训练学生模型拟合教师模型的分布，使用80万高质量样本训练，最终小模型性能优异。

### （五）失败尝试
- 尝试用过程验证器（PRM）和蒙特卡洛树搜索（MCTS）实现推理，但存在问题：MCTS搜索空间大，需限制节点扩展；PRM的Best-of-N技术需反复训练验证器防“奖励黑客”，计算开销大。

## 四、结论与资源
### （一）核心结论
- 推理型LLM的发展标志着从“训练时计算缩放”到“测试时计算缩放”的范式转移，测试时计算通过让模型在推理阶段更充分思考，成为提升LLM性能的关键方向。
- DeepSeek-R1通过强化学习和监督微调结合的方式，成功实现高效推理能力，其蒸馏技术也为小模型拥有强推理能力提供了可行路径。

### （二）推荐资源
- 《The Illustrated DeepSeek-R1》：Jay Alammar撰写的深度指南。
- Hugging Face文章：含测试时计算缩放的实验分析。
- 视频《Speculations on Test-Time Scaling》：详解测试时计算技术细节。
- 作者著作《Hands-On Large Language Models》：含更多LLM可视化内容，可在官网、亚马逊购买，代码托管于GitHub。
