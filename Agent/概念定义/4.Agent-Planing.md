### 4. Agent Planning

参考来源: Understanding the planning of LLM agents: A survey

《Understanding the planning of LLM agents: A survey》文章中针对大语言模型驱动的智能体的**“规划能力”**（planning ability）进行全面系统的综述。

论文首次以规划能力为核心，提出了完整的分类体系，并对各类方法进行了深度分析。

### 4.1 整体分类方法

作者构建了一个五大类的规划方法分类体系：

<img width="562" height="515" alt="image" src="https://github.com/user-attachments/assets/3bdce6ac-2d8a-4e0c-a1a8-f2ad3cc6903a" />


- **任务分解（Task Decomposition）**：将复杂任务拆成多个子任务后逐步规划。
- **生成并选择多条计划（Multi-plan Selection）**：LLM生成多个可能方案，再进行优选。
- **外部规划器辅助（External Planner-Aided Planning）**：将任务形式化后，借助传统符号规划器或神经规划器处理。
- **反思与优化（Reflection & Refinement）**：通过生成 - 反馈 - 改进的循环增强规划质量。
- **带记忆的规划（Memory-Augmented Planning）**：通过引入记忆机制（如检索或调优）提高规划效果。

### 4.2 任务分解（Task Decomposition）
#### 4.2.1 核心思想
采用**“分而治之”**的策略，将复杂的多步骤任务分解为更简单、更易管理的子任务。

#### 4.2.2 两种实现方式
##### 4.2.2.1 分解优先方法（Decomposition-First）
- 工作流程：**首先完整分解任务 → 然后逐一规划子任务**
- 代表方法：
  - HuggingGPT：将多模态任务分解为子任务，明确任务间依赖关系，协调不同模型协作
  - Plan-and-Solve：改进零样本CoT，用“先制定计划，再执行计划”的两步提示
  - ProgPrompt：将自然语言任务转化为编程问题，通过函数调用实现规划

##### 4.2.2.2 交替分解方法（Interleaved）
- 工作流程：**动态交替进行任务分解和子任务规划**
- 代表方法：
  - Chain-of-Thought（CoT）：通过逐步推理链引导复杂问题解决
  - ReAct：交替进行推理（Thought）和行动（Action），解耦思考和执行
  - PAL/PoT：结合编程能力，将推理过程形式化为代码执行

#### 4.2.3 优势与挑战
- 优势：降低复杂任务难度，提高可解性
- 挑战：分解粒度控制、上下文长度限制、错误传播问题

### 4.3 多方案生成与选择（Multi-Plan Selection）
#### 4.3.1 核心思想

**生成多个候选规划方案，通过搜索算法或评估机制选择最优方案。**

#### 4.3.2 两个关键步骤
##### 4.3.2.1 多方案生成
- **采样策略**：利用解码过程的不确定性（温度采样、top - k采样）
- **显式生成**：通过提示工程明确要求生成多个方案
- 代表方法：
  - Self-consistency：对同一问题采样多个推理路径
  - Tree-of-Thought（ToT）：构建思维树结构，支持采样和提议两种生成策略

##### 4.3.2.2 最优方案选择
- 投票机制：Self-consistency采用多数投票
- 树搜索：ToT支持BFS/DFS等传统搜索算法

#### 4.3.3 优势与挑战
- 优势：扩展搜索空间，提高方案质量和鲁棒性
- 挑战：计算成本显著增加，LLM评估能力有限，随机性影响一致性

### 4.4 外部规划器辅助（External Planner-Aided Planning）
#### 4.4.1 核心思想
结合**LLM的语义理解能力**与专门规划器的**高效性和可靠性**。

#### 4.4.2 两类外部规划器
##### 4.4.2.1 符号规划器
- 工作机制：**LLM负责任务形式化 → 符号规划器生成方案**
- 代表方法：
  - LLM+P：将任务转换为PDDL格式，使用Fast-Downward求解器
    - 三阶段管道：1.自然语言 → PDDL转换；2.使用Fast-Downward求解器规划；3.PDDL结果 → 自然语言翻译
    - 挑战：需要准确的PDDL建模，对复杂领域适应性有限
  - 什么是PDDL：PDDL（Planning Domain Definition Language）是人工智能规划领域的标准语言，用于描述规划问题。
  - LLM-DP：针对动态环境，结合BFS求解器
    - 创新点：处理不完整信息环境下的动态规划；LLM生成多个可能的世界状态，规划器为每个状态生成计划。
    - 适应性：能够根据环境反馈动态调整规划。

##### 4.4.2.2 神经规划器
- 工作机制：**结合轻量级神经网络的领域特定规划能力**
- 代表方法：
  - SwiftSage：基于双过程理论，快思考（Decision Transformer） + 慢思考（LLM推理）
    - 理论基础：基于认知心理学的双过程理论
    - 快思考：Decision Transformer进行快速直觉响应
    - 慢思考：复杂问题时切换到LLM进行深度推理
    - 自适应切换：根据执行结果动态选择思考模式

#### 4.4.3 优势与挑战
- 优势：理论完备性、稳定性、可解释性强
- 挑战：需要准确的任务形式化，领域适应性限制

### 4.5 反思与优化（Reflection & Refinement）
#### 4.5.1 核心思想
通过自我反思和错误分析来迭代改进规划质量，类似**“语言层面的强化学习”**。

#### 4.5.2 实现机制
##### 4.5.2.1 反思过程
- 错误检测：识别规划执行中的失败点
- 原因分析：分析失败的根本原因
- 经验总结：提炼可用于后续规划的经验教训

##### 4.5.2.2 改进策略
- 代表方法：
  - Self-Refine：生成 - 反馈 - 改进的迭代循环
  - Reflexion：扩展ReAct，加入轨迹评估和自我反思

#### 4.5.3 优势与挑战
- 优势：提高容错能力，支持从失败中学习
- 挑战：文本更新的收敛性缺乏理论保证，**可能陷入循环反思**

### 4.6 带记忆的规划（Memory-Augmented Planning）
利用外部记忆模块存储和检索有价值信息：
- RAG基础记忆：Generative Agents、MemoryBank、MemGPT等
- 具身记忆：通过参数微调嵌入历史经验
- 权衡：RAG方法灵活实时但依赖检索效果；微调方法容量大但更新成本高

### 4.7 小结
⚽ Planning 和核心能力有哪些：
- **任务分解（Task Decomposition）**：将复杂任务拆成多个子任务后逐步规划。
- **生成并选择多条计划（Multi-plan Selection）**：LLM生成多个可能方案，再进行优选。
- **外部规划器辅助（External Planner-Aided Planning）**：将任务形式化后，借助传统符号规划器或神经规划器处理。
- **反思与优化（Reflection & Refinement）**：通过生成 - 反馈 - 改进的循环增强规划质量。
- **带记忆的规划（Memory-Augmented Planning）**：通过引入记忆机制（如检索或调优）提高规划效果。

| 类别               | 核心思想                          | 优势与挑战                        |
|--------------------|-----------------------------------|-----------------------------------|
| 任务分解           | 分解复杂任务→逐步规划             | 可提升性能，但上下文长度与计算成本限制 |
| 多方案生成与选择   | 生成多计划并优选                  | 探索能力强，代价高，选择机制需优化  |
| 外部规划器辅助     | LLM形式化任务→符号/神经规划器处理 | 结合统计与符号优势，方向明确        |
| 反思与优化         | 循环生成—反思—改善计划            | 增强容错性，但缺少理论收敛保障      |
| 带记忆的规划       | 引入RAG或微调，记忆辅助规划        | 提升记忆能力，成本与检索准确性仍是挑战|
