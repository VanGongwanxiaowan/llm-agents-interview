

## 1. RLVR 的核心思想

RLVR 的目标是：

* **让强化学习更可靠**，尤其在大模型（LLM / Multimodal Agent）中。
* 强化学习的关键是 **奖励函数（Reward Function）**，但在真实应用中，奖励函数往往难以定义、容易偏差。
* RLVR 引入 **可验证奖励（Verifiable Reward）** 的机制：

  * 不是完全依赖人工设计的 reward
  * 而是通过 **形式化验证 / 自动化检查 / 判别器** 来确保 reward 的可靠性

换句话说，RLVR 是在强化学习里增加了一层 **Reward Verification（奖励可验证性）**，避免 agent 学到投机取巧的行为。

---

## 2. RLVR 在 Agent 训练中的问题背景

在 Agent 里，如果直接用 RLHF（Reinforcement Learning with Human Feedback）：

* 优点：人类偏好更自然
* 缺点：

  * 人类反馈代价高（需要人工标注）
  * 偏好有噪声和主观性
  * 在多步骤任务（workflow）中，奖励延迟、信用分配困难

于是 RLVR 的作用就是：

* 用 **自动化规则 + 检查器** 替代一部分人工
* 在 reward 层面做 **约束和验证**，提升稳定性和可靠性

---

## 3. RLVR 的具体机制

可以分成 3 个部分：

### (1) **Reward 建立**

* 来自任务目标（比如 “SQL 语句必须执行成功”）
* 来自判别器（Discriminator 或 LLM-based Judge）
* 来自可验证条件（逻辑约束、语法校验、单元测试等）

### (2) **Verification（验证过程）**

* 确认输出是否满足 **明确可验证的标准**

  * 例如：代码是否能跑通？
  * 结果是否通过单元测试？
  * 输出格式是否符合 JSON Schema？

### (3) **Policy Update（策略优化）**

* 将 **验证通过** 的作为正向奖励
* **未通过** 的直接给予惩罚
* 强化学习算法常见选择：PPO / A2C / GRPO

---

## 4. RLVR 在 Agent 当中的应用场景

✅ **代码代理（Code Agent）**

* 通过编译器、单测、静态检查来验证 reward
* 比 RLHF 更客观，不会被人类偏见干扰

✅ **多步骤推理 Agent（ReAct / Workflow Agent）**

* 验证每个中间步骤的逻辑是否正确
* 防止出现“幻觉”（hallucination）却依然得到奖励的情况

✅ **对话 Agent（QA 系统 / 搜索 Agent）**

* 检查输出是否包含事实支持（比如引用来源）
* 验证是否符合任务指令（格式、长度、包含关键词）

---

## 5. RLVR 相比 RLHF 的区别

| 对比点  | RLHF       | RLVR          |
| ---- | ---------- | ------------- |
| 奖励来源 | 人类偏好       | 可验证规则 / 自动化检查 |
| 成本   | 高（人工标注）    | 低（程序验证）       |
| 主观性  | 强          | 弱             |
| 应用范围 | 自然对话、风格化任务 | 明确目标、有约束的任务   |
| 稳定性  | 中等         | 高             |

所以通常会结合使用：

* **RLHF**：保证输出更符合人类价值、偏好
* **RLVR**：保证输出在客观可验证层面正确

---

## 6. 在 Agent 系统中的位置

在三类 Agent 解决方案里（你之前让我总结过的 1-2-3 框架）：

1. **Prompt engineering + Workflow** → 基础层
2. **协议 token + 工具调用接口 + GRPO 微调** → 进阶层
3. **RLVR + 系统级可靠性验证** → 高级层（面向实际部署）


