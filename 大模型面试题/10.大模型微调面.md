好的，这是关于大模型（LLMs）微调方面的面试题详细解答。

### 三、大模型（LLMs）微调面

---

#### 39 大模型 sft 过程中，为什么会出现第二个epoch的时候loss会突然下降问题？

这种现象并不常见，但有时会发生。一个可能的主要原因是**学习率预热（Learning Rate Warm-up）和数据集顺序的影响**。

1.  **学习率预热结束**：很多训练设置中，第一个epoch会使用一个从很低值逐渐增加到目标值的学习率（即Warm-up）。在第一个epoch结束时，学习率可能才刚达到预设的峰值。进入第二个epoch时，学习率已经稳定在较高的有效值，模型开始以“全功率”进行学习，因此loss可能会有一个比较明显的下降。
2.  **数据集顺序的随机性**：在第一个epoch中，模型第一次接触所有数据，它可能还在努力学习和适应数据集的整体分布和不同任务。在第二个epoch开始时，数据被打乱。模型可能先遇到了某些更容易学习或更符合其当前状态的数据样本，导致loss一开始就快速下降。
3.  **过拟合的开始**：如果数据集规模较小，模型在第一个epoch可能还没有完全记住训练数据。进入第二个epoch后，模型开始“回忆”起之前见过的样本，导致训练loss迅速下降，但这可能是过拟合的早期信号，需要密切关注验证集loss的变化。

**核心要点**：这种现象通常与训练超参数（尤其是学习率调度）和数据集的特性有关，并不一定是模型或代码有问题。但需要监控后续epoch的验证集性能，避免过拟合。

---

#### 1 如果想要在某个模型基础上做全参数微调，究竟需要多少显存？

全参数微调（Full Fine-Tuning）的显存占用主要由四部分组成：

1.  **模型权重（Model Weights）**：通常以FP16（2字节/参数）或BF16存储。例如，一个7B（70亿）参数的模型，权重占用约为 `7e9 * 2 bytes = 14 GB`。
2.  **优化器状态（Optimizer States）**：对于常用的AdamW优化器，每个参数需要存储：
    *   动量（momentum）： 2字节/参数
    *   方差（variance）： 2字节/参数
    *   参数的副本（通常为FP32）： 4字节/参数
    *   总计： **~8字节/参数**。7B模型的优化器状态需要 `7e9 * 8 bytes = 56 GB`。
3.  **梯度（Gradients）**：通常以FP16或BF16存储，占 `2字节/参数`。7B模型的梯度需要 `7e9 * 2 bytes = 14 GB`。
4.  **前向传播激活值（Forward Activations）**：为计算反向传播，需要保存中间激活值。这部分开销与批次大小（batch size）、序列长度（sequence length）密切相关，非常巨大且可变。一个粗略的估计是，它可能与模型权重本身的大小相当甚至更大。

**总显存估算公式（粗略）**：
`总显存 ≈ (模型权重 + 梯度 + 优化器状态 + 激活值)`

对于7B模型，仅前三项（权重+梯度+优化器状态）就至少需要 `14 + 14 + 56 = 84 GB`。再加上巨大的激活值，很容易超过 **100GB**。这就是为什么全微调一个7B模型通常需要多张A100（80GB）显卡。

**降低显存的方法**：使用**参数高效微调（PEFT）** 技术，如LoRA、QLoRA，可以极大地减少优化器状态和梯度的开销。

---

#### 2 为什么SFT之后感觉LLM傻了?

SFT（监督微调）后模型变“傻”或能力下降，通常被称为**灾难性遗忘（Catastrophic Forgetting）**。主要原因如下：

1.  **指令数据分布狭窄**：SFT使用的指令数据如果多样性不足，只集中在某个领域或某种任务上，模型会过度优化以适应这些特定指令，而遗忘在预训练阶段学到的广泛通用知识和能力。
2.  **过拟合**：在数据量较小或训练轮次过多时，模型可能会记住训练集中的具体样本，而不是学会泛化的指令遵循能力。这导致它在训练集上表现很好，但遇到新问题时表现呆板、缺乏创造力。
3.  **数据质量差**：如果SFT数据中包含低质量的回答、错误的事实或糟糕的推理逻辑，模型会学会这些不好的模式，导致输出质量下降。
4.  **对齐税（Alignment Tax）**：为了让模型更安全、更无害、更符合人类偏好（通过RLHF），有时会以牺牲一部分通用性能为代价。SFT是RLHF的第一步，也可能引入一定的“税”。
5.  **学习率等超参数不当**：过高的学习率可能会“冲刷”掉预训练中获得的知识。

**缓解方法**：在SFT数据中混合一部分高质量的通用指令数据（如Alpaca数据、OpenAI的ShareGPT数据），或者在训练时混合一部分预训练损失（PTX Loss），以帮助模型保留原有知识。

---

#### 3 SFT 指令微调数据 如何构建?

构建高质量的SFT数据是微调成功的关键。核心原则是：**高质量、多样性、与目标对齐**。

1.  **数据格式**：通常采用`<Instruction>`, `<Input>`, `<Output>`的JSONL格式。
    *   `Instruction`： 清晰、明确的指令。
    *   `Input` (可选)： 为指令提供的额外上下文或输入信息。
    *   `Output`： 期望模型生成的、符合指令的高质量回答。
2.  **数据来源**：
    *   **人工撰写**：质量最高，但成本也最高。由专家或标注人员编写指令和标准答案。
    *   **self-instruct**：使用一个强大的教师模型（如GPT-4）自动生成指令和回答。**常用方法：** 人工撰写少量种子示例 -> 用大模型批量生成新指令和回答 -> 人工或模型筛选过滤。
    *   **现有数据集转化**：将已有的NLP数据集（如问答、摘要、翻译）转化为指令格式。
    *   **用户真实数据**：收集产品中用户的真实提问和人工助理的优秀回答。这是非常宝贵的数据。
3.  **数据内容**：
    *   **任务多样性**：涵盖分类、生成、摘要、翻译、代码、推理、创意写作等多种任务。
    *   **风格多样性**：包含不同长度、不同风格（正式、口语化）的回答。
    *   **负样本（可选）**：可以加入一些被标注为低质量的回答，帮助模型学会辨别好坏。

##### 3.1 提升sft的prompt的代表性有什么好的方法？
*   **任务分类法**：先定义一个详细的任务分类体系（如：知识问答、创意写作、逻辑推理、文本修订、编程……），然后确保每个类别下都有足够数量的prompt。
*   **聚类采样**：从海量的prompt池中，通过文本嵌入（Embedding）进行聚类，然后从每个簇中采样prompt，确保覆盖不同的语义空间。
*   **模板变异**：对核心指令进行改写、复述，生成多个表达不同但语义相似的prompt，增加覆盖度。
*   **对抗性生成**：主动思考模型在哪些方面表现薄弱，针对性地生成这些方面的prompt进行加强训练。

##### 3.2 提升sft的prompt的数据量有什么好的方法？
*   **Self-Instruct**：如上所述，这是最核心的扩量方法，利用大模型本身进行数据扩充。
*   **Web数据挖掘**：从网络上挖掘符合指令-回答对格式的数据，例如一些论坛的Q&A（Stack Overflow）、教程文章等，并进行清洗和格式化。
*   **数据合成**：通过代码等手段合成数据，例如为代码模型合成“写一个函数实现X功能”的指令和对应的代码。
*   **翻译与回译**：将其他语言的高质量指令数据翻译成目标语言（如中文），或者用回译的方式（中文->英文->中文）进行数据增强。

---

#### 4 领域模型Continue PreTrain 数据选取？

领域持续预训练（CPT）的数据选取直接决定模型学到的知识质量。

1.  **高质量与高相关性**：数据必须来自目标领域，且质量要高。优先选择权威教材、学术论文、专业网站、经过审核的文档等，避免选择来源不明、错误百出的数据。
2.  **数据规模**数据量要足够大，通常需要数十GB甚至TB级别，才能有效更新模型的参数。
3.  **数据纯净度**：需要进行严格的数据清洗，去除无关内容（广告、导航栏、HTML标签）、重复数据、低质量文本（乱码、内容过短）。
4.  **多样性**：虽然聚焦领域，但领域内部也应有多样性。例如，金融领域应包含宏观经济、公司财报、市场新闻、风险管理等子领域的数据。
5.  **长文本占比**：包含一定比例的长文档（如完整的报告、论文），有助于模型学习长程逻辑依赖和领域知识的结构化组织。

---

#### 5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？

缓解“灾难性遗忘”是领域适配的关键挑战。常用方法：

1.  **混合训练（Mix Training）**：在领域数据中混合一定比例（如5%~20%）的通用预训练数据（如C4、Wikipedia等）。这是最常用且有效的方法。
2.  **控制领域数据比例**：在每一个训练批次（batch）中，严格控制领域数据与通用数据的比例，避免模型被领域数据完全主导。
3.  **调整学习率**：使用较小的学习率进行领域预训练，温和地更新模型参数，减少对通用知识的破坏。
4.  **参数高效微调（PEFT）**：采用LoRA等PEFT方法，只微调一小部分参数，而冻结主体模型参数，从而最大程度地保留原始能力。
5.  **评估与迭代**：持续在通用基准（如MMLU、C-Eval）和领域基准上评估模型，根据结果调整数据混合策略和超参数。

---

#### 6 领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？

1.  **数据质量与数量**：核心是提供**大量**且**高质量**的领域文本。知识蕴含在数据中。
2.  **数据多样性**：确保领域数据覆盖所有子领域和应用场景，避免偏差。
3.  **保留完整上下文**：尽量使用长文本、完整的文档进行训练，而不是切成碎片。这有助于模型学习知识之间的关联和逻辑。
4.  **任务式预训练**：在预训练中引入一些简单的提示，例如“摘要上述文档：”、“这篇文章主要讲了：”，让模型在预训练阶段就潜移默化地学习一些指令理解的能力，为后续SFT打好基础。
5.  **优化训练目标**：除了标准的LM Loss，可以尝试加入一些辅助目标，但主流仍以LM Loss为主。

---

#### 7 进行SFT操作的时候，基座模型选用Chat还是Base?

**强烈建议使用Base模型（如LLaMA-2-7B-Base， ChatGLM3-6B-Base）作为SFT的基座。**

**原因：**
*   **Base模型**：是经过海量数据预训练得到的“知识库”，它拥有最原始、最广泛的知识和能力，没有经过任何指令对齐和人类偏好调整。它是一张“白纸”，适合你在上面绘制任何你想要的指令遵循模式。
*   **Chat模型**：是在Base模型基础上，已经经过了SFT和（通常还有）RLHF的模型。它已经被对齐到一种通用的对话和指令遵循模式。如果你在此基础上用你的领域数据做SFT，会发生两种对齐模式的冲突，可能导致效果不佳、性能下降或行为不可预测。

**结论**：如果你想得到一个完全适配你特定需求和风格的模型，请从Base模型开始微调。Chat模型更适合直接推理使用，或者通过少量提示工程（Prompt Engineering）来完成任务。

---

#### 8 领域模型微调 指令&数据输入格式 要求？

格式需要与基座模型的训练格式保持一致，否则需要调整模型架构（如tokenizer）。

1.  **遵循基座模板**：例如，如果基座是LLaMA-2，就应使用其Chat格式的模板：
    ```
    <s>[INST] <<SYS>>
    {你的系统提示}
    <</SYS>>

    {用户指令和输入} [/INST] {模型理想回答} </s>
    ```
    如果是ChatGLM3，则使用其自定义的格式：
    ```
    <|system|>
    {系统提示}
    <|user|>
    {用户指令和输入}
    <|assistant|>
    {模型理想回答}
    ```
2.  **系统提示（System Prompt）**：在领域微调中，系统提示非常重要，用于设定模型的身份、领域背景和回答风格。例如：“你是一名专业的医疗助手，请根据你的知识用中文谨慎、准确地回答用户问题。”
3.  **一致性**：所有训练数据都必须严格按照相同的格式进行构建，否则模型会感到困惑。
4.  **空格和特殊Token**：严格遵循原模型要求，注意特殊token（如`<s>`, `</s>`, `<INST>`等）和换行空格的使用。

---

#### 9 领域模型微调 领域评测集 构建？

“没有测量，就没有改进。” 构建领域评测集（Evaluation Benchmark）至关重要。

1.  **评估维度**：
    *   **领域知识准确性**：模型回答的事实是否正确。
    *   **推理能力**：在领域内的逻辑推理是否正确。
    *   **安全性**：回答是否谨慎、无害，特别是在医疗、法律等高风险领域。
    *   **指令遵循**：是否遵守了指令中的要求（如格式、长度）。
    *   **与通用能力的平衡**（可选）：在通用基准上的表现是否下降太多。
2.  **数据来源**：
    *   **人工编制**：由领域专家编写高质量的问题和标准答案（或评分标准）。
    *   **从专业考试中抽取**：例如，医学模型可以USMLE（美国执业医师考试）题目作为评测集。
    *   **从文档中构造QA对**：从领域文档中构造“开卷考试”题，确保答案明确存在于参考文档中。
    *   **用户真实问题**：收集真实场景中的用户问题作为测试题。
3.  **评估方式**：
    *   **自动化评估**：对于有标准答案的客观题，可以使用精确匹配（EM）、 Rouge-L、BLEU等指标。
    *   **人工评估**：对于主观题或开放题，必须由专家从“准确性、完整性、有用性”等维度进行打分（如1-5分）。这是黄金标准。
    *   **模型评估**：使用强大的模型（如GPT-4）作为裁判，对其他模型的回答进行评分。这种方法成本低、可扩展，正变得越来越流行。

---

#### 10 领域模型词表扩增是不是有必要的？

**通常不是首要任务，但在特定情况下非常必要。**

*   **不需要扩增的情况**：大多数领域（如金融、医学、法律）的专业术语，虽然生僻，但通常可以被现有词表（尤其是大词表模型如ChatGLM）拆分成合理的子词（Subword）单元（如`心血管` -> `心` + `血管`），对理解和解码效率影响不大。
*   **需要扩增的情况**：
    1.  **化学、生物学等领域**：有大量非常长且复杂的专业词汇（如化合物名称`甲基乙基氨基甲酰氯`、基因名称）。这些词被拆散后会导致序列过长、语义丢失、解码困难。
    2.  **多语言模型**：如果基座是英文词表，而要用于中文领域，扩增高质量中文字词是必要的。
    3.  **代码模型**：扩增代码相关的词汇可能有益。

**扩增的挑战**：词表扩增意味着要**重置并重新训练模型的嵌入层（Embedding Layer）和输出层（Output Layer）**，这是一个非常昂贵的操作，几乎相当于从头预训练。因此，需要谨慎决策。

**建议**：优先考虑使用现有词表的模型。如果确实需要，可以先尝试在领域数据上对现有tokenizer进行训练，统计出最高频的未登录词，然后谨慎地添加最必要的一小部分新词。

---

#### 11 如何训练自己的大模型？

对于绝大多数公司和研究者，所谓的“训练自己的大模型”指的是**微调**，而非从零开始预训练。

**从零预训练（成本极高，不现实）**：
1.  **数据准备**：收集TB级别的高质量文本数据，并进行 rigorous 清洗、去重、排序。
2.  **架构选择**：确定模型架构（如Decoder-only的GPT类）、参数规模、词表等。
3.  **基础设施**：准备成千上万张高端GPU（如A100/H100）集群和高速网络。
4.  **训练**：使用Megatron-LM、DeepSpeed等分布式训练框架，进行数周甚至数月的训练。
5.  **评估与迭代**：在大量基准上评估模型性能。

**微调（现实路径）**：
1.  **选择基座模型**：根据需求选择开源Base模型（如LLaMA-2, ChatGLM3, Qwen, Baichuan）。
2.  **数据准备**：收集和构建高质量的指令微调（SFT）数据或领域持续预训练（CPT）数据。
3.  **选择微调方法**：
    *   **全参数微调**：效果好，但显存要求极高。
    *   **参数高效微调（PEFT）**：如LoRA、QLoRA，是当前的主流。用少量显存达到接近全微调的效果。
4.  **训练**：使用训练框架（如Transformers, PEFT, DeepSpeed）在单机或多机GPU上进行微调。
5.  **评估与部署**：使用构建的评测集评估模型效果，满意后部署应用。

---

#### 12 训练中文大模型有啥经验？

1.  **选择合适的基础模型**：优先选择**原生中文能力强**的开源模型作为基座，例如Qwen（通义千问）、ChatGLM、Baichuan（百川）、InternLM（书生）等。它们在预训练阶段就包含了大量中英文数据，其中文词表和对中文的理解远胜于LLaMA等主要为英语设计的模型。
2.  **高质量中文数据**：数据是灵魂。确保SFT或CPT数据是**高质量、地道的中文**。避免使用机翻的英文指令数据，其中文表达往往生硬不自然。
3.  **词表问题**：如果坚持要用LLaMA等模型，可能需要扩充中文字词，但如上所述，成本很高。直接用原生中文模型可避免此问题。
4.  **文化背景适配**：中文数据和应用场景包含许多中国特有的文化、历史、社会现象和政策法规，需要在数据构建和评测中充分考虑。
5.  **评测基准**：使用权威的中文评测基准来衡量能力，如**C-Eval**（学科知识）、**Gaokao-Bench**（高考题）、**LongBench**（长文本理解）、**CMB**（中文医学基准）等。

---

#### 13 指令微调的好处？

指令微调（SFT）是赋予预训练模型“智慧”和“可用性”的关键一步。

1.  **激发指令遵循能力**：让模型学会理解并执行以自然语言指令形式给出的任务，使其从“下一个词预测器”变成“任务助手”。
2.  **解锁涌现能力**：很多复杂的推理、概括、思维链能力在预训练模型中虽然潜在存在，但需要通过SFT来激发和引导出来。
3.  **塑造对话风格和输出格式**：教会模型以期望的风格（如友好、专业、简洁）和格式（如JSON、列表）进行回答。
4.  **提高安全性和有用性**：通过精心设计的SFT数据，可以引导模型生成更安全、更无害、更有帮助的回答，为后续的RLHF打下基础。
5.  **领域适配**：通过领域特定的指令数据，可以快速地将一个通用模型适配到专业领域。

**简而言之，SFT教会了模型“如何与人交流并为人所用”。**

---

#### 14 预训练和微调哪个阶段注入知识的？

**知识主要是在预训练阶段注入的。**

*   **预训练（Pre-training）**：模型在TB级别的海量无标注文本上进行自监督学习（预测下一个词）。这个过程的核心就是**将人类知识压缩并存储到模型的千亿级参数中**。模型学到的是**通用的语言规律和世界知识**。
*   **微调（Fine-tuning, 包括SFT）**：**不注入大量新知识**，而是**激发和引导**预训练阶段已经学到的知识。它更像是一个“教学”过程，教会模型如何根据指令或偏好来组织和运用它已有的知识。
    *   **SFT**：教的是**任务格式和指令理解**。
    *   **RLHF**：教的是**回答的偏好和风格**（哪种回答更好）。

如果模型在预训练后缺乏某些领域知识，仅靠SFT是很难补上的。这时就需要**持续预训练（CPT）**，CPT本质上是预训练的延伸，是真正注入**新领域知识**的阶段。

---

#### 15 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？

这取决于目标领域知识与基座模型已有知识的差距。

1.  **选择持续预训练（CPT）的情况**：
    *   **知识鸿沟大**：领域包含大量**基座模型在预训练时几乎未接触过的全新知识**（例如，非常前沿的科研领域、高度机密的公司内部文档、某种极其小众的语言）。
    *   **数据量大**：你拥有该领域**海量的纯文本数据**（GB/TB级别）。
    *   **目标**：目标是让模型**从根本上掌握这些新知识**，成为该领域的“专家”。

2.  **选择指令微调（SFT）的情况**：
    *   **知识鸿沟小**：领域知识**大部分已经存在于基座模型中**，只是模型不知道如何有效地调用和组织它们来回答专业问题。（例如，让一个通用模型扮演医学助手，大部分医学知识它已经学过，但不会用）。
    *   **数据量小**：你只拥有**相对少量的指令-回答对数据**（几千到几万条）。
    *   **目标**：目标是**激活和引导**模型已有的知识，教会它如何以领域专家的方式和格式进行对话和回答问题。

**常见且强大的策略**：**CPT + SFT**。
*   先通过**持续预训练**，将大量的领域新知识注入模型。
*   再通过**指令微调**，教会模型如何灵活运用这些新知识来完成任务。
这是打造高质量领域模型的最有效路径。
