好的，我们来详细探讨相似度函数和对比学习相关的面试题。

---

### 相似度函数篇

#### 一、除了cosine还有哪些算相似度的方法？

相似度（或距离）度量是机器学习和数据科学的基础。除了最常用的**余弦相似度（Cosine Similarity）**，还有许多其他重要方法，可以根据数据类型和问题需求选择。它们大致可分为**距离度量**和**相似度系数**两大类。

**1. 距离度量（值越小越相似）**
   *   **欧氏距离 (Euclidean Distance)**
       *   **公式**：$d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$
       *   **解释**：最直观的“直线距离”。衡量的是空间中两点的绝对距离。
       *   **适用场景**：适用于低维、连续且量纲一致的特征空间（如物理坐标）。对异常值敏感。

   *   **曼哈顿距离 (Manhattan Distance)**
       *   **公式**：$d(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^n |x_i - y_i|$
       *   **解释**：得名于纽约曼哈顿的网格状街道规划。想象一下从一个十字路口到另一个只能沿街道走，不能走对角线。
       *   **适用场景**：常用于离散变量、稀疏向量（如词袋模型），在某些情况下比欧氏距离更鲁棒。

   *   **切比雪夫距离 (Chebyshev Distance)**
       *   **公式**：$d(\mathbf{x}, \mathbf{y}) = \max_i |x_i - y_i|$
       *   **解释**：各维度数值差绝对值的最大值。可以理解为“国王的移动”，国王可以朝任何方向移动一步，从一点到另一点所需的最少步数。
       *   **适用场景**：适用于特别关心单个维度上最大差异的场景，如物流、仓储规划。

   *   **马哈拉诺比斯距离 (Mahalanobis Distance)**
       *   **公式**：$d(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T \mathbf{S}^{-1} (\mathbf{x} - \mathbf{y})}$，其中 $\mathbf{S}$ 是协方差矩阵。
       *   **解释**：欧氏距离的升级版，考虑了数据各维度之间的相关性并进行了标准化。如果数据是各向同性的（协方差矩阵是单位矩阵），则退化为欧氏距离。
       *   **适用场景**：处理高度相关或量纲不一致的数据，用于异常检测非常有效。

**2. 相似度系数（值越大越相似）**
   *   **皮尔逊相关系数 (Pearson Correlation Coefficient)**
       *   **公式**：$\rho_{\mathbf{x}, \mathbf{y}} = \frac{\text{cov}(\mathbf{x}, \mathbf{y})}{\sigma_{\mathbf{x}} \sigma_{\mathbf{y}}}} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}$
       *   **解释**：衡量两个向量**变化趋势**的一致性（线性相关性）。取值范围为[-1, 1]。
       *   **与余弦相似度的关系**：**余弦相似度计算的是两个向量在原始空间中的夹角，而皮尔逊相关系数计算的是两个向量各自减去均值后（中心化）的夹角**。如果数据已中心化，两者等价。

   *   **Jaccard 相似系数 (Jaccard Index)**
       *   **公式**：$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$
       *   **解释**：衡量两个集合的相似度，即交集大小除以并集大小。非常直观。
       *   **适用场景**：处理集合特征或稀疏二元向量（如用户购买商品集合、关键词集合）。

   *   **点积 (Dot Product) / 内积 (Inner Product)**
       *   **公式**：$s(\mathbf{x}, \mathbf{y}) = \mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^n x_i y_i$
       *   **解释**：最基础的线性相似度。其值受向量模长影响很大。
       *   **与余弦相似度的关系**：**余弦相似度是归一化后的点积**，$cosine(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{||\mathbf{x}|| \cdot ||\mathbf{y}||}$。点积更关注强度和方向，而余弦相似度只关注方向。

**总结与选择**：
*   **关注方向而非强度**（如文本相似度）：首选 **余弦相似度**。
*   **关注绝对距离和强度**（如空间聚类）：使用 **欧氏距离**。
*   **数据存在相关性/量纲不一**：使用 **马氏距离**。
*   **衡量线性相关趋势**：使用 **皮尔逊相关系数**。
*   **处理集合或二元数据**：使用 **Jaccard 系数**。

---

#### 二、了解对比学习嘛？

**1. 核心思想**
对比学习（Contrastive Learning）是自监督学习的一种范式，其核心思想非常直观：**通过拉近相似样本（正样本对）的表征，推远不相似样本（负样本对）的表征，来学习高质量的数据表示。**

**一个生动的比喻**：教一个模型识别“猫”。
*   你不需要告诉它“猫有尖耳朵、胡须、尾巴”。
*   你只需要给它看很多图片，并告诉它：**“这两张是同一只猫的不同角度（正样本对），那两张一张是猫一张是狗（负样本对）”**。
*   模型通过无数次这样的对比，自己就能学会捕捉“猫”的本质特征。

**2. 关键组成部分**
一个典型的对比学习框架（如SimCLR）包含以下几个部分：
*   **数据增强（Data Augmentation）**：这是构建正样本对的关键。对同一张原始图片 $\mathbf{x}$ 进行两次随机增强（如裁剪、颜色抖动、高斯模糊等），得到两个不同的视图 $\mathbf{x}_i$ 和 $\mathbf{x}_j$，它们构成一个**正样本对**。
*   **编码器（Encoder）**：通常是一个神经网络（如ResNet），用于将增强后的图像映射到低维表示空间，$\mathbf{h} = f(\mathbf{x})$。
*   **投影头（Projection Head）**：一个小型MLP，将编码器的输出 $\mathbf{h}$ 进一步映射到一个更适用于对比学习的空间 $\mathbf{z} = g(\mathbf{h})$。在这个空间里计算相似度。
*   **对比损失函数（Contrastive Loss）**：最常用的是 **NT-Xent（Normalized Temperature-Scaled Cross Entropy Loss）**，它鼓励正样本对的相似度远高于与所有负样本对的相似度。

**3. 损失函数（以InfoNCE为例）**
对于一个包含 $N$ 个样本的批次，通过增强可以得到 $2N$ 个样本。对于一对正样本 $(i, j)$，其损失函数为：
$$\mathcal{L}_{i,j} = -\log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)}$$
其中：
*   $\text{sim}(\cdot)$ 是相似度函数，通常用余弦相似度。
*   $\tau$ 是一个温度参数，用于调节分布的尖锐程度。
*   分母是对所有样本的求和，但排除了 $k=i$ 自身。对于样本 $i$，$j$ 是它唯一的正样本，批次中其他 $2(N-1)$ 个样本都是它的**负样本**。

**4. 应用与价值**
*   **自监督预训练**：在海量无标签数据上通过对比学习预训练一个强大的特征编码器。
*   **迁移学习**：将预训练好的编码器冻结，后面接一个简单的分类器，用少量有标签数据进行微调，即可在下游任务（如图像分类、目标检测）上取得优异性能。
*   **核心价值**：**极大地减少了对昂贵人工标注数据的依赖**，利用海量无标注数据学习通用表征。

---

#### 三、对比学习负样本是否重要？负样本构造成本过高应该怎么解决？

**1. 负样本是否重要？**
**极其重要。** 负样本是对比学习的基石之一。

*   **作用**：负样本提供了“什么是不同”的边界信息。如果没有负样本，模型只会学习到一个平凡解：把所有表征都映射到同一个点，这样正样本对之间的相似度自然就最高了，但这毫无意义。负样本的存在**阻止了模型坍塌（Collapse）**，迫使模型学习到有区分性的特征。
*   **数量**：理论和使用经验表明，**负样本的数量越多，对比学习的效果通常越好**。更多的负样本提供了更丰富的“对比背景”，让模型能够学到更精细、更鲁棒的区分边界。大批次训练是SimCLR等模型成功的关键。

**2. 负样本构造成本过高应该怎么解决？**
使用大批次意味着需要极大的显存和计算成本，这限制了对比学习的应用。研究者们提出了多种巧妙的解决方案：

**a) 使用动量编码器（Momentum Encoder）**
*   **代表工作**：**MoCo（Momentum Contrast）**
*   **思想**：维护一个**动态的字典**来存储大量负样本的表示，而不是依赖当前批次。
*   **如何实现**：
    1.  使用两个编码器：一个在线编码器（参数通过梯度更新）和一个动量编码器（参数是在线编码器的移动平均：$\theta_k \leftarrow m \theta_k + (1-m) \theta_q$）。
    2.  当前批次的正样本对通过在线编码器得到查询表示 $\mathbf{q}$。
    3.  负样本来自一个**队列（Queue）**，这个队列存储了之前若干批次通过动量编码器得到的键表示 $\mathbf{k}$。队列遵循FIFO（先进先出）原则，不断用新的批次更新。
*   **优势**：
    *   队列可以远大于物理批次大小（如65536），提供了大量且一致的负样本。
    *   动量更新使得键编码器的变化是平滑的，保证了队列中负样本表征的一致性，不会因为编码器的快速变化而失效。

**b) 不使用负样本：非对比方法**
这类方法完全摒弃了显式的负样本，通过其他机制来避免模型坍塌。

*   **代表工作**：**BYOL（Bootstrap Your Own Latent）**
*   **思想**：**自己和自己学**。它只需要正样本对。其中一个视图（ online branch ）通过预测另一个视图（ target branch ）的表征来学习。Target branch 的参数是 online branch 的动量移动平均。
*   **如何避免坍塌**：其避免坍塌的机理非常微妙，最初被认为是因为BN的存在，后续研究有不同观点。但实践表明，即使移除BN，通过其他技巧（如L2归一化、可学习参数等）也能工作。它的成功表明，**负样本并非绝对必需，但需要极其精巧的设计来隐式地提供“不一致性”**。

*   **代表工作**：**SimSiam（Simple Siamese）**
*   **思想**：进一步简化，连动量编码器都不要了。两个分支共享编码器权重。它通过一个**预测头（MLP）** 和 **停止梯度（Stop-Gradient）** 操作来避免坍塌。
*   **核心操作**：对于一个分支，计算损失；对于另一个分支，将其输出视为常数（ detached ，阻止梯度回传）。这相当于让一个视图去预测另一个视图的“静态目标”，这个目标在不断变化，从而防止了模型走捷径坍缩。

**c) 其他技巧**
*   **困难负样本挖掘（Hard Negative Mining）**：不是随机选择负样本，而是选择那些与锚点相似度较高（但实际不是正样本）的“困难”负样本。这些样本对模型来说更难区分，能提供更大的信息量，让学习更高效。

**面试回答总结**：
首先强调负样本**至关重要**，它定义了学习的边界并防止坍塌。然后阐述大批次负样本的成本问题，并重点介绍两种主流解决方案：
1.  **MoCo的路径**：承认负样本的重要性，通过**动量编码器+队列**的机制来低成本地获得大量负样本。
2.  **BYOL/SimSiam的路径**：另辟蹊径，通过**停止梯度、动量平均、预测头**等精巧设计，完全**摒弃显式负样本**，实现更简洁高效的学习。

这表明了对比学习领域从依赖大量负样本到探索更高效、更简洁方法的演进历程。
