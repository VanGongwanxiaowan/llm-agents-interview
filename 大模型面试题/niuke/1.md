好的，我们来逐一详细解答这些问题。

---

### 3. DAPO的改进有哪些？（见P2）

**DAPO** 通常指的是 **Diffusion Model Alignment via Preference Optimization**，它是用于对齐扩散模型（如文生图模型）的一种偏好优化方法，类似于大语言模型中的DPO。其核心改进和特点如下：

1.  **无需训练奖励模型：**
    *   **传统方法：** 传统的RLHF（如PPO）需要先训练一个复杂的奖励模型来评估生成图像的人类偏好，这个过程成本高且不稳定。
    *   **DAPO改进：** DAPO直接利用成对的偏好数据（一张优选图像 `y_w` 和一张劣选图像 `y_l`），绕过了对独立奖励模型的依赖，简化了训练流程并降低了成本。

2.  **稳定的直接偏好优化：**
    *   DAPO的损失函数设计受到了语言模型DPO的启发，但其针对扩散过程进行了适配。其核心思想是通过一个闭式解，将奖励最大化问题转换为一个简单的监督学习问题。
    *   损失函数会**拉高** 模型生成**优选图像** `y_w` 的概率，同时**压低** 生成**劣选图像** `y_l` 的概率。公式上，它通过对比在相同提示词下模型对 `y_w` 和 `y_l` 的似然概率来实现这一目标。

3.  **更高效的对齐：**
    *   由于避免了在强化学习框架中进行高方差的策略梯度计算（如PPO），DAPO的训练过程**更加稳定和高效**，更容易收敛。
    *   它能够直接将人类的主观偏好（如审美、图像质量、指令遵循能力）注入到扩散模型中，使生成的图像更符合人类的期望。

**总结：** DAPO的主要改进在于**省去了奖励模型的训练步骤**，并提供了一个**更稳定、更高效** 的基于偏好数据的端到端优化方法，从而实现了扩散模型与人类偏好的更好对齐。

---

### 4. LoRA原理？（又问了数学原理，照样说的模模糊糊）

**LoRA** 的全称是 **Low-Rank Adaptation**（低秩适应），是一种用于高效微调大模型（如LLMs）的技术。

#### 核心思想：
神经网络的权重更新在适应特定任务时具有较低的“内在秩”。这意味着，一个完整的权重更新矩阵可以用两个更小（低秩）的矩阵的乘积来近似表示，从而大幅减少需要训练的参数数量。

#### 数学原理：
1.  **常规微调：** 对于预训练权重矩阵 `W₀` (维度为 `d x k`)，微调时会更新整个矩阵：`W = W₀ + ΔW`。
2.  **LoRA的改进：** LoRA并不直接更新 `W₀`，而是冻结它。它引入一个低秩分解来表示权重更新 `ΔW`：
    *   `ΔW = B * A`
    *   其中，`A` 是一个 `r x k` 的矩阵（随机高斯初始化），`B` 是一个 `d x r` 的矩阵（初始化为0）。`r` 是远小于 `d` 和 `k` 的秩（rank）。
3.  **前向传播：** 在微调过程中，模型的前向计算变为：
    *   `h = W₀ * x + ΔW * x = W₀ * x + B * A * x`
    *   原始权重 `W₀` 保持不变，只训练低秩矩阵 `A` 和 `B`。
4.  **推理：** 训练完成后，可以将低秩更新合并回原始权重中：`W' = W₀ + B * A`。这样推理时不会引入任何额外的计算开销，和原始模型完全一样。也可以不合并，但需要额外计算 `B*A*x`。

#### 优势：
*   **大幅减少参数量：** 可训练参数从 `d * k` 减少到 `(d + k) * r`。例如，`r=8` 时，参数量减少可达百倍甚至千倍。
*   **降低硬件门槛：** 只需训练少量参数，大大降低了显存需求，使得在消费级GPU上微调大模型成为可能。
*   **高效切换任务：** 只需切换不同的 `A` 和 `B` 适配器，就可以在同一个基础模型上快速切换不同的微调任务，无需保存多份完整的模型副本。
*   **无推理延迟：** 合并后与原始模型性能完全一致。

---

### 5. 推理分为哪两个阶段？（prefill和decode两个阶段）

在大语言模型（LLM）的推理过程中，尤其是处理长序列或提供流式响应时，通常分为两个 distinct 的阶段：

1.  **Prefill 阶段（预填充阶段）：**
    *   **任务：** 处理用户输入的整个**提示（Prompt）**。
    *   **过程：** 模型并行地计算提示中所有 token 的 Key 和 Value 向量，并将它们存储在 **KV Cache** 中。同时，生成第一个输出 token 的分布。
    *   **特点：** 此阶段计算量大，耗时与**提示长度**的平方相关（因为要计算整个序列的Self-Attention），但因为是并行处理，所以相对高效。

2.  **Decode 阶段（解码阶段/生成阶段）：**
    *   **任务：** 逐个 token 地生成回复内容。
    *   **过程：** 利用 Prefill 阶段准备好的 KV Cache，模型每次只处理**最新生成的那个 token**，计算其 Query 向量与 KV Cache 中所有 Key 向量的注意力，从而预测下一个 token。新生成的 token 又会更新到 KV Cache 中，为下一步生成做准备。
    *   **特点：** 此阶段计算量小，每次只处理一个 token，但其耗时与**已生成的序列长度**线性相关（因为注意力计算需要遍历Cache中所有Key），并且是串行过程，无法并行化，因此是推理速度的主要瓶颈。通常所说的“吞吐量”主要受此阶段影响。

---

### 6. Self Attention怎么做的？（qkv含义+计算流程）

Self-Attention（自注意力）机制是Transformer架构的核心。

#### Q, K, V 的含义：
对于输入序列中的每一个词（或token），它都会被线性投影到三个不同的向量空间：
*   **Query：** “查询”向量。代表当前词，用于“询问”或“查询”其他词与自己的相关程度。
*   **Key：** “键”向量。代表所有词（包括自己），用于“被查询”。它用来与Query进行匹配，计算相似度得分。
*   **Value：** “值”向量。代表所有词所包含的实际信息。最终的输出是基于注意力权重对Value的加权求和。

#### 计算流程：
1.  **线性投影：** 输入序列 `X` 分别乘上三个权重矩阵 `W^Q`, `W^K`, `W^V`，得到 Query、Key、Value 矩阵：
    *   `Q = X * W^Q`
    *   `K = X * W^K`
    *   `V = X * W^V`

2.  **计算注意力分数：** 计算 Query 和所有 Key 的点积，得到原始注意力分数。分数越高，表示两个位置越相关。
    *   `Scores = Q * K^T`

3.  **缩放：** 将分数除以 `√(d_k)`（`d_k` 是 Key 向量的维度）。这一步是为了防止点积结果过大，导致Softmax函数的梯度消失。
    *   `Scaled_Scores = Scores / √(d_k)`

4.  **应用Softmax：** 对缩放后的分数应用Softmax函数，将其转换为概率分布（所有权重为正数且和为1），即**注意力权重**。
    *   `Attention_Weights = Softmax(Scaled_Scores)`

5.  **加权求和：** 将注意力权重作为系数，对 Value 矩阵进行加权求和，得到最终的 Self-Attention 输出。
    *   `Output = Attention_Weights * V`

**最终，输出矩阵中的每一行都是对应输入token基于整个输入序列上下文的新表示。**

---

### 7. 预训练模式是啥？和SFT的区别？

#### 预训练模式：
*   **目标：** 让模型学习语言的通用知识、语法、事实和推理能力。旨在构建一个强大的“基础模型”。
*   **数据：** 使用海量的、无标注的原始文本数据（如网页、书籍、代码等）。
*   **训练任务：** **Next Token Prediction（下一个词预测）**。给定前文的一系列token，模型被训练来预测序列中下一个最可能出现的token。这是一个**自监督**学习过程。
*   **Loss计算：** 计算**所有位置**上预测结果和真实下一个token之间的交叉熵损失。

#### SFT（监督微调）：
*   **目标：** 让预训练好的基础模型学会遵循指令、执行特定任务（如对话、总结、翻译等）或符合某种输出风格。
*   **数据：** 使用规模小但质量高的**标注数据**，通常是人工精心编写的（指令，期望回复）配对数据。
*   **训练任务：** 给定指令（Prompt），训练模型生成对应的期望回复（Response）。
*   **Loss计算（关键区别）：** 通常**只计算回复部分（Response）** 的token的损失，而**不计算指令部分（Prompt）** 的损失。模型的任务是学会如何生成正确的回复，而不是去学习或记忆指令本身。

#### 核心区别总结：
| 特性 | 预训练 | SFT |
| :--- | :--- | :--- |
| **目标** | 获得通用语言能力 | 获得执行指令的能力 |
| **数据** | 海量无标注文本 | 高质量指令-回复对 |
| **任务** | 下一个词预测 | 条件文本生成（给定指令，生成回复） |
| **Loss计算** | **计算所有token**的损失 | **通常只计算回复部分**的损失，忽略提示部分的损失 |
