以下是对你提供的“顺丰技术一面”面试题目的详细回答，每个问题都先抄写原题，再给出完整回答：

---

## 1. 介绍科研项目一的内容  

**题目原文：**  
- 项目背景？具体是什么图片？描述是怎样的 举例？  
- 如何引导模型去描述位置？在微调之前模型是否具备对这个场景caption的能力吗？  
- 数据构造？  
- 为什么选用Qwen2v17B？微调方法？lora的rank？如何评估模型微调的效果？  
- 医学的诊断报告有额外的要求，比如幻觉、准确性比较高，是如何额外处理的？  

**回答：**  
该项目是一个面向医学影像的视觉语言模型（VLM）应用项目，旨在生成对医学图像（如X光片、CT扫描图像）的准确、结构化描述，辅助医生撰写诊断报告。  

- **项目背景与图像类型**：项目聚焦于胸部X光片和腹部CT图像。例如，一张胸部X光片可能包含肺纹理增粗、心影增大等特征，模型需要输出如：“双侧肺野清晰，心影大小形态正常，纵隔无移位。”  
- **引导描述位置的方法**：在训练数据中，我们引入了位置编码提示，如使用“左上肺野”、“右下腹”等区域标签，并在指令中明确要求模型按解剖结构顺序描述。微调前，基础模型（如Qwen2-VL）已具备一定的通用图像理解能力，但对医学特定位置描述能力较弱，容易出现描述模糊或错误定位的问题。  
- **数据构造**：使用了公开医学影像数据集（如MIMIC-CXR、NIH ChestX-ray），并邀请放射科医生对图像进行标注，生成结构化报告文本。数据增强包括对图像进行旋转、裁剪，文本部分进行同义替换以增强多样性。  
- **模型选型与微调**：选择Qwen2-VL-7B因其在多模态理解上的较强表现和较好的中英文支持。微调采用LoRA（Low-Rank Adaptation）方法，rank设置为8，在4张A100上训练。评估方式除了使用BLEU、CIDEr等自动指标外，还进行了医生人工评估，重点考察描述的准确性和临床合理性。  
- **处理幻觉与准确性**：在训练中引入“负样本”（如错误描述）进行对比学习，并在推理阶段使用约束解码策略，限制模型输出与医学知识库冲突的内容。此外，引入后处理规则，对关键术语进行校验。

---

## 2. Qwen2.5vI有哪些改进？  

**题目原文：**  
Qwen2.5vI有哪些改进？  

**回答：**  
Qwen2.5-VL相比Qwen2-VL主要改进包括：  
- **视觉编码器增强**：使用更高分辨率的图像输入（如1120×1120），提升细粒度视觉特征提取能力。  
- **语言模型升级**：基座语言模型升级至Qwen2.5系列，推理能力和上下文理解更强。  
- **多模态对齐优化**：通过更大量的视觉-语言对齐数据训练，改善图文理解与生成的连贯性。  
- **支持更长上下文**：部分版本支持128K上下文长度，适用于长文档或多图推理任务。  
- **推理速度提升**：优化了模型结构与推理流程，降低延迟。

---

## 3. 现在类似qwen的多模态大模型在结构上还是在其他什么上的改变比较多？强化学习是否了解？  

**题目原文：**  
现在类似qwen的多模态大模型在结构上还是在其他什么上的改变比较多？强化学习是否了解？  

**回答：**  
当前多模态大模型的改进主要集中在以下几个方面：  
- **结构上**：  
  - **统一Transformer架构**：越来越多模型采用纯Transformer统一处理图像、文本、视频等模态。  
  - **交叉注意力机制优化**：如Perceiver-Resampler、Q-Former等模块，增强模态间交互。  
  - **模块化设计**：如LLaVA-Next、InternVL等引入可插拔的视觉编码器与投影层。  
- **非结构改进**：  
  - **数据质量与规模**：使用更高质量、多来源的图文对进行训练。  
  - **训练策略**：如多阶段训练（预训练→指令微调→RLHF）。  
  - **推理优化**：模型量化、蒸馏、 speculative decoding 等。  

**强化学习了解情况**：了解强化学习在大模型中的应用，尤其是RLHF（Reinforcement Learning from Human Feedback）和DPO（Direct Preference Optimization），用于对齐人类偏好，减少有害输出或提升回答质量。

---

## 4. 强化学习PPO算法介绍  

**题目原文：**  
强化学习PPO算法介绍  

**回答：**  
PPO（Proximal Policy Optimization）是一种基于策略梯度的强化学习算法，由OpenAI于2017年提出，广泛应用于大模型RLHF阶段。其核心思想是**在更新策略时避免过大步长，确保训练稳定性**。  

**主要特点**：  
- **裁剪目标函数**：通过限制新旧策略之间的概率比，防止更新幅度过大。  
- **优势函数估计**：使用GAE（Generalized Advantage Estimation）来估计当前动作的优劣。  
- **交替优化**：交替进行采样和优化，提升样本效率。  

**公式简要表示**：  
\[
L^{CLIP}(\theta) = \mathbb{E} \left[ \min\left( r(\theta) A, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon) A \right) \right]
\]  
其中 \( r(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \)，\( A \) 是优势函数，\( \epsilon \) 是裁剪参数（通常0.1~0.2）。

---

## 附注  
你在面试中提到“脑子抽了讲RLHF去了”，说明你可能在PPO问题上联想到了RLHF的应用场景。这其实是可以理解的，因为PPO确实是RLHF中的核心算法之一，只需注意在回答时先明确PPO本身定义，再自然过渡到RLHF即可。

如果需要我帮你模拟更多面试问题或整理答题模板，可以继续告诉我。
