好的，这是一份针对这份百度大模型面经的详细解答，我会以通俗易懂的方式先抄原题再进行讲解。

***

### **1. 自我介绍 & 2. 项目介绍 + 拷打**

这两部分属于个性化内容，无法提供标准答案，但可以给出回答策略。

*   **自我介绍**：应简洁明了，突出与岗位最相关的技能和经验。模板：`我是XXX，来自XXX学校/公司，有XXX年的NLP/大模型相关经验。我主导或参与了XXX项目，该项目主要目标是XXX，我负责了XXX部分，使用了XXX技术，取得了XXX成果。我对大模型训练、微调、应用等方向非常感兴趣，希望能在百度深入发展。`
*   **项目介绍 + 拷打**：面试官质疑准确率和召回效果是非常常见的压力测试，旨在考察你对自己项目的理解深度、评估方法的科学性以及解决问题的思路。
    *   **应对策略**：
        1.  **承认不足**：首先承认当前指标并非完美，有提升空间，这表明你有清醒的认知。
        2.  **解释原因**：分析指标不高的可能原因。例如：“我们的项目目前准确率是85%，分析bad cases发现主要误差来源于XXX领域的数据匮乏”或“召回率偏低主要是因为检索模块在面对长尾问题时，向量化表示不够精准”。
        3.  **阐述解决方案**：提出你想到的或已经计划要做的改进方案。例如：“针对这个问题，我们下一步计划通过人工标注一批高质量数据注入训练集”或“我们考虑引入一个重排序（Re-ranker）模型来对检索结果进行精细化排序，以期提升召回率”。
        4.  **展示科学评估**：强调你的评估方法是科学的。“我们不仅看整体准确率，还按照问题难度和类型划分了多个测试集，目前模型在常见问题上的准确率已经达到了92%”。这能体现你的专业性。

### **3. 八股文**

#### **1. deepseek（只说上了 grpo、mla，追问了其他没答上）**

*   **原题**：deepseek（只说上了 grpo、mla，追问了其他没答上）
*   **解答**：
    *   **DeepSeek**：是深度求索公司推出的一系列大模型，如 DeepSeek-V2, DeepSeek-Coder等。面试官这里可能不是问模型本身，而是问你是否了解他们公司**训练技术**或**模型结构**上的创新。
    *   **GRPO**：**Group Relative Policy Optimization**。一种先进的强化学习（RL）优化算法，是PPO的改进版。它通过**分组（Group）** 的方式来更稳定、高效地优化模型策略，相比PPO能取得更好的效果。
    *   **MLA**：**Multi-head Latent Attention**。这是 **DeepSeek-V2** 模型的核心创新架构。它是一种高效的混合专家（MoE）模型。
        *   **传统MoE**：每层有多个专家（FFN），每个token会激活Top-K个专家，但**所有专家都需要加载到显存**中，显存占用大。
        *   **MLA**：将专家们**存储在内存（CPU）或硬盘（SSD）上**，而非显存。通过一种特殊的注意力机制（Latent Attention），只将当前需要激活的**少量专家动态地、快速地**加载到显存中。这极大地**减少了显存占用**，使得可以用极低的成本训练和推理超大规模模型（如DeepSeek-V2规模达236B，但激活参数量仅21B）。
    *   **其他可能追问的点**：**DeepSeek-MoE**（早期的MoE架构）、**FlashAttention**（加速计算）、**条件计算**等。

#### **2. MHA、MQA、GQA**

*   **原题**：MHA、MQA、GQA
*   **解答**：这三者是Transformer中注意力机制的不同变体，主要在**key（K）和value（V）矩阵的共享策略**上有所不同。
    *   **MHA (Multi-Head Attention)**：**多头注意力**，标准方案。
        *   **工作原理**：每个头都有**自己独立的** K、V、Q 权重矩阵。不同头可以关注不同方面的信息。
        *   **优点**：模型容量大，表现力强。
        *   **缺点**：**KV缓存占用显存大**，推理生成（Decoder）时速度慢。
        *   **图示**：`[H1-K, H1-V], [H2-K, H2-V], ... [Hn-K, Hn-V]`
    *   **MQA (Multi-Query Attention)**：**多查询注意力**。
        *   **工作原理**：**所有头共享同一套 K 和 V 矩阵**。每个头只有自己的 Q 矩阵是独立的。
        *   **优点**：极大地**减少了KV缓存**，**推理速度非常快**。
        *   **缺点**：因为KV共享，模型容量和表现力可能下降，可能导致质量轻微损失。
        *   **图示**：`[H1-Q], [H2-Q], ... [Hn-Q]` -> `[Shared-K, Shared-V]`
    *   **GQA (Grouped-Query Attention)**：**分组查询注意力**，是MHA和MQA的折中方案。
        *   **工作原理**：将头分成 G 个组，**组内共享一套 K 和 V 矩阵**。组间不共享。例如，8个头分成2组，那么就有2套不同的KV矩阵。
        *   **优点**：在**推理速度**和**模型质量**之间取得了很好的平衡。用比MQA稍大的KV缓存代价，换取了比MQA更好的模型性能。
        *   **图示**：`[G1-H1-Q, G1-H2-Q], [G2-H1-Q, G2-H2-Q]` -> `[G1-Shared-K, G1-Shared-V], [G2-Shared-K, G2-Shared-V]`
    *   **总结**：**MHA重质量，MQA重速度，GQA是平衡之道**。目前很多最新模型（如LLaMA-2/3）都采用了GQA。

#### **3. Deepspeed（只说上了 zero1、zero2、zero3），问是否了解混合精度（很具体，哪些部分用什么精度）除了这些还有什么减少显存占用的办法（没答上，和我说用 llamafactory 训练肯定见过这个参数）**

*   **原题**：Deepspeed（只说上了 zero1、zero2、zero3），问是否了解混合精度（很具体，哪些部分用什么精度）除了这些还有什么减少显存占用的办法（没答上，和我说用 llamafactory 训练肯定见过这个参数）
*   **解答**：
    *   **DeepSpeed ZeRO (Zero Redundancy Optimizer)**：
        *   **ZeRO-Stage1**：优化器状态分片。每个GPU只存储和更新**1/N**的优化器状态（如动量、方差），减少显存。
        *   **ZeRO-Stage2**：梯度分片。在Stage1基础上，每个GPU只存储**1/N**的梯度。
        *   **ZeRO-Stage3**：参数分片。在Stage2基础上，每个GPU只存储**1/N**的模型参数。这是最节省显存的模式，但通信开销最大。
    *   **混合精度 (Mixed Precision)**：
        *   **核心思想**：在训练中同时使用**FP16（半精度）** 和 **FP32（单精度）** 两种数据类型，兼顾速度和稳定性。
        *   **FP16**：用于：
            1.  存储**模型参数**的副本（前向传播时使用）。
            2.  存储**梯度**。
            3.  进行**前向传播**和**反向传播**的计算。
            *   *优点：速度快，显存占用减半。*
        *   **FP32**：用于：
            1.  存储**模型主参数**（Master Weights），用于参数更新。
            2.  存储**优化器状态**（如动量）。
            3.  进行**梯度更新**（优化器step）。
            *   *优点：数值精度高，防止下溢（梯度值太小变成0）。*
        *   **工作流**：`FP32参数 -> 转为FP16进行前向计算 -> 得到FP16损失 -> 反向传播得到FP16梯度 -> 转为FP32梯度 -> FP32优化器更新FP32主参数`
    *   **其他减少显存占用的办法**：
        *   **梯度检查点 (Gradient Checkpointing)**：这就是面试官提到的`llamafactory`里的参数（`--gradient_checkpointing`）。它用**时间换空间**。在前向传播时**不保存全部中间激活值**，而是在反向传播需要时**重新计算**一部分中间结果。这可以显著减少显存占用（可能减少20%-30%），但会使训练时间增加约25%。
        *   **Flash Attention**：一种高效的Attention计算算法，通过分块计算和IO感知的优化，减少中间激活值的显存占用，同时还能**加快计算速度**。
        *   **LoRA**：一种参数高效的微调方法，冻结原模型参数，只训练新增的低秩矩阵，极大减少显存需求。

#### **4. ppo、grpo、dpo**

*   **原题**：ppo、grpo、dpo
*   **解答**：这些都是用于对齐（Alignment）和大模型微调的强化学习（RL）算法。
    *   **PPO (Proximal Policy Optimization)**：**近端策略优化**，是RLHF（人类反馈强化学习）中传统的策略优化算法。
        *   **工作原理**：通过限制策略更新的步长（“近端”），避免新策略相对旧策略偏离太远，从而保证训练的稳定性。它需要训练一个奖励模型（Reward Model, RM）来提供反馈信号。
        *   **流程**：`SFT模型 -> 训练奖励模型(RM) -> PPO用RM的奖励信号优化SFT模型`。
    *   **DPO (Direct Preference Optimization)**：**直接偏好优化**。
        *   **工作原理**：一个**无需训练奖励模型**的替代方案。它直接利用**人类偏好数据**（即一对回答，一个更好一个更差）来优化策略。其数学推导非常巧妙，将奖励函数隐含地表示了出来。
        *   **优点**：比PPO更稳定，更简单，不需要额外训练RM，计算成本更低。
        *   **缺点**：严重依赖偏好数据的质量。
    *   **GRPO (Group Relative Policy Optimization)**：**分组相对策略优化**，可以看作是PPO的一个增强版。
        *   **工作原理**：它将一批样本分成多个组（Group），在组内进行**相对比较**（类似于DPO的思想），而不仅仅是像PPO那样对每个样本进行绝对分数的奖励。这种相对比较能提供更丰富的优化信号，使得训练更高效、更稳定。

#### **5. lora的显存占用计算，全参的显存占用计算，项目里微调用到了多少显存**

*   **原题**：lora的显存占用计算，全参的显存占用计算，项目里微调用到了多少显存
*   **解答**：
    *   **全参微调 (Full Fine-tuning) 显存占用**：主要包括四部分：
        1.  **模型参数**：`参数量 * 4字节（FP32）` 或 `参数量 * 2字节（FP16/BF16）`。
        2.  **梯度**：与参数同精度，`参数量 * 4字节（FP32）`。
        3.  **优化器状态**：
            *   **Adam优化器**：需要存储**动量（momentum）** 和**方差（variance）** 两个状态。如果是FP32训练，每个参数需要 `4 + 4 = 8` 字节。如果是混合精度，动量方差用FP32，模型参数用FP16，则每个参数需要 `4 + 4 + 2 = 10` 字节。
        4.  **前向激活值**：中间计算结果，与模型结构、批次大小、序列长度有关，很难精确估算。通常用**梯度检查点**来大幅减少这部分占用。
        *   **粗略估算公式（混合精度）**：`显存占用 ≈ 参数量 * (2 + 8 + 10) bytes`。对于一个7B模型：`7e9 * 20 bytes ≈ 140 GB`。这解释了为什么全微调需要多张高端显卡。
    *   **LoRA微调显存占用**：主要包括：
        1.  **冻结的原模型参数**：以FP16存储，`参数量 * 2字节`。这部分在推理时占用显存，但**训练时反向传播不计算其梯度**，所以不占用优化器状态和梯度的显存。
        2.  **可训练的LoRA参数**：主要是两个低秩矩阵 `A` 和 `B`。参数量为 `2 * rank * (d_model * d_model)`。假设只对QKV投影层加LoRA，则参数量更少。
        3.  **LoRA参数的梯度+优化器状态**：因为可训练参数量极少，这部分开销很小。
        *   **粗略估算**：LoRA显存占用**略大于**原模型推理的显存占用。对于7B模型，推理需`14GB`，LoRA训练可能只需`16-20GB`，一张消费级显卡（如3090/4090）即可胜任。
    *   **项目里用到的显存**：这是一个需要你根据自己项目情况回答的问题。例如：“我们项目使用7B模型，采用LoRA（rank=64）进行微调，在批次大小为4、序列长度为1024的情况下，单卡A100（40GB）的显存占用大约在22GB左右。”

### **4. 业务题**

#### **1. dpo 数据量不够怎么构造？**

*   **原题**：dpo 数据量不够怎么构造？
*   **解答**：DPO严重依赖高质量的偏好对数据 `<chosen, rejected>`。如果人工标注数据不足，可以考虑以下方法：
    1.  **使用强模型生成**：使用GPT-4等更强的模型作为“裁判”，对同一个提示词（prompt）生成多个回答，然后让强模型对这些回答进行排序或打分，自动构造出偏好对。例如，生成4个回答，让GPT-4选出最好和最差的，组成一对。
    2.  **规则法生成**：针对特定错误类型，通过规则自动构造。例如，对于事实性问题，可以从知识库中找出正确答案作为`chosen`，然后对答案进行一些篡改（如替换实体、颠倒关系）作为`rejected`。
    3.  **数据增强**：对已有的少量优质偏好对进行扩充。例如，对`prompt`进行 paraphrasing（复述），生成语义相同但表述不同的新提示词。
    4.  **半监督学习**：先用少量数据训练一个初版的DPO模型，然后用这个模型对大量无标签的`(prompt, response)`数据进行打分，筛选出其中置信度高的、得分差异大的样本，作为新的偏好数据加入训练集，迭代训练。

#### **2. 数据质量不好不使用人工怎么改善？**

*   **原题**：数据质量不好不使用人工怎么改善？
*   **解答**：完全不用人工很难，但可以尽量减少对人工的依赖：
    1.  **基于规则/模型的自动过滤**：
        *   **长度过滤**：过滤掉过长或过短的文本。
        *   **关键词/正则过滤**：过滤包含脏话、敏感词、隐私信息的文本。
        *   **语言质量模型**：训练一个分类器来判断语法是否通顺、语言是否流畅。
        *   **困惑度（Perplexity）过滤**：用一个预训练的语言模型计算文本的困惑度，过滤掉困惑度异常高的样本（可能是乱码或低质量文本）。
    2.  **一致性清洗**：
        *   **去重**：对文档级、段落级、句子级进行去重，移除重复数据。
        *   **嵌入过滤**：计算文本的嵌入向量，移除那些与高质量数据集群偏离太远的异常点。
    3.  **利用模型自净化**：
        *   **自训练（Self-training）**：先用高质量数据训练一个初始模型，然后用这个模型为大量无标签数据打上伪标签（Pseudo-label），并选择高置信度的样本加入训练集。
        *   **合成数据**：用已有的高质量数据作为范例，让大模型（如GPT-4）模仿其风格和质量生成新的合成数据。

### **5. 代码题**

*   **原题**：一个路径转换题，非leetcode
*   **解答**：由于没有具体题目，无法给出答案。但路径转换类问题通常考察**字符串处理**和**栈**的应用。常见考点是简化类似Unix的文件路径（如：将 `/a/./b/../../c/` 简化为 `/c`）。
    *   **核心思路**：通常使用栈（Stack）来模拟进入和退出目录的过程。
        1.  按`/`分割路径字符串，得到一个个部分（parts）。
        2.  遍历这些部分：
            *   如果是空字符串或`.`，跳过。
            *   如果是`..`，弹出栈顶元素（如果栈不为空），表示返回上一级目录。
            *   如果是其他字符串，将其压入栈中，表示进入该目录。
        3.  最后用`/`连接栈中的元素，并在开头加上`/`，形成简化后的绝对路径。
    *   **考察点**：边界条件处理（空输入、根目录、多个连续斜杠）、逻辑清晰、代码简洁。
