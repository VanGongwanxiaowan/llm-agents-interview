DeepResearch Agent 有一个很大的问题就是多次的搜索阅读很容易就把上下文窗口用光了，常规的做法是像 Claude Code 一样，超过阈值就触发记忆压缩。通义的论文《ReSum》提出了一种在 RL 中让模型学会更好地利用压缩内容的方法。

这个方法我们之前也考虑过，但这样做在强化学习的时候会有一个问题：一旦触发记忆压缩，整个历史记录都会变成压缩后的内容，此时模型就只能看到压缩后的 token，压缩前的就丢掉了，此时模型就学不到压缩前的动作了。我们当时没想到好的解法，而 ReSum 提出一种可行的方案：把压缩前和压缩后的轨迹分成两条分别给奖励。

举个例子：

正常的轨迹是这样的：“用户查询 → AI 助手 → 工具调用 → AI 助手 →... → AI 助手 → 答案”

加入了 summary 工具之后，当轨迹接近上下文窗口的时候，系统就会触发总结。

接近上下文窗口长度的轨迹 A：“用户查询 → AI 助手 → 工具调用 → AI 助手 →... → AI 助手 → summary”

新的轨迹 B：“用户查询 + 摘要 → AI 助手 → 工具调用 → AI 助手 → 答案”

关键点来了，当 B 答对时，B 的奖励会复制给 A。为什么要这样做？

尽管 A 没有直接得出答案，但它找到了一个有用的摘要，最终导向了正确的答案，所以 A 中的所有动作也得到了正向的激励。这样模型能通过 A 学会收集能够产生优质摘要的关键信息。而模型则通过 B 学会了利用摘要信息来高效地完成任务。这就是一箭双雕。



<img width="1199" height="408" alt="image" src="https://github.com/user-attachments/assets/b9845358-85b1-4c4b-b39c-f21d32634783" />
