Thinking模型的本质是Test time scaling（TTS）, TTS是一种对人类认知 "慢速深思"的 system 2的模拟。

links：

https://open.spotify.com/episode/1yuSRv4cOgbJyOQpJYQB7h?si=wtEiAdBbRiS8kf1blIf_CQ


「LLM, Reasoning」论文

Reinforcement Learning for Reasoning in Large Language Models with One Training Example

心有灵犀一点通，只用1 个训练样本去打动模型，让LLM的reasoning能力巨大提升。

先看结果，在 Qwen-2.5-Math-1.5 B 上，作者只用一道可验证的数学题做 RLVR ，MATH500 正确率从 36 % 激增到 73.6 %。

如何做到？关键两点：
1: 训练样本选择，浓缩单位最大信息量
2: Reward设计，用RLVR引爆LLM潜能

首先，作者如何挑出这道典型题？

作者让模型过一遍所有的题，根据Historical Variance Score ，历史正确率方差，选择最典型的题。高方差题意味着“差一点就能做对”， 说明模型已有相关知识，但尚不稳定。

这很像我们做题应对考试的过程，要做典型题：
- 最难：几乎总错，梯度方向单一，信息少；
- 最简单：几乎总对，梯度几乎为0；
- 典型题：高方差则兼有“错 ->负梯度、对->正梯度”的完整信号，最适合 RL。

这种选择，也很像那些不屑于题海战术的学霸，他们只做最典型的课后习题，贴合考试大纲和课程重点。

Reward设计：

这道题在训练中会被重复采样上千次

-- 放大奖励信号： 二元 0/1 reward：失败永远给 −1，成功给 +1，梯度方向稳定且幅度大。
-- 探索熵正则 Entropy Bonus： 迫使策略保持输出多样性，防止模型死记硬背，奖励同一道题的发散思维。熵正则迫使它不断尝试不同推理链，从而归纳出“哪些共同点最可靠”。

最后关于RLVR的思考：

Reinforcement Learning with Verifiable Reward (RLVR)，是用外部可验证的“绝对正确”信号训练， 是把“能自动判对错”的任务包进 RL。

一道历史方差高的数学题胜过千题低信噪样本，从信息论视角即“单位 reward 的互信息最大”。

RLVR的过程，不是产生新的知识，而是主要是在激活大模型已有能力。

在LLM + RL 时代，最昂贵的也许不再是数据，而是高质量、可执行的约束，写得出 reward 函数，就能用 RL 把潜能变功能。

https://arxiv.org/abs/2504.20571
