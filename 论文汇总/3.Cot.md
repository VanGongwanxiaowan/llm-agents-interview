让人类能读懂的 CoT 是否可以提高 LLM 任务的准确度？

答案也许是 No. 

Subbarao Kambhampati 的最新工作探讨了这一点。

CoT 当然可以提高模型表现，但这不等于 CoT reasoning trace 可以帮助人类理解 LLM 的 reasoning 过程。

[ Do Cognitively Interpretable Reasoning Traces Improve LLM Performance? ]

[参考链接](https://arxiv.org/pdf/2508.16695)

大量使用 Claude Code 后，重新读了 SWE-Agent，开始理解使用中的一些问题。

cd/ls/cat/grep/find, Vim-style next/prev 本质上是Human-oriented CLI。

所谓 Human-oriented, 意为着这些CLI设计初衷迎合人类的眼球转动速度，使用CLI过程中的short term 记忆。

Human-oriented CLI与LLM的特性矛盾，导致CLI Agent使用这些CLI的时候，给Agent的单次action的信号太低，导致CLI Agent 在实质上倾向于消耗大量token，并且更容易犯错。

所以，虽然相比于GUI，CLI更加适合 Agent， 但CLI终究是为human设计的，并不是ACI (Agent Computer Interface) 的最终形态。

如果你做的仅仅是another CLI Agent，几乎没有价值。

附上论文中关于 ACI 设计的原文，大家也思考一下，到底什么样的交互，是真正的 Agent Computer interface。


我觉得有可能跟LLM search API和MCP一样，针对SWE agent把一批cli封装成这样：

-input是一小坨json，output是一大坨json，里面有各种状态和exception、flag

-不要任何进度条、颜色、翻页、确认、选项

然后拆分subtask给一个agent，让它反复input、check output，经过几个来回，把subtask汇报回去。


[参考链接](https://arxiv.org/abs/2406.08534)


给agent的单次的action的信号太低了，导致cliagent实质上是倾向于消耗大量的token的

训练的时候的json范式导致模型每一步都要输出一堆resoning，导致过度理解

训练的时候的json的范式导致模型每一步都要输出一堆的reasoning，导致过度理解
