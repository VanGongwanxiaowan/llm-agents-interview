当前大模型之所以“看起来富有同理心”，主要得益于以下两点：
① 在预训练阶段，通过海量对话语料学习到共情类话术；
② 在对齐阶段（如 RLHF 或 RLVER）进一步利用情绪或人类反馈信号强化“共情”行为。

SAGE 通过情感推理将这种“情绪价值”转化为 0–100 的可追溯评分，为 RLVER 提供了精确的奖励信号，使得参数量较小的 7B 模型也能迅速获得较高的情商表现。

将严谨的心理测评融入 RL 框架，为大语言模型从“理解文本”（智商）向“理解人”（情商）演进开辟了新路径。期待未来在更多任务和场景下对该方法的更多实践与验证。


「Sentient Agent, 情绪价值 」

为什么大模型总是那么有同理心，会提供情绪价值？

分享两篇文章，关注 LLM 智商之外的另外一个维度，情商，即情绪价值的能力。

[ 论文 1 ] Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models

[ 论文 2 ] Reinforcement Learning with Verifiable Emotion Rewards

先看 [ 论文 1 ] 的SAGE，找到量化LLM情商指数的心理学理论基础：BLRI 与 Rogers/Kolden 共情指标

BLRI，有四个维度：
同理理解、
尊重感、
真实一致性、
无条件积极关注

Rogers/Kolden，关注三个维度：
自然流畅、
专注力、
深度连接

SAGE把 LLM 推理生成的 0–100 分的Emotion Score，用 BLRI 和 Rogers-Kolden 三维共情指标做外部校标

有了SAGE，为认知评估流程提供了内部一致性，意为着可以为情绪打分的Sentiment Agent成为可能，即Sentient Agent as a Judge。

于是[ 论文 2 ]来了，依托 Sentiment Agent，让情绪分变成可用奖励，让小模型也可以高情商。

模型生成-> 
Sentient Agent 更新情绪 
-> 回合奖励 ；
终局 eT/100 作为整段对话的 PPO / GRPO 目标。

读完论文的最大收获，每天默念BLRI和Rogers/Kolden，让自己学会提供情绪价值：😀

同理理解
尊重感
真实一致性
无条件积极关注

自然流畅
专注力
深度连接


RLVER:
https://github.com/Tencent/digitalhuman/blob/main/RLVER/RLVER.pdf

SAGE:
https://arxiv.org/abs/2505.02847

