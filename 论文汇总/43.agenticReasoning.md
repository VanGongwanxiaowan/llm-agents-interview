Agent, Reasoning」论文 

ARTIST: Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning  

ARTIST 让我recall起 ReCall，二者配方和味道相似，但 ARTIST 的最大亮点在于使用了 τ‑Bench 这一难度极高的 Agent 评测集。  

看完ARTIST，已经对最近的工作审美疲劳。  

如果说近来的 Agent 论文已经呈现出一种“套路”，大致如下： 

- 预留 special token（也就是我之前总结的“协议 token”）； 
- 通过 RL（如 GRPO）微调模型，让它在合适的时机输出这些 token，以决定 when / how / which 工具调用。  

来看ARTIST的方法： 
– 目标：解决 multi‑turn tool calling（何时、如何、调用哪一个工具）。 
– 协议 token：<think>  <tool>  <output>  <answer>。 
– 训练：采用 GRPO 进行 RL 微调。 

 如果你看过我前面的分享，这与Recall paper在方法上非常相似，回顾Recall 
– 目标：突破仅会使用 search API 的限制，让模型掌握多种工具。 
– 协议 token：<think>  <tool_call>…</tool_call> <tool_response>。 
– 训练：同样使用 GRPO。 

 即时方法略有不同，但思想基本一致。  

– ARTIST 在 τ‑Bench这一难度极高的 贴合实际Agent场景的benchmark上评测；（τ‑Bench 之前也分享过，感兴趣度读者可以去看） 
– ReCall 目前在 SynTool 沙盒环境和 MuSiQue multi‑hop QA 上验证，仍在迭代中。  

学术套路化的好处在于：这一整套为私有模型注入 Agent 能力的 RL‑微调方法已经足够清晰，工业界可以直接采纳并按需调整。  仔细读我分享文章的朋友，或许已经对这个套路了然于心了。

paper:

https://arxiv.org/abs/2505.01441
