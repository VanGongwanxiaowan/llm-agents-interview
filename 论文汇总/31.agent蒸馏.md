「Agent Distillation」

Distilling LLM Agent into Small Models with Retrieval and Code Tools

Agent 蒸馏：将大模型的 agent 能力迁移到仅 0.5 B–3 B 参数的小模型上。

如何做到的？

作者提出的 Agent Distillation 框架:

保留 Thought 和 Action 两个核心决策信号，把 Observation 仅作为上下文输入而不计入损失，让小模型把有限容量专注在 怎么想 + 怎么做上，而非死记工具返回的长文本／数值。

即与传统 ReAct 完整推理轨迹 Thought - Action - Observation 蒸馏方式不同：
- 监督：只在 Thought + Action 上求交叉熵
- Observation：保留在输入里供模型阅读，但不要求逐字复制，因而避免噪声干扰并节省参数和算力。

方法上：
- First-Thought Prefix (FTP)：在教师轨迹开头插入“一句总规划”，让每条示范都有清晰的全局思路，提升训练数据质量。
- Self-Consistent Action Generation (SAG)：推理阶段一次采样多条 Action，解析-执行-过滤错误，显著降低语法 / 运行错误。

My take：

Observation 不计损失 并非完全忽略环境反馈；模型仍能读取 Observation 并在下一步利用，而把学习重心放在策略本身。

局限性大，若未来工具返回格式变化，需要再训练适配。

Agent Distillation， 作者起的一手好论文名。


https://arxiv.org/abs/2505.17612
