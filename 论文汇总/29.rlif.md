「RLIF , Reasoning」
Learning to Reason without External Rewards

不依赖外部奖励，RLIF 让LLM 靠“自信” 就学会推理！

非常精彩，但，Self-Certainty 是否也是一种Spurious Reward？

“RLIF, a paradigm where LLMs learn from intrinsic signals generated by the model itself. ”

RLIF的优势：
把 RLVR 外部可验证奖励替换成模型的内在信号，训练过程零标注、零测试集依赖，成本与适用场景瞬间打开！

方法上：
INTUITOR 只用模型自己的 self-certainty（输出分布与均匀分布的 KL 距离）当奖励。

这个指标简单却有效：
- 自信高往往真做对，
- 自信低多半在胡说，
因此强化 LLM 自信就等价于强化正确推理。

非常喜欢这篇文章，但不免与Rethink-RLVR的文章产生联想：

Self-Certainty是不是另一种 Spurious Reward？

- 它并不直接检查答案是否正确，只奖励“我很确定”。从信息论角度看，没有向模型提供“正确性”这一随机变量的额外信息。
- NTUITOR 仍用 GRPO，同样带有裁剪偏差；裁剪会优先推高原本概率大的 token，有放大模型固有行为的特点。
- 依然围绕 Qwen 做实验，是否重踏“先验陷阱”？
https://arxiv.org/abs/2505.19590
