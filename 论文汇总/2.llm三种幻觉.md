LLM 发展到今天, hallucination 已经不能准确的表达它错误生成的现象。 

第一种， Hallucinate，幻觉。
LLM 不确定真相，但回答的动机是诚实的，只是事实错误。  

第二种，Lie，谎言。 
LLM知道真相，但要完成某种目的，故意误导，编造谎言。  

第三种，Bullshit，胡扯。 
LLM 根本不在乎真相是什么，对真相漠视，只是完成输出。  

第三个我觉得特别明显的场景就是，比如你去问Michael Jackson的生平或者Michael Jordan之类的名人生平，或者一些名著的具体情节，答案一长就开始胡编乱造。我猜原因是token权重相似，LLM只为了语法上的完备就乱组合了。这个就不是说LLM不知道的问题，而是知道，但胡扯。非常难解决



要理解这三种现象，看这三篇论文：  


幻觉： Why Language Models Hallucinate  

谎言： Can LLMs Lie? Investigation beyond Hallucination  

胡扯： Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models

“人类说谎的时候，由于需要付出努力来编造和压制矛盾信息，会比说真话会产生更高的认知成本。”

那么，大模型会不会说谎？

如果把hallucination定义为单纯的傻，LLM的说谎行为则隐含着为了达到某种目的的intend，大模型如果有这种intend，会带来比幻觉更加严重的安全问题。

分享论文：
[ Can LLMs Lie? Investigation beyond Hallucination ]

文章发现，当 LLM 被提示说谎时，模型会将内部计算（作者称之为 steel compute）重新分配给特定中间层，这证明内部工作量不同。

这与人类说谎的额外认知成本以及大脑的特殊区域控制联系了起来。

日夜陪伴愚蠢人类的 AI，它可能真的会对我们撒谎。

https://arxiv.org/abs/2509.03518

[参考链接](https://arxiv.org/abs/2509.03518)


真正的智能，是让模型在生成的时候就做正确的选择，而不是事后验证那个选项是正确的。

