「 Byte Dance, Agent, Coding 」

Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning

把代码生成模型（Coder） 和测试生成模型（Tester） 看成两名对弈选手，那么每一次“编译-执行-断言” 的成败都能反向优化双方。

Test-Driven Development 思想在 Agent 时代的实践。

一段代码能否通过某个测试用例，本身就是最直接的功能正确性反馈。

沿着这个思路，作者提出 CURE，把“代码-测试自博弈”提炼为可微奖励，摆脱对真实参考代码的依赖。

在方法上：
- 用公开指令模型作为初始 Coder 与 Tester

- 对同一个自然语言编程题：
   Coder 采样 k 份候选代码 
   Tester 采样 m 条单元测试 
   对每一对 (代码 + 测试) 编译运行，得到通过/失败二元结果，形成 交互矩阵。

- Coder 奖励 通过的测试越多越好。
- Tester 奖励 则要既鼓励“杀死”错误代码，又避免琐碎或冗长用例。
- 对 Coder 与 Tester 分别使用 Policy Gradient更新，梯度只依赖自身输出与对应奖励，无需外部标注代码。

在代码生成方面，全面超越同尺寸 Qwen-Coder、DeepSeek-Coder、Seed-Coder。训练后的 Tester 亦可作为外部 reward。

太精彩了！

我最喜欢的一点，CURE实际上把 RLVR 的“固定验证器”替换成 可学习的 Tester。

RLVR = 固定可验证信号 -> 奖励模型
CURE = 让可验证信号本身也用 RL 来进化

另外，读完论文让我想起早期的软件开发：那时 Code Designer （是的，那时候的coder叫designer） 和 Tester 通常是分开的两个人。

Tester 先理解需求并设计测试用例，Code Designer 则负责实现能通过这些测试的代码。

两人常常“斗智斗勇”。Tester 挖坑，Designer 填坑，在对抗中不断进步，最终也形成了惺惺相惜的默契。


https://arxiv.org/abs/2506.03136
